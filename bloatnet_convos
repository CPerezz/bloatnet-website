> Kamil Chodo≈Ça:
@marcin_d_s @goldbillka please schedule a call with @CPerezz even today if fesible

> Marcin:
@CPerezz, would you have some time for a quick meeting to discuss ideas around artificial state growth? I'm available every day this week (including today), if that works for you. What time would be convenient?

> CPerezz:
Today should work. What about 3pm CEST?

> CPerezz:
I can also do earlier or later

> Marcin:
I already have a meeting at 3pm, maybe 2:30pm?

> CPerezz:
Can do 2:30 yepp

> CPerezz:
carlos.perez@ethereum.org is my email in case you want to send an invite

> Marcin:
Great. I created a meeting and sent invitation

> Kamil Chodo≈Ça:
Hi!

> Gary Rong:
Is it possible to also invite a person outside of EF? He is a EF grantee working on the bloatnet as well

> Kamil Chodo≈Ça:
Yeah sure

> Kamil Chodo≈Ça:
I'm still off this week, will sync up with a team and we can organize it to accelerate as much as we can :)

> Kamil Chodo≈Ça:
Would love to have some artifical state of at least 2x mainnet on Berlin

> Kamil Chodo≈Ça:
Did anyone considered base-mainnet state? Which is already bigger than mainnet probably?

> CPerezz:
The problem with this is that we need a funded wallet with tons of ETH to distribute funds.

And with mainnet state we don't have that.

Though, we can edit the DB directly and somehow hack 1block approval such that we have this wallet funded and we can continue from mainnet state

> Marek Moraczy≈Ñski:
@CPerezz we can do it with withdrawals and engine api :)

> Marek Moraczy≈Ñski:
we can "fund" some accounts and do whatever we want to :)

> Gary Rong:
Please invite @delweng

> Gary Rong:
üñº https://github.com/jsvisa he is actively working on the Geth performance measurement with a larger state size

> CPerezz:
With a mainnet replica state? Wow that's awesome!!! Ww should def start there

> CPerezz:
Is there any way to contribute to the tool?

> CPerezz:
Would be then nice to implement a mainnet throughput multiplier

> Marek Moraczy≈Ñski:
yup, exactly, or somehow use your test cases

> CPerezz:
My test cases will help to bloat the state much more than a mainnet multiplier as they are designed to be optimal for that. 

BUT, the mainnet throughput is real. Which has a lot of value. 
I think keeping both might be a good strategy.
Nevertheless, something to discuss next week likely.

Im still researching üôÇ

> Marek Moraczy≈Ñski:
@marcin_d_s would be good if you show the tooling and you can guys discuss it together

> Kamil Chodo≈Ça:
We had a call with Ef Devops team already on how to solve funding issues on shadowfork in variety of ways so we will overcome that :)

> Kamil Chodo≈Ça:
Next week I'm back so let's jump in a call to discuss

> Marek Moraczy≈Ñski:
withdrawals + Engine API is our answer :)

> CPerezz:
Could we add @cskiraly here?? He wants to test some networking stuff :)

> CPerezz:
Hey @KamilChNethermind should we meet by the end of this week to concrete a bit more how we can kickstart the efforts? Also have some ideas I wanted to share too.

LMK!

> Kamil Chodo≈Ça:
Hi!

Yes for sure! trying to dig out of all of the things and get back on speed - can we make it Thursday maybe?

> CPerezz:
Sure, if you're too busy we can even do next week if you prefer. Monday or Tue.

I keep coding spamoor scenarios on the meantime and carrying over some research on more state bloat attack vectors + partial statelessness work. So no worries!

> Kamil Chodo≈Ça:
Want to have it ASAP :)

> Kamil Chodo≈Ça:
To align well on that :)

> Kamil Chodo≈Ça:
Great that you guys work on that! Marcin from our team who is most experienced in that is now on 2 weeks vacation (we got an unlucky streak in that now :D ) so want to make sure we can join our efforts well while we also work on 2 other things for scaling path

> CPerezz:
https://lettucemeet.com/l/5D5a3

Will add our team availabilities here. Requires no login nor nothing. 
Will send an invite afterwards!

> Kamil Chodo≈Ça:
Nice!

> Kamil Chodo≈Ça:
BTW - did any of you analyzed Sepolia after 60MGas increase?

> Kamil Chodo≈Ça:
I'm trying to analyze the data from Nethermind nodes and dont' see anything interesting for now - only thing is that we are afraid of state bloat but it will happen only if with increased gas limit usage of Eth will grow at least linear

> Kamil Chodo≈Ça:
I mean - if we will increase gas 2x but we won't get 2x more usage (meaning 2x more users are working on chain) state growth won't be 2x bigger

> Kamil Chodo≈Ça:
Even if usage will grow 2x doesn;t mean we have 2x more users but it may be 2x more usage of calling some light contarcts which do not affect state that much

> Kamil Chodo≈Ça:
Even tho AVG mgas per block went from around 18 to 29.5 now state growth is exactly the same

> Kamil Chodo≈Ça:
Scheduled a meeting for tomorrow! :)

> CPerezz:
I think you need to look for worst cases

> CPerezz:
Cheap reads

> CPerezz:
Disrupting cache etc..

> CPerezz:
That's odd

> CPerezz:
What txs are landing in blocks?

> CPerezz:
@asqyzeron @ignaciohagopian could you share your emails with @KamilChNethermind so he can add you to the meeting?

> Guillaume Geth team:
guillaume@ethereum.org and ignacio@ethereum.org

> CPerezz:
Hey @KamilChNethermind I did not get any invite to carlos.perez@ethereum.org

Could you double check?

> Kamil Chodo≈Ça:
yes

> CPerezz:
Got it! Will see u then!

> Kamil Chodo≈Ça:
Hmmm i jsut noticed it does not have any URL to join then meeting

> Kamil Chodo≈Ça:
And probably it was created by 
delweng@gmail.com ?

> Kamil Chodo≈Ça:
At least under this account

> CPerezz:
Unsure who has this email

> CPerezz:
But we can share a link when the time arrives

> jsvisa:
Hi all, this email is mine üôÇ Do we have the meeting via Google meet or some other alternatives

> CPerezz:
I'm fine with google meet.
Let me create it again with Ethereum account st we can have a recording, transcript or whatever!

> Kamil Chodo≈Ça:
@CPerezz do yoyu have any draft PR with any scenario ready for testing?

> Kamil Chodo≈Ça:
I just need any even most basic one to test one thing

> Kamil Chodo≈Ça:
we have one very beefy machine on which we want to check some parametrization

> CPerezz:
I can put the PR updated later tonight/tomorrow morning. As I'll close after ACD calling it a day.

You can atm use the 7702 scenario to bump up txcode for accounts.
Same with EOA -> EOA tx for funding new accounts (which you can already do with your tool deflecting validator revenue/coinbase txs to EOAs).

> CPerezz:
Hey @KamilChNethermind the spamoor scenario for contracts is ready and fully tested.

See: https://github.com/ethpandaops/spamoor/pull/41

To use it: https://github.com/ethpandaops/spamoor/blob/22ad2c08b9659abdb4b91a7ed3d57189267b173e/scenarios/statebloat/contract_deploy/README.md#-usage I've using Anvil to test it locally. But should work with any RPC endpoint well.

Apologies for the delay, but Friday was quite busy and did not have the time.
LMK if you need anything, will keep adding scenarios and checking the existing ones to see if they can already work for us.

> CPerezz:
Also, on another note, I've created: https://cperezz.github.io/bloatnet-website/

This is just a preview. But I hope it can get us some traction and make people collaborate and submit metrics or be able to follow our proces, findings etc...

LMK if anything is not ok or you want any changes (I haven't shared it anywhere, so should not be an issue if something is wrong).

The findings there btw are just so that you can see how it looks, will empty it once we start!

> Kamil Chodo≈Ça:
Thanks @CPerezz !

Will look on allt hat shortly - we will start working with network today so will apply those tests

> Kamil Chodo≈Ça:
Btw @asqyzeron you have maybe any ideas on this bloat based on Geth DB? And later on simply sync other clients?

> Guillaume Geth team:
could you please refine what you mean here? what bloat in particular are we talking about?

> Kamil Chodo≈Ça:
The thing we discussed on a last week call so that we can have special version of Geth where on sync of mainnet you take every single thing from state (contracts, accounts etc) and "duplicate it" so you clone it but under slightly different address.

Was wondering if you was thinking about it a bit more and have some ideas if this is actually doable.

> CPerezz:
I think the main issue for that is that while it replicates mainnet state, it doesn't allow us to use it unless we substitute addresses for accounts we know the Sk.

The problem is those might have tokens that came from a contract and the dependency tree gets really messy.

> CPerezz:
I think one thing that could make sense is to multiply it from genesis already

> CPerezz:
On this way we actually can own all accounts. Thus allowing us to use them later

> Kamil Chodo≈Ça:
Is a actual "usage of them" really needed? We can always at the end generate ourselves new set of accounts which will be funded and then we can interact with other contracts/accounts, right?

> CPerezz:
You can fund them with ETH but not with ERC20 for example

> Kamil Chodo≈Ça:
Yeah valid

> Kamil Chodo≈Ça:
To go from genesis we need a EL which syncs very fast on archive

> Kamil Chodo≈Ça:
Still will take a lot

> CPerezz:
That's fair. Also it's important that we can't bump gas limit up there. As txs have dependencies that we might hit

> CPerezz:
Maybe we could leave one with 1s block times or something just doing it's job while we do our first experiments with regular bloatnet

> Kamil Chodo≈Ça:
BTW - was wondering if we actually need network for a state bloating.

What if we would be able to generate thousands of newPayloads and hammer the implementations with that so we can skip all the network things and just simply start growing the state and reuse it later for testing? 

It is very young idea and don't have clue yet if it will be any simpler or not

> Guillaume Geth team:
this is definitely doable

> jsvisa:
I think there might be another issue that needs to be considered. How long will it take for us to reach a spamed state db of 100GB, 500GB. in my local exp, seems it can grow as much as 2GB/hour at most

> Guillaume Geth team:
we could control that, I think. But overall, if the point is to replay mainnet txs in a bloated testnet, I think that's a feature actually, because then we have "dead weight" which is what we want.

> Guillaume Geth team:
how would that be an improvement in your view?

> Guillaume Geth team:
Depending on how the ERC20 is implemented, I don't think it's necessarily a problem. At least the ref impl of an ERC20 uses a hash table. If you just add more entries to it, you can do that.

> Guillaume Geth team:
for the bloating itself, no. And there are lots of things you can test without it, e.g. pure execution of blocks. But if you want to test the sync/txpool, you need the network.

> Guillaume Geth team:
the only problem is how realistic your payloads will be, but otherwise, it's doable

> Kamil Chodo≈Ça:
Spamming is great (either via network directly or by newPayloads and then reuse resulting DB on network for testing) - we will try to kickoff the network today/tomorrow.

Generating it via modified version of EL tho seems like something which can quickly give decent results. 

Seems like it is not colliding work as eventually both can be used later for different things (spamming heavily with state heavy scenarios and checking how network behaves is super useful - while initial huge state can give us better insights much faster)

@asqyzeron anything I or Nethermind team can help you to start with that somehow? I don't know how much time you have now for such research but feels like super valuable to work on for sure.

> CPerezz:
You need the private keys of some account to be able to send such txs. 
That's the part that I'm dubious. 

They can fund wallets with ETH by deviating validator rewards or coinbase txs. 

But unsure if with ERC20s you can do that

> Guillaume Geth team:
I just need the certainty that this is something that is valuable, and also what flavor of it you want. After that, I'll make the time. Then I'll need some support from your devops people to make it run, but it's only a matter of coding time.

> Guillaume Geth team:
What I said is completely absconse: I mean, I'll make it my priority, if you are ok with me writing that tool.

> Guillaume Geth team:
so just to be clear, please üëç or üëé:

 - I will write a tool that takes a geth DB synced up to block n
 - v1: it just makes a copy of every account, including storage + deploys some "test ERC20"
 - v2: it tries to do add accounts to known ERC20s, copy the accounts to addresses we control, etc...

Running it will take some time, as jsvisa pointed out (btw hi @delweng, we never talked but I'm a big fan üëã) but then we could start this forked off network and try to sync it, or just generate a ton of blocks using dev mode.

> Guillaume Geth team:
Typing the above, I just realized I have a few questions:

 - in the case we just follow mainnet, we need to disable root hash verification. This is a problem to perform snap sync. So it might be smarter to just have our own shadowfork that produces blocks containing the same txs as mainnet (or more).

 - But if we do our own network, we need to have a validator set we control. I assume this is well handled by people accustomed to shadow forks, but I have never done it myself. I'm asking Pari (and asking him if he can join this channel)

> jsvisa:
I'm also interested in this tool. If possible, I'd like to develop it together.

> Guillaume Geth team:
sure!

> CPerezz:
I could add a spamoor scenario that does exactly this @asqyzeron

> CPerezz:
I get mainnet blocks, amplify them and sends them on the shadowfork

> Guillaume Geth team:
by "does that" you mean copying the txs from mainnet blocks? or something else?

> Guillaume Geth team:
ah yeah re-reading that, it's pretty obvious

> jsvisa:
could you please give more details about how to send them to the shadowfork, by sending the raw transaction?

> Kamil Chodo≈Ça:
Shadowfork is doable and we do that now on other network, v1 feels already like a huge thing to use so super happy if you will find time to experiment with that!

Adding ≈Åukasz and ben as they may have interesting point of view on state bloat via plugin from Neth Devs point of view.

> Kamil Chodo≈Ça:
And of course - all the infra needed and devops support you have here - just let us know what is needed so we can accelerate with that

> CPerezz:
Yes exactly. 
At least there will be a huge amount of them that are easy to construct.

Maybe we could have a 30min meeting to start thinking more on this. I think is doable via spamoor. But unsure if there are easier/better ways to do this.

In any case, would be interesting to decide if we start shadowforking mainnet or instead we go from 0 (I'd advocate mainnet too) 

@KamilChNethermind any strong opinions on your side?

> CPerezz:
üñº On another note, would be nice if from Nethermid you could fill in ideas for: https://hackmd.io/ob66NuW9QBORHLz7emhSCw

I've asked all the clients already. They will be doing it soon

> Guillaume Geth team:
üñº @delweng I have started on the tool, it's not quite ready yet but should be enough to get the conversation started https://github.com/ethereum/go-ethereum/compare/master...gballet:go-ethereum:bloatnet-command?expand=1

What it does, is that it iterates the snapshot, and updates the tree with generated data from it. Then, at the end, it regenerates the snapshot and creates a new block with the new root, which it sets as the new head block.

It has two outstanding issues:
 - I have to catch up with the recent changes in the trie commitment layer, I have been working on an older branch this past year - so I'm still working out the details of flushing everything to the triedb.
 - I can't build the new tree in one go, I have to figure out a good point for flushing the tree to disk. I'm thinking RAM usage but this approach has its flaws, 

Then I still have to do account storage duplication, but that's an easy one.

> CPerezz:
Can I help on anything?

> Guillaume Geth team:
if you can keep a 3 year old from waking up, yes!

> Guillaume Geth team:
short of that, I should be fine

> CPerezz:
lmao. Never felt so useless ahahhaha

> Guillaume Geth team:
I wouldn't worry about that üòÇ

> Guillaume Geth team:
but you're also working on the spamoor, we need that as well

> Guillaume Geth team:
in case you're bored üòÖ

> CPerezz:
I'm almost done with the mods needed for the 7702 bloating scenarios.
Once done, I just need the Yul contract for the random slot SSTOREs

And then I'll start with the attacks (though I'm not sure it makes sense to perform them with spamoor (some of them do, but others it's a meh)

> CPerezz:
Anyways, might be better to keep it all in one tool only

> CPerezz:
Otherwise will be too much

> jsvisa:
That‚Äôs sounds really a good starting point, it‚Äôs really fast to bootstrap the statedb. Yeah, the triedb write buffer is 256mb, I‚Äôm not sure whether or not it will persist into db when the mutations are larger than it

> Kamil Chodo≈Ça:
Small updarte after silence 

We have some strong problems with VM providers with Perfnet2 which hanged us a bit with bloatnet

> Kamil Chodo≈Ça:
Now thaat EF folks solved that and perfnet2 is kicking off we also discussed with Pari and Barnabas Blaotnet a bit

> Kamil Chodo≈Ça:
We will use super strong self hosted machines from PandaOps to start generating state as fast as we can

> Kamil Chodo≈Ça:
@cbermudez97 Will handle setup on that infra and will use @CPerezz scenarios + some additions

> Kamil Chodo≈Ça:
Also Carlos experiemtned a bit with higher gossip limit - on local network for sure you can pass 10MB limit

> Kamil Chodo≈Ça:
But not by a lot - seems like on 19MB block everything collapsed but

> Kamil Chodo≈Ça:
But still may be just a proper CL selection

> Kamil Chodo≈Ça:
@CPerezz 

About a call we should definitely schedule one for next week please - @marcin_d_s is back so our whole working team will be again on full throttle.

Trying to deliver as much goodies as possible for Interop + Nethermind day 2 day work with releases and stability causes a bit of overwhelm

> Ben {chmark} Adams ‚ü†:
This makes it a problem not being a protocol rule as what will happen will be undefined

> Kamil Chodo≈Ça:
Is defined now - everything dies xD

> Kamil Chodo≈Ça:
nethermind missing slots, reth producing empty

> Kamil Chodo≈Ça:
But various CLs used

> Ben {chmark} Adams ‚ü†:
yeah but you said at 19MB; so what will happen above 10MB depends on the make up of nodes and what route they get blocks on

> Ben {chmark} Adams ‚ü†:
So is undefined behaviour (for consensus)

> Kamil Chodo≈Ça:
yeah - env is super local so networking is almost eliminated

> Kamil Chodo≈Ça:
We can always tests that out "slowly" - 11, 12 etc

> Kamil Chodo≈Ça:
but wasn;t a goal

> Kamil Chodo≈Ça:
Goal was to check if we can do 100MB on local network

> Kamil Chodo≈Ça:
And probably we can't

> Kamil Chodo≈Ça:
At least not with that setup

> Kamil Chodo≈Ça:
Would be great to speed up few times

> Ben {chmark} Adams ‚ü†:
A challenge for QA though ;)

> Kamil Chodo≈Ça:
Too many challenges exception

> Ben {chmark} Adams ‚ü†:
Photo

> Pari:
So a higher gossip limit isnt a matter of just bumping the value, the issue is that there's a gossip amplication factor on the CL side. So if a 5mb block doesn't actually meant that your node has to tx 5mb more, it means it has to transmit 5mb * gossip amplication factor times more. Last I checked, this value was 8. So we ideally shouldn't change this value without properly understanding the networking side of things, however I think im missing something - why does state bloat need to think about CL gossip limits?

> Kamil Chodo≈Ça:
If we want to start a local network and using kurtosis scenarios being worked by @CPerezz to "speed up things" we may want to go slightly above

> Kamil Chodo≈Ça:
So we can process even bigger blocks

> Ben {chmark} Adams ‚ü†:
Faster bloat

> Pari:
So @asqyzeron said he is also working on this topic, is this the same project or different ones?

> Kamil Chodo≈Ça:
Yeah - I could wrap it up to these 2 words :D

> Kamil Chodo≈Ça:
We decided to work on two streams in parallel - Bloatnet with spamoor bloating tests and Plugin for Geth to make a duplication of state

> Kamil Chodo≈Ça:
Both having their own advantages

> Pari:
Ah sure, so the idea is to generate one really large bloat state and then snapshot that to reuse i guess?

> Kamil Chodo≈Ça:
Exactly

> Pari:
Yup that makes sense, yeah on the same page now :D

> Kamil Chodo≈Ça:
You can achieve it in both ways - one will be fast, second will also stress test nodes "on chain"

> Pari:
Also for the spamoor approach, you technically dont need more than 1 node. So no p2p node == no gossip limit :P

> Pari:
(atleast on paper)

> Kamil Chodo≈Ça:
Second (spamoor load) can also dictate how the state will look like (more addresses, more focus on erc20, more focus on contracts etc)

> Kamil Chodo≈Ça:
Once I tried with single node with kurtosis something didn;t worked out well

> Kamil Chodo≈Ça:
But maybe was my failure so is why i always use at elast 2 nodes

> Pari:
ahh it depends on your CL, you can pick them and ignore some log msgs to get a single node setup working

> Kamil Chodo≈Ça:
But it makes sense - @cbermudez97 we should check that once more :D

> Kamil Chodo≈Ça:
Which one you suggest?

> Pari:
i think lighthouse needed some flag that basically tells it to be fine with 0 peers. 

participants:
  -cl_type: lighthouse 
   cl_extra_params: ["--target-peers=0"]
additional_services:
  - dora

> Pari:
That should do the trick, lemme know if it doesnt and ill see what could work

> Kamil Chodo≈Ça:
NICE - thanks!

> Kamil Chodo≈Ça:
Will help a lot

> Guillaume Geth team:
same project afaik

> CPerezz:
Im all yours. Just lmk the day. Im done with account + bytecode & storage bloating scenarios. Now working on big-state attacks

> Kamil Chodo≈Ça:
Let's do tuesday? Want to let Marcin catchup after 2 weeks off

> Kamil Chodo≈Ça:
And to prepare everything for bloating and maybe have some first data if we will manage to

> CPerezz:
We will try to evaluate it and try to stick as close as possible to mainnet %-wise

> CPerezz:
Sure!!!!

> CPerezz:
There are 2 approaches.

1. We bloat the state with @asqyzeron 's tool and we run the attack vectors and also test syncing, db compaction etc at concrete DB sizes.

2. We artificially bloat to ~400GB or so and from there we collect data while bloating at a high pace. That will give us a lot more info and data regarding lots of aspects. Specially how things degrade/evolve with time.

> CPerezz:
That's at least what I have in mind for now

> CPerezz:
Still, would be useful if client teams would submit some data they're interested on

> Csaba:
There are limits on message sizes in libp2p and in rplx. In all implementations i have seen, these are just constants, but you have to hunt them down and bump them. I can help with Geth and Nimbus.

> Csaba:
For GossipSub, we can reduce the degree. The original degree was tuned for a network of 10K nodes. With a degree of D=8, you don't have too many hops, because it spreads exponential with a factor of D=1 ... but you have all those nasty duplicates.
With a small network, you don't need such a degree. With D=4 you still scale by *3 in each hop, which is enough (I guess you don't have many nodes)

> Csaba:
As I understand here the goal is not to test networking, so we can just hand-tune these parameters, simply to enable testing the rest. To do a network scaling test, the setup here is anyway unrealistic, so better separate it out into a different testnet.

> Pari:
Yup agreed, we settled on a single node network where networking wont play a role at all since the aim is to just test state. The networking related stuff would need to look different anyway

> Csaba:
Depending on how you generate TXes, and whether you push them through the mempool, parameter tweaking might also be needed on the EL side. I guess you don't need this, but if needed parameters can also be bumped on this. Would mostly mean more RAM.

> Csaba:
what you mean by a single node network?

> Guillaume Geth team:
we do want to test networking as well, especially how long it will take to sync

> Pari:
The aim is to locally generate a large state, then import said state into other nodes. For this you don't need more than one node to submit transactions to. In the config listed, you can bloat the state by setting a very large gas limit and there's no networking params you have to tweak to allow for it.

> Pari:
Once you have the state, you can import it into other nodes and then start a real network for other tests

> Pari:
but first step is getting a state that can act as a starting point

> Kamil Chodo≈Ça:
What if later for networking we will make a network with static selection of peers so we can simulate quite big depth with small amount of nodes?

> Kamil Chodo≈Ça:
Off course out of scope for this task

> Csaba:
Might worth trying to separate the state size and the state growth aspects of that.

> Csaba:
When you say import, is it sync or something else?

> Guillaume Geth team:
we are building two different tools for each of these purposes indeed

> Pari:
for the same client, you can just copy over the database (after deleting the nodekey) and the client would startup with the larger state. for other clients, they'd need to fetch it via sync

> Kamil Chodo≈Ça:
Ultimate goal is to serve it and sync it via network while fuzzing with heavy txs so both serving node will have hard conditions for serving while still being in sync and syncing node to sync and heal.

> Guillaume Geth team:
Hello, so I just published what I would call "v0", which is a bit of a departure from what I promised in our last call, but will serve as a very good testbed for the tooling/testing the setup while I work on the final version. This came out from a conversation with Gary, in which he made a few good points, enough to convince me this was a reasonable first step.

So what this does, is that it will create about ~10k accounts per block, and 15 million leaves (30 leaves every two account). It will therefore take some time to bloat the state.

Why is this v0 useful? To get stuff started quickly and "safely" while I resolve the issues in the v1. It's also going to make it possible to get clients that don't support snap sync to join as well. If they use the same randomness, that is.

Why is v1 more useful? This approach dumps uniform accounts, and each account has a max of 30 leaves. it comparatively slow to bootstrap the network. These need to be addressed, but I wanted to make sure other people could make progress while I fix the remaining issues.

Why not the v1 directly? Because everything is snapshot-based, and I want to iterate the snapshot as I read it. That means I need to move some data around. I believe it's doable, but still need a few days to get it to work.

> Guillaume Geth team:
üñº This is the branch if you need it https://github.com/gballet/go-ethereum/tree/bloat-per-block

> Guillaume Geth team:
You set it up by adding a BloatNetTime in the network's config

> CPerezz:
EOA spamming + 7702 auth abuse is complete.
You can see it here: https://github.com/ethpandaops/spamoor/pull/41/commits/86ddd4b437e159fbe606f23476e4616ea848fb8f

> CPerezz:
Now going for attack vectors. 
Will take care of the rest of "less bloating rate" scenarios after

> Kamil Chodo≈Ça:
Great! @cbermudez97 today is setting everything up on one super beefy machine so we will both start local bloatnet and also will try this generation out

> Carlos Bermudez Porto:
Hi. For running this Geth version what does it requires? Do just starting Geth with a Mainnet snapshot and that configuration in its config ( genesis.json?) is enough or it requires a CL client connected?
About the configuration, can you share some example in its usage? Where would the BloatNetTime param go and what value it expects?

> Guillaume Geth team:
It needs to be built like any other shadow fork: add BloatNetTime to the genesis.json and generate all other configuration files. ethpandaops has the tooling for it. It might require some tweaks on their side.

> Guillaume Geth team:
But maybe Nethermind has a team that could do the same work?
> Carlos Bermudez Porto:
Thanks. So BloatNetTime will be like an additional EL Fork?

> Guillaume Geth team:
yes

> Guillaume Geth team:
it's like other shadow forks except verkle, which was more complicated

> Pari:
@cbermudez97 do you have this? If not i can take it up next week

> CPerezz:
@KamilChNethermind planning to run the bloater today?

> Carlos Bermudez Porto:
Hi team. Sorry for the late update.

I finished the setup for the Mainnet state bloat using Kurtosis. However, run into an issue that didn't notice before when asking on how to trigger the bloating. Since we are using a Mainnet Snapshot for the Shadowfork there is no call to geth init --config genesis.json which causes the bloatTime param to remain undefined. Is there any other way to update fork configs params for an existing db? If not then will need something to change this config from flags, something similar to --override.prague would be great.

cc @asqyzeron @KamilChNethermind

> jsvisa:
@asqyzeron could you please take a look https://github.com/gballet/go-ethereum/pull/542 add --override.bloat and enable bloatnet in local devnet

> Pari:
You can specify the image and the flag in the kurtosis config, no changes needed to the package üòÑ

> jsvisa:
üñº Thank you. However, I would like to base it on the gballet branch. Any subsequent changes will also be merged into his branch. BTW, May I ask where can I access the kurtosis config, couldn't find anything related to bloat state in https://github.com/ethpandaops/ethereum-package

> Kamil Chodo≈Ça:
Carlos left me some instructions on where he ended yesterday so will try to continue from there

> CPerezz:
Currently, trying to ask @parithoshj to give permissions to add all of the DB metrics in Grafana for Geth that @delweng prepared!!

@KamilChNethermind, lmk when we will have our call. As I'll prepare a list of all the metrics in detail (that we want and we already have) such that we can compare Nethermind and Geth as much as possible (as I assume these would be the most responsive clients when it comes to modify nodes to gather data).

> Kamil Chodo≈Ça:
Let's make it tomorrow?

> Kamil Chodo≈Ça:
Or thursday?

> Kamil Chodo≈Ça:
Open for both and will organize it on our side

> CPerezz:
Both work. Up to you though slight preference for Thu

> Kamil Chodo≈Ça:
BTW added @tanishqjasoria - he said we could eventually do the similar thing as @asqyzeron does so we could cooparate, compare and maybe do some more interesting verifications for state bloat from code perspective.

> Kamil Chodo≈Ça:
Let's make Thursday then!

> Pari:
is this sorted? or can i help ?

> Guillaume Geth team:
Just got aware of this, reviewing jsvisa's branch and merging if I like it.

> Guillaume Geth team:
Which I don't üòÇ

> Guillaume Geth team:
I need a few minutes

> CPerezz:
@KamilChNethermind have you setup the Geth node for Bloatnet? Ideally, we should also run promethues scrape job and datasource as pointed by @delweng such that we can get access to the data in the Grafana he prepared.

> CPerezz:
Should be running Geth master

> Guillaume Geth team:
@KamilChNethermind you can now use my branch, I have merged jsvisa's PR. Thank you jsvisa

> CPerezz:
Is there anything that needs to be done from kamil's side to setup the prometheus and data source?

> CPerezz:
So that we can get the metrics in grafana?

> Kamil Chodo≈Ça:
It was started yetserday as per Carlos just needs some tweaks

> Kamil Chodo≈Ça:
So we will work this out shortly - quite a busy day :D

> Carlos Bermudez Porto:
Hi @CPerezz. Can you share some example on how to run these scenarios from this Spamoor branch? Also do we want to run them while Geth is doing the bloating?

> jsvisa:
Geth bloat the states by inserting 10000 random accounts just before the block finalizing, so I think it‚Äôs ok to run them both at the bootstrap stage

> CPerezz:
The readmes of: Setcodetx and statebloat/contract-deploy scenarios should give you the commands

> CPerezz:
But ping if its not clear and will send the exact ones!

> Guillaume Geth team:
@delweng reviewed your PR

> Carlos Bermudez Porto:
Hi team. Bloatnet boostrap node metrics will be available here: https://perfnet.core.nethermind.dev:8084/d/geth_dashboard/geth?orgId=1&from=now-5m&to=now&timezone=browser&var-instance=bloatnet-bootstrap-0&var-percentile=0.95&var-query0=&var-prometheus_ds=prometheus_ds_1

I have restarted the boostrap process as last run ran into some issues which I couldn't found the cause. Network stayed alive but Blocks were processed very slowly once the bloating started. Due to blocks delays spamoor instances weren't able to work fine. In the latest restart the spamoor instances manage to start doing spamming and will leave them running.

Let me know if there is something else that I should add and will take a look at it tomorrow. üôè

> Carlos Bermudez Porto:
üìé This is the network_params.yml Im using from Kurtosis in case you want to reproduce it locally(should work for smaller chains with minimal changes).

> Guillaume Geth team:
This is normal, it's bloating and it takes time to write data to the db. This is why I wanted to bloat offline, which I might still want to do at a later stage.

> Guillaume Geth team:
If this is too slow, we can deactivate the Spammoor for a while or we can reduce the amount of data being written per block

> CPerezz:
My suggestion if you want faster bloating is to broadcast from different instances the 2 different scenarios I created.

Fund the two wallets massively overtime and go for it (ideally use diferent sK to avoid nonce-issues).

Also, bump gas_limit up to something like 100Mgas. And decreate block-time to something like 4 or 5 seconds.

> CPerezz:
üìé On another note, is there any way on which you could:
1. Include @delweng 's grafana dashboard metrics to your grafana (attaching the json here)
2. Share a link for external view of the dashboard via iframe? Wanted to add it to Bloatnet website for people to follow.

> Guillaume Geth team:
What I understand is happening, is that the spamoor is running at the same time as the bloating tool

> CPerezz:
Hmmmmm. I thought we ran the bloating tool first against the raw-db. Then when we make it to certain size, we mine a block to re-compute the root and then we sync the rest of clients and start spamming

> jsvisa:
Agree with it, after reaching to certain size, disable the bloating for each block

> jsvisa:
Else for the metrics we collected will also include the bloating time

> Guillaume Geth team:
that was and is still the plan, but yeah the v0 doesn't do that: it bloats as blocks are built. Doing v0 meant we could start faster and we could find issues in the deployment. I think ti's fine because we just need a few days of running before we can run the spamoor. But if you really really want the v1 now, I'll get back to it.

> CPerezz:
I see. @cbermudez97 the Grafana also doesn't show state-growth nor bytes written to db nor anything like that. So is hard to estimate how much the slowdown is an issue or not. 

Anything we can help you with?

> Kamil Chodo≈Ça:
Hi guys!

Will look at metrics shortly - maybe there is more to expose.
Carlos is Cuba based so he will start in 5-6h :)

> Guillaume Geth team:
I could add a metric if you want

> CPerezz:
I wanted to start adding all the ones from @delweng and then continue adding what we need

> CPerezz:
üñº So far, one useful thing to do is to figure out what precise metrics should we gather for the metrics requested by teams.

See: https://hackmd.io/ob66NuW9QBORHLz7emhSCw

I think would be nice to go more granular and try to gather exactly what metrics we need t ocollect from nodes to evaluate the metrics well.

Tried to do that for sync and some others. But it needs some work and love

> CPerezz:
Will do it myself otherwise. I'm fixing some issues in the contract-deploy scenario and finalizing the extcodesie one (which can't test because it bloats my disk (did it twice already üôÅ )

> Guillaume Geth team:
yeah this is pretty general and should be collected from the metrics we have in grafana.

> Guillaume Geth team:
ok I guess one thing that is missing, is how much state there is...

> Guillaume Geth team:
that would be a good one to have

> CPerezz:
Lmk if you can have a quick chat at some point. Want to discuss this deeper.

@s1nam @delweng happy to discuss all that with you too and get feedback and ideas!

> jsvisa:
Seems we need to expose the state size via metrics, either by the disk size of  the kvdb, or for each block inserting, expose the written state size

> jsvisa:
üñº @asqyzeron when I'm running bloatnet in devnet, it seems like a memory leak? top usage as below:

and the cadvisor container memory usage is increaing until to mem limit, and then OOM

> jsvisa:
Photo

> Pari:
awesomee, do you need something from me on the kurtosis or machine side? is this running on the fuzzer machine or elsewhere?

> CPerezz:
@parithoshj what's the status on phisically-owned recommended-hardware machines?

> CPerezz:
These ones are one of the most interestings to get metrics from

> Pari:
ill sort out ordering them this week :D

> CPerezz:
Thanks a lot!!! That's awesome!!

> Guillaume Geth team:
that's normal since it will put stuff a lot of stuff in ram, if you don't have enough ram it's going to OOM.

> Carlos Bermudez Porto:
üñº Hi. I will add the dashboard you shared. üëç

For  DB size best would be to have it available from client which can perhaps split it into more detailed metrics. See something similar from Neth DB sizes. I can try to have disk added but will be only a total size.

> Carlos Bermudez Porto:
Yes it is running from the fuzzer machine.

> Carlos Bermudez Porto:
In this run networks seems in a much better shape, but with some final metrics from previous try you can see the memory usage going quite high: https://perfnet.core.nethermind.dev:8084/d/geth_dashboard/geth?orgId=1&from=2025-06-04T01:32:40.584Z&to=2025-06-04T02:28:51.911Z&timezone=browser&var-prometheus_ds=prometheus_ds_1&var-instance=bloatnet-bootstrap-0&var-percentile=0.95&refresh=10s

Not sure why didn't happen this time. Looks like the . Maybe an issue with the bloating starting as it looks db has increase much compared to previous run as well. Still in the 1.2tb total.

> Carlos Bermudez Porto:
Also, from spamoor I see these logs in the setcodetx scenario:
time="2025-06-04T13:37:37Z" level=info msg="‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê BLOATING PHASE #394 ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê" scenario=setcodetx
time="2025-06-04T13:37:37Z" level=error msg="failed sending max bloating tx: oversized data: transaction size 132582, limit 131072" rpc="10.20.50.52:32003" scenario=setcodetx
time="2025-06-04T13:37:37Z" level=warning msg="Resyncing pending nonce for 0x86cF016FB873D50a7B8F31EB154c9234DD31b058 from 780105 to 765500"
time="2025-06-04T13:37:37Z" level=error msg="failed to send max bloating transaction for iteration 394: failed to send max bloating transaction: oversized data: transaction size 132582, limit 131072" scenario=setcodetx

> Carlos Bermudez Porto:
Here is the working dashboard. Let me know if you want an Editor user so new ones can be added faster. https://perfnet.core.nethermind.dev:8084/d/denslx0obry0wa/go-ethereum-metrics?orgId=1&from=now-6h&to=now&timezone=browser&var-chain=bloatnet-bootstrap&var-instance=bloatnet-bootstrap-0&var-quantile=0.5&var-query0=&var-prometheus_ds=prometheus_ds_1&refresh=10s

Still they might require changes to the datasources and to some of the labels used in metrics.

> Kamil Chodo≈Ça:
Do we have any more dashboard to push here?

> CPerezz:
In this case, We're also interested on pure state size. ie, bytecode, accounts and storage slots purely.
As history is not going to grow anymore. So that should be what guides our metrics. Rather than pure DB size.

> CPerezz:
Ahhhhhh! Dammit so close!! That's setcodetx right? I'll need to reduce the auth tuples batched to make the tx fit.

Has this happened since the beggining?

> CPerezz:
Will have a meeting soon with some people from the Geth team including @asqyzeron to dive much deper on metrics and start creating the needed dashboards as well as adding metric capturing if needed.

> CPerezz:
If you want a quick fix, you need to lower: DefaultTargetGasRatio = 0.99 to something like 0.96 or so. That should reduce the auth tuples which should ideally lower the tx size just enough to be within bounds.

> CPerezz:
That was useful @cbermudez97 Will fix the scenario tomorrow to split into multiple txs when the size is bigger than the max!

> jsvisa:
@CPerezz  Added 4 metrics to track the statedb size, ref https://github.com/ethereum/go-ethereum/pull/31914/commits/c9cfc5ef9338c931df4bae85b1c2d592c60c5f48  If this is what we want, I will try to cherry-pick it onto the bloatnet branch.

> CPerezz:
That's exactly what I wanted!!! Awesome!

Was thinking on more interesting metrics to have. For example, you have one that measures avg. and max/min diverges on read and write performance.
It would be nice to capture this data and mark it with a compaction flag. To have best avg and worst cases whith and without compaction.

And that should be collected for either some reads and writes across a block or for all of them (whatever is easier and more manageable).

WDYT?

> CPerezz:
Also, I was wondering what are good measurements for syncing & compaction.

For compaction specifically I wanted to test compaction frequency at different gas_limits with heavy I/O load blocks.
Also wanted to gather how much time it takes depending on the gas_limits of the network and total time it takes to compact each time.

Unsure if there are insightful metrics I'm missing here.

> Guillaume Geth team:
could you make this against my branch?

> jsvisa:
sure, wait for a minute, the size calculation here might not be entirely accurate, but let's just let it run for now.

> CPerezz:
What divergence are we talking about?

> jsvisa:
eg: for the address size, should be considered as 20bytes or 32bytes?

> jsvisa:
here please https://github.com/gballet/go-ethereum/pull/545

> CPerezz:
@asqyzeron @s1nam any recommendations to get max accurancy?

> Guillaume Geth team:
32 bytes

> Guillaume Geth team:
that's what is stored

> jsvisa:
updated, and also change the storage key size from 32 to 64(account hash + storage hash)

> Guillaume Geth team:
had a hectic morning, trying to get your PR reviewed

> jsvisa:
I see, because it stores the last 128 layers trie in memory, and as the bloat is running, that did consumes a lot mem

> Guillaume Geth team:
exactly, and it stores a lot of stuff in memory at each block

> Guillaume Geth team:
so 128 x a lot

> Guillaume Geth team:
From memory, it's 256 MB

> Guillaume Geth team:
so 32GB

> Guillaume Geth team:
low bound

> jsvisa:
I‚Äôm curious about that, my node consumes 60gb, the log says the triediff only eats 12gb. Without any spam running

> Guillaume Geth team:
yeah sorry I mixed two things up, I'm trying to get the exact number

> Guillaume Geth team:
it's actually not easy to find out because the code size changes

> Guillaume Geth team:
right it's more like 11GB

> Guillaume Geth team:
x 128

> Guillaume Geth team:
unless there's a calculation mistake of course

> Guillaume Geth team:
per layer or total? if it's total then I definitely have an error somewhere
> jsvisa:
12gb in total

> jsvisa:
Photo

> Guillaume Geth team:
I'm not quite done reviewing your PR (getting to it now) but isn't that for a single line?

> Pari:
First draft of docs for berlinterop are here: https://notes.ethereum.org/@ethpandaops/berlinterop-perf

please provide some feedback on what you'd want added or removed.

> Guillaume Geth team:
can't edit my own posts for some reason. I meant: isn't that for a single range or is it cummulative?

> Guillaume Geth team:
ok I can't find these numbers any way I look at it. But even if it's correct, what are the other 50GB? That's pretty high memory usage.

> jsvisa:
Can‚Äôt quite understand well, but if you means the memory usage, yes it is, this is the total memory held by geth right now

> jsvisa:
I‚Äôll have a memory prof later, need to go out for a while üòÖ

> Guillaume Geth team:
enjoy

> Guillaume Geth team:
I'll have reviewed your PR when you are bck

> Guillaume Geth team:
Merged the metrics PR. I left some comments for things that I believe are incorrect, but I think the most important part right now is to test the tooling, so @KamilChNethermind @cbermudez97 could you please deploy the latest version of my  branch?

> Kamil Chodo≈Ça:
Will ask Carlos as he have full access here now

> Kamil Chodo≈Ça:
he will start shortly

> jsvisa:
I‚Äôll look back the comments later today

> CPerezz:
Left feedback. Feel free to ignore whatever doesn;t make sense or doesn;t seem relevant!

> jsvisa:
@asqyzeron  just raised another PR https://github.com/gballet/go-ethereum/pull/546 , PTAL, else the geth will panic after geth stop

> Carlos Bermudez Porto:
Hi team. Had some issues with the machine I was issues so this update took a bit more time to apply. I have updated to the new version and added this panel with the new metrics: https://perfnet.core.nethermind.dev:8084/d/denslx0obry0wa/go-ethereum-metrics?orgId=1&from=now-3h&to=now&timezone=browser&var-chain=bloatnet-bootstrap&var-instance=bloatnet-bootstrap-0&var-quantile=0.5&var-query0=&var-prometheus_ds=prometheus_ds_1&refresh=10s&viewPanel=panel-202

Let me know if somethings needs more changes. üôè

> CPerezz:
Seems the machine was just turned down? 
Is the bloater working better now after @delweng 's patch?

Let us know what we can specifically do if there's any need from your side @cbermudez97 

Also, for spamoor, have you tried making the edit mentioned to the scenario?

> CPerezz:
This ^

> Carlos Bermudez Porto:
Looks like the node stopped processing blocks, the bloating started around 20mins ago which is close to the issue. I havent applied the spamoor fix. Is that something in the code or in the command?

> Carlos Bermudez Porto:
Think I will need to restart it, this time wont run the spamoor instances as I could see a lot of txs being sent which might have crashed the node

> Guillaume Geth team:
ok if the bloater is such a problem, I will try to get v1 to work asap

> CPerezz:
Code. Though planning to fix it tonight so that you just need to pull.

This is just a tmp fix

> Guillaume Geth team:
merged üëç

> Ben {chmark} Adams ‚ü†:
bloaty related tx https://etherscan.io/tx/0x6683cc74649d74f2619c862ecb761f14ff3d1fef54ccc089eec3b546e31d2e85/advanced

> Guillaume Geth team:
üñº Hey I published my v0.9, that still needs testing. https://github.com/gballet/go-ethereum/tree/bloat-at-genesis

> Pari:
How's the bloat stuff going btw?

> CPerezz:
@asqyzeron is fixing the bloater. There's a bug in it. 

We can otherwise start. 
We mostly have all the metric obtention implemented in Geth. (Unsure about Nethermind cc @KamilChNethermind )

Seems unrealistic to try to get other teams to implement collection for it TBH.

Aside from that, the scenarios for bloating are ready, and attacks are in progress (haven't had much time to code the last days).

Finally, would be nice to discuss how we can setup the sync scenario as is one of the first things we want to test together with compaction

> Carlos Bermudez Porto:
üñº The current run using the modified geth, without spamoor scenarios, has reached this size over the weekend. Now it seems stuck due to the fuzzing running again. Current size:
47M     execution-data/blobpool
2.1T    execution-data/chaindata
560K    execution-data/geth
0       execution-data/geth.ipc
4.0K    execution-data/keystore
0       execution-data/LOCK
64M     execution-data/nodes
396K    execution-data/_snapshot_eth_getBlockByNumber.json
4.0K    execution-data/_snapshot_metadata.json
4.0K    execution-data/_snapshot_web3_clientVersion.json
0       execution-data/transactions.rlp
Also these are the latest values from the new metrics: LINK

> CPerezz:
LOL. It seems it's all contracts.

Can you run the setcodetx scenario in max_bloating mode to compensate a bit? 

Then the last step will be to bombard contract storage

> CPerezz:
Will write a scenario for it ASAP (I'm with an attack for extcodesize abuse atm)

> Pari:
should i ask for a fuzzer to stop again?

> Carlos Bermudez Porto:
Network been down for a while so not sure if it can be restarted. Txs wont be processed right now

> Carlos Bermudez Porto:
Think I can try taking current db as a snapshot and create another kurtosis run from it. Without the modified geth and only run spamoor scenarios.

> Carlos Bermudez Porto:
Think the fuzzer can run until I have that ready

> CPerezz:
hmmm I see.

We should for sure stop spamming contract deployment and balance it out. setcodetx will do this exactly.

> Carlos Bermudez Porto:
We have tried running the new version and it finished without issues. However, the process finished quite quickly and the end results was a geth datadir with a total of 210gb. Is that the expected result or should it be different?

cc @KamilChNethermind

> Guillaume Geth team:
Yeah it's hardcoded to 256. I think it's fine for now, it's a run to test the tooling. Let's use that for now, I'll give you a new version tomorrow.

> CPerezz:
@cbermudez97 Guillaume meant that will change tge params to bloat until 500GB so you can leave it running and we can starr testing tomorroe

> CPerezz:
He should get back to you in 1h or so

> Kamil Chodo≈Ça:
BTW it was quite fast

> Kamil Chodo≈Ça:
Like an 1-2 hours?

> Guillaume Geth team:
yeah it was using some trick to make things faster. The problem is that the internal structure is imperfect, the tree is very imbalanced. But for testing the sync, it will still be good enough.

> Kamil Chodo≈Ça:
It is pure state with no history data?

> Guillaume Geth team:
I have already pushed the fix for making it to 500GB.

> Kamil Chodo≈Ça:
What Carlos should do now? Just start Geth with this DB to generate snapshot, right?

> Guillaume Geth team:
not sure what you mean by this

> Guillaume Geth team:
yes

> Kamil Chodo≈Ça:
Nethermind biased - we have quite clean split on "state" and "the rest" like blocks, receipts, bloom etc

> Guillaume Geth team:
for this, btw, you need to delete the db and start over

> Kamil Chodo≈Ça:
Wondering what is inside this 256GB generation here

> Guillaume Geth team:
so in there, you have mostly state, because there is only one block (so no history), nothing was executed (so no logs etc)

> Guillaume Geth team:
btw the bloater won't work, because I don't have a funded address that we control. Could you give me an address that the spamoor needs ? I will give it some funds.

> CPerezz:
Hey @gakonst

> Georgios Konstantopoulos:
gm!

> Georgios Konstantopoulos:
üñº https://github.com/ethereum/go-ethereum/compare/master...gballet:go-ethereum:bloat-at-genesis#diff-f0fd3235d22fb95cd73f1073467ac23397e9fc949e359fccb3ad0ff278dfac5eR325-R392 <‚Äî This is something we can port over to Rust to start from a same-genesis state

> Guillaume Geth team:
yeah, although this is not the final version, this would allow you to join the same testnet

> Guillaume Geth team:
Some stuff is randomly generated for now, but it'll become deterministic in further version of the bloater

> CPerezz:
hmmmmm

But how is it helpful that you port it? We mainly wanted to test sync and db compaction first. Targeting sync as is one of the scenarios that requires less effort. 

And if you have already a synced state, we can't test anything.

> Guillaume Geth team:
I think that in the current version it would not be very useful beyond checking how reth behaves with at 500GB state, but for tooling verification and such, it would still be useful. Let's discuss it tomorrow after my session

> CPerezz:
I'm afraid we just need to accept that we need to bloat block-by-block some Geth node with spamoor? 

An alternative (I agree) is that you an Erigon port this once is deterministic. 
As I assume we would need to re-bloat many many times. And we can't wait for syncing ofc.

> Georgios Konstantopoulos:
Yeah, IMO re-syncing w/ snap sync is gonna be unacceptably slow, so we need a thing that does:
1. big initial genesis state bootstrap
2. spammoor which continues the bloat block by block

> Guillaume Geth team:
that's what we are currently testing indeed

> Kamil Chodo≈Ça:
Is a guess that it will be slow - and if it will be we need to now if on 2x state it will be 2x or 4x slower - so to capture the data.

Also is a serving bottleneck, what will happen on healing part etc - so it is very valuable test to do indeed to know what next with snapsync

> Kamil Chodo≈Ça:
Thanks to @CPerezz we already have nice sceanrios for spamoor - just bloating alone will take some time and we want any results even this week.

@asqyzeron do you think we can somehow dump the state after generation to generate huge genesis file and provide it to reth/erigon for any testing needed? Feels like doable thing

> Pari:
So wed need to convert the state into a json dump that i can append into the regular genesis tool?

> Guillaume Geth team:
Not with this version. And the json would be terabytes anyway. I will make another version this afternoon

> Guillaume Geth team:
But you'll need some kind of binary importer.

> Carlos Bermudez Porto:
@KamilChNethermind

> Kamil Chodo≈Ça:
Was it running anything else afterwards

> Kamil Chodo≈Ça:
?

> Carlos Bermudez Porto:
That was the last state after the network stopped

> Kamil Chodo≈Ça:
Was it also running spamoor in parallel?

> Carlos Bermudez Porto:
No, that run was just geth bloating

> CPerezz:
@KamilChNethermind added @pk910 who will tell us how to import the node into kurtosis(or the data) and the json. So that we can start working on it

> Pari:
@pk910 i wouldnt do kt directly, the bloatnet stuff has a big diff from our usual devnets. So even manually starting a node first and seeing what happens is already useful

> CPerezz:
How can we best help you? We spoke a bit with @pk910 already Pari so he is aware of the differences. But we have a bloated node (the spamoored one which @cbermudez97 left running and has only bytecode)

So we can use this genesis which doesnt have any weird things

> Philipp (pk910):
I was thinking about making it a "external network", so we can use kurtosis to sync nodes to it, just as we do for public testnets..
Just needs a bootnode, so clients can find each other?
Or do I miss something?

> Kamil Chodo≈Ça:
@cbermudez97 can you please provide needed details for pk?

> Kamil Chodo≈Ça:
I know it was stopped for a brief moment but now restarted again and we can call it operational.

> Carlos Bermudez Porto:
üìé The current node is running in a kurtosis devnet inside smc-boulder-01 machine so its behind a firewall and not directly accessible from internet.  As for the network config these are the details to connect to it:
# geth node enode
enode://7fa991cc86d7dfaced5347b2e8033b42b3e3e477556d9ba8b6ea0172a4f759d1e4a6c1ad79b46bd13f0e82cd1010ddac62651336a61b625343a30f440dd59a04@10.20.50.51:32000

# lighthouse node enr
enr:-N24QHodMaqg_vFdm_zHhfJiynEXEh7v53LuIkTyTuxVsjk_YEBpX6eKZzq15Z3JESyBaxzZqzdMyH-xyKdhtImqCzUHh2F0dG5ldHOIAAMAAAAAAACGY2xpZW500YpMaWdodGhvdXNlhTcuMC4xhGV0aDKQpUSpy2AAADj__________4JpZIJ2NIJpcIQKFDIzhHF1aWOCfWeJc2VjcDI1NmsxoQMBsIUulRwxlfK1TqYBHGCdj_GyHggyg804ianL36EhS4hzeW5jbmV0cw-DdGNwgn1kg3VkcIJ9ZA

# lighthouse http api
http://10.20.50.51:32101

> Philipp (pk910):
the running lighthouse node has --target-peers=0, which prevents any external node from connecting :/

Can we easily restart that kurtosis devnet to get rid of that flag?
We probably need to, as we can't change the flags of a running container

(tbh. not fully sure how to get rid of that flag yet,  as lighthouse has problems when starting as solely peer in a devnet without that flag)

> Pari:
Are we building blocks or is this a stalled node?

> Philipp (pk910):
it is building blocks.
it's the only node running on that devnet and building blocks.
And we'd like to connect a second, which is prevented by the flag

> Pari:
hmm, we might need to set target-peers higher and test it then :/
or we have to swap lighthouse for another CL

> Philipp (pk910):
yea.  It's just the question of how to restart it? :D
I'm not fully sure about the hacks that have been applied to get te massive state imported.
Just kurtosis clean & run will probably not work

> CPerezz:
@cbermudez97 @KamilChNethermind can you help with this?

> Pari:
Yeah it might be messy :/ also might make sense to copy out the datadir and start a new node with it

> CPerezz:
Where should we setup the next node that we bloat and shadowfork? So that we can start overcoming this and snapsync issues?

> Philipp (pk910):
Any other cl client than lighthouse should be fine, same setup :)
It's only affecting lighthouse

> Philipp (pk910):
the kt setup is fine otherwise

> Philipp (pk910):
ok wait a moment,  we'll try something

> Kamil Chodo≈Ça:
/home/devops/cbermudez97/bloatnet/network_params.yaml

> CPerezz:
Hey! Bloater is running. Hope to have something in a couple days.

On the meantime, Marius asked if we could have a blockexplorer to follow the bloatnet.

> CPerezz:
Do @pk910 or @parithoshj have any way to quickly deploy such a thing in bloatnet? 

Cc: @cbermudez97 @KamilChNethermind

> Kamil Chodo≈Ça:
There is dora probably - not sure if can restart now adding something else

> Kamil Chodo≈Ça:
As it was Kurtosis run by Carlos B

> CPerezz:
I can halt spamoor on the meantime

> Philipp (pk910):
it's a mainnet shadow fork, right?
We can have dora for sure for the CL side,  but blockscout on EL side will be a bit tricky as it needs to sync through the whole history :/

> CPerezz:
It is shadowfork yes!
> CPerezz:
Does it do snapsync?

> CPerezz:
I assume not right?

> Kamil Chodo≈Ça:
http://209.127.228.122:32300/

> Kamil Chodo≈Ça:
Dora here

> Philipp (pk910):
not at all :D
the explorer works on rpc, so it goes through the chain block by block / transaction by transaction :D

> CPerezz:
http://209.127.228.122:32300/blocks will do I think!

> CPerezz:
At least has some very basic info

> CPerezz:
Bytecode done. Now Spamming account code to create enough new accounts.

Last stop will be contract storage which will take some more time

> Guillaume Geth team:
But do we need the whole history or is it ok to only have data after the shadow fork ?

Also, would it be possible to simply copy the db of an existing mainnet Dora install and start from there ?

> Pari:
dora is a CL explorer, we already have it working. the issue is the EL explorer, blockscout. we need the ability to tell it a start index block to realistically run it

> Guillaume Geth team:
Ah right, I meant Blockscout indeed. Don't you have a version that follows mainnet, that you could get the db from ?

> Guillaume Geth team:
And by this, you mean that this ability doesn't exist yet ?

> Pari:
nope we don't :/
also im not sure we want to run that - it would likely be ~20-30tb of data

> Pari:
we havent checked if they have it or tested running it tbh :/

> CPerezz:
Was a question by @vdWijden 

But seems he's Ok with Dora. Hence why I signaled in my last message that Dora will do üôÇ

> Guillaume Geth team:
Yeah I would be curious why we need an explorer on bloatnet to begin with

> CPerezz:
@cbermudez97 

Could you take a look to the following stuff I sent to @KamilChNethermind ? He's off I think. And need some help so that I can keep bloating:

> CPerezz:
Also, seems the chain got stuck at current block. I can send txs but a new block is never mined

> CPerezz:
The docker process seems to be working. Restarted quite some times and no luck so far

> CPerezz:
Is completelly stuck at: INFO[0000] processed block 22758966:  54 total tx, 0 tx confirmed from 0 wallets (2.635321ms, 1.986314ms)

> CPerezz:
Should I restart the CL too? Or the machine in general?

> CPerezz:
We lost finality it seems

> MariusVanDerWijden:
Uff

> CPerezz:
Second time already..

BLoating block by block it's proven to be a whole more complex that anticipated. 

High gas limits and tx load just obliterate Geth's RPC which can't keep up.

Low gas limits force a really slow procedure.

> CPerezz:
Also, DB died once already while bloating and we were quite close to the 500GB state mark. And it got competelly corrupted due to some issue in Kurtosis

> MariusVanDerWijden:
How high is the gas limit? Is the network public? Can I connect to it? If not could I ssh into one of the nodes? Would like to see where the bottlenecks are in the rpc

> Pari:
Should we setup multiple machines for the bloating by block approach? we can do one of each client and then use spamoor to round robin it

> CPerezz:
That would for sure help. Multiple Geth instances will aliviate RPC burden. But at 500M gas limit. The RPC just dies no matter what. You can't even propagate mempool txs

> Pari:
Could you point me to the current location the configs are?

> CPerezz:
Is just too high I think. Once @cbermudez97 recovers the instance, we might need to downgrade to 300M or 200M and bloat slower but without issues.

I want to reach the 500GB and store a snapshot. Then we can re-evaluate how to keep bloating in the future once we can start running tests for all the cases we have pending.

> Pari:
How long does bloating to 500gb take at 300M gas?

> Pari:
the benefit of just having all clients is that we can snapshot them all

> Pari:
the current appraoch wouldnt work for erigon/reth right?

> CPerezz:
Should be quite quick if RPC behaves well. I'd say a day or 2 max. 

The issue is that if RPC missbehaves, I can't gather tx success. Neither pending/dropped ones. And becomes a mess to send replacement txs or to keep nonces up to date

> CPerezz:
Yes. Otherwise we would have been already testing since Interop

> CPerezz:
We just have to do this because they don't support snapsync

> Pari:
are you bloating via spamoor? i thought spamoor takes care of replacements and nonces?

> CPerezz:
It does via RPC. But if RPC is satturated, it can't keep up with it

> CPerezz:
THat's the main issue. Is that Geth (AFAIU) isn't designed to support 20-50k pending txs  and recieving 20-50k each 3s or so

> CPerezz:
It just can't handle it

> Pari:
got it :D
so more nodes + lesser gas should do the trick for now

> CPerezz:
Even just lesser gas should do

> ≈Åukasz Rozmej:
Can you do it with less transactions?

> Pari:
In general ill setup something like perf-devnet-1, so it'll be a mainnet shadowfork + unlimited funds + 1 of each EL and CL. Would that work for your usecase?

(so far everything was local with 1 kurtosis node right?)

> CPerezz:
Sure, then will take more time hahahahah

> CPerezz:
That would do yes!

> CPerezz:
http://209.127.228.122:32300/epochs Is the current status of the shadowfork

> CPerezz:
Sfinality lost arround 1h ago when running a db inspection in Geth and the container was stopped

> ≈Åukasz Rozmej:
why? each transaction can write more storage slots, they can do it in a loop to avoid calldata bloat

> ≈Åukasz Rozmej:
I keep forgetting I'm on vacation though

> CPerezz:
For some scenarios I agree. For some others you seemply need a bunch of txs. Not just 1 big one

> CPerezz:
@parithoshj Are you setting up the env? Or I missunderstood you?

> Pari:
Yeah ill setup the env, i need to order servers first :D

> CPerezz:
I highly appreciate!

> Pari:
gimme a few hours, it'll also take a bit to download the mainnet state

> CPerezz:
Sure no worries. Do you want to take the snapshot of the current bloated DB from Nethermind's kurtosis?

> Pari:
I would only be able to start geth from that, so it might be faster to start fresh where we bloat all DBs at the same time

> MariusVanDerWijden:
Whats your tool to bloat @CPerezz ?

> CPerezz:
We had a bloater made by GUillaume which wrote to Pebble directly. So uniformly bloating which was really nice and fast. Sadly, it doesn't support non-snapsync clients as the state-transition needs to be "bypassed" for it to work. And otherwise, if we do it at block 0, the genesis file is MASSIVE.

So as of now, the tooling is a bumch of bloating scenarios I did on spamoor that @pk910 updated. 

They work well under lower gas limits. But 500M gas and 4s block times just killed the whole setup.

> Philipp (pk910):
I can have a look at spamoor when the new bloatnet is up.
but yea, I've also noticed that tx inclusion seems to get worse with very high throughput and gas limits.
TXs are randomly getting dropped from mempools and only get included after several submissions..
Feels a bit buggy,  but honestly not sure if it's spamoor or one or multiple client mempools. 
It's very hard to debug such high throughput only issues üòÖ

> Pari:
bloatnet machines arent yet ready, so blocked on that :/

> CPerezz:
It's clearly geth imo. And it's not a criticism on any kind. It's just not designed to handle such a load

> CPerezz:
And probably not needed TBH. Who runs a single Geth that handles 20-50K txs/3sec?

> CPerezz:
Sure. I'm ready to start kicking ASAP. WIll start again with bytecode and then move to account + storage slots.

Ping me as soon as you have somehting Pari ‚ù§Ô∏è

> CPerezz:
@parithoshj any luck with the machine setup? 

@cbermudez97 any news from your side also?

> Pari:
Nope, the machines arent ready yet - I can ask for an update from the cloud provider but im hoping its ready by tomorrow. 

I'd suggest working on something else for a day or two until stuff is up! I can let you know as soon as its there

> CPerezz:
Thanks so much!

> CPerezz:
Will do as you say then!

> Carlos Bermudez Porto:
Hi. Im creating the snapshot from the new db. Its already compressed just verifying everything is okay before restarting from it and removing the old one.

> CPerezz:
Ping me please so that I can turn it on again ASAP

> Carlos Bermudez Porto:
The kurtosis devnet is running again from block 22758966
Dora: http://209.127.228.122:32300

> Csaba:
You can easily bump (or remove) DoS protection values if you are just abusing it anyway.

> CPerezz:
Will try. Unsure will change much but..

> CPerezz:
Thanks! Have been bloating all night! Working like a charm till now at least!

> Pari:
@CPerezz i have the machines now, do you have a state for me? I can try using that as the genesis state for the new network. That way we start at a massive state and we can grow it even more.

> CPerezz:
I have one if you want it. Can you ssh to 209.127.228.122 ?

> CPerezz:
via root. If you can't @cbermudez97 or @KamilChNethermind will need to grant you permissions. After that, the state we need is in /var/lib/docker/volumes/data-el-1-geth-prysm--cc7272a8dd0c462ab9e8e1988620c86a/_data. If you prefer I can scp it. But will take twice the time to downoad in my machine then upload to the next one.

Tell me what you prefer üôÇ

> Kamil Chodo≈Ça:
Hi guys! Back from break wanted to know the status :)

Are we big? :D @parithoshj are you preparing for syncing tests? Or just trying to spaw new network for now with that?

> Pari:
I can ssh in :D

and how do i stop the bloatnet so that the db is fully flushed and written to?

> Pari:
Yup, i am prepping machines for the sync tests and a more public network. its gonna be called perf-devnet-3

> Kamil Chodo≈Ça:
NICE

> CPerezz:
If you want I can stop both EL and CL when you copy. On the meantime I still bloat. It's proven to take more time than expected. So want to take profit of every second I have lol

> Pari:
what size are we at right now?

> Pari:
because the aim might not always even be bigger the better - if its way too big then we know it'll cause problems and not allow us to observe the degradation phase

> CPerezz:
1.4something TB last time I checked. (of whole DB not just state)

> CPerezz:
So we have arround 100GB of extra state

> Pari:
nice! - we should be at 1.5x state?

> CPerezz:
And a couple massive contracts with huge storage which will be good for certain tests even for Perfnet

> CPerezz:
Approx yes!

> Kamil Chodo≈Ça:
@CPerezz weren't we almost at 500GB state already?

> CPerezz:
Nope. Finality was lost. We lost a day there. Then if you check Dora now: http://209.127.228.122:32300/blocks You'll see Geth doing random stuff like having 10k+ pending txs but shipping 0tx blocks

> CPerezz:
So I've been dealing with this since it started

> CPerezz:
Mempool management is still a mystery

> Kamil Chodo≈Ça:
Shall we try other EL maybe? If we will manage to do sync tests we can easily shuffle, right?

> CPerezz:
Sure that is possible. It's just we are quite close so I'm not sure is worth the time to do the whole setup again.

> CPerezz:
But, as you wish ofc

> Pari:
Yeah. So my plan is the following:
- Ill shadowfork 1x geth into the new network, so it will have the full state
- Ill then spinup 1xeach EL and sync from the main geth
- Once its up, ill redistribute the validator keys so that everyone has a validator
- We can then spin up sync tests and so on

> CPerezz:
I'd make sure we are constantly spamming to all clients syncing (serving state ofc)

> CPerezz:
So that we can see if the heavy load affects them

> CPerezz:
This means we should set a gas limit that we want to test. (45 is marked safe, so we maybe can go for 60M)

> Pari:
Yeah once we're at stable state we can do that

> Kamil Chodo≈Ça:
I'd say that we should:
1. Have perfnet (Pari)
2. Have even bigger gas limit (100-150?)
3. Constant spam WHILE constant resync (so we have spamoor running (but not too heavily) + 5 nodes looped so are constantly syncing over and over again
4. Do snapshots every 100GB of state - so we can preserve it and reuse if needed

> Kamil Chodo≈Ça:
wdyt on above?

> Pari:
sec, ill send my plan post ACD :D

> CPerezz:
Unsure on 3. Constant spam WHILE constant resync (so we have spamoor running (but not too heavily) + 5 nodes looped so are constantly syncing over and over again. The rest seems good!

> Pari:
we have a basic sync test tool up, rafael is working on a better one like the benchmarking tool

> Kamil Chodo≈Ça:
@CPerezz Sp perf-devnet3 seems like could be a new bloatnet - living thing which we can keep on "bloating" - and to understand wherte problems start to occur you need to have kind of monitoring - so whenevr state grows like for example 5GB a day - you have set of nodes which are verifying "Am I able to sync to this still?"

> CPerezz:
Not every 5GB. I'd say every 100-200GB while heavy bloating happens is fine.(At least for now) I say heavy-bloating because this is what is going to trigger compaction. And Compaction + Sync + tx processing smells to catastrophe.

> Kamil Chodo≈Ça:
You mean to try sync once every 100GB of state? Why not having some tooling which simply syncs 24/7 so we can observe at least degradation of performance over time? and make early decisions on for example "stopping bloating" - as further bloatind may not have much sense for now when critical stuff will be found?

> Pari:
pls feel free to bloat till tomorrow morning! i only now managed to finish provisioning the machines, so ill pick back up in the morning

> CPerezz:
Firing till tomorrow!

> CPerezz:
@parithoshj did you stop the network?? Or @KamilChNethermind ??

> Kamil Chodo≈Ça:
Nope

> CPerezz:
I assume it was Pari. There's something in tmux. All my bloater instances are gone and only 1 tab remains with perfnet-4 stuff on it. Will wait to know where to keep bloating üôÇ

> Pari:
Nope, I also havent touched the machine since i checked that ssh worked. I was working on seeing up the new machines

> CPerezz:
üòï

> Pari:
they are up though, so could you stop bloating so that i can move the data over?

> CPerezz:
It's stopped. Someone stopped it already lol

> CPerezz:
if you log in tmux

> CPerezz:
yyou'lll see what i mean

> Kamil Chodo≈Ça:
@cbermudez97 maybe? But I doubt it

> CPerezz:
üñº No, I inspected the DB this morning. And right after left it bloating. And checked back 1h later and was stoped with this

> CPerezz:
Says dora and the rest of stuff is running but It's not. Also killed my 20+ bloating tabs (not a big deal ofc, but annoying)

> Kamil Chodo≈Ça:
ANyone else have access here?

> Kamil Chodo≈Ça:
As I believe it could be either me, Carlos P or Carlos B or Pari

> Kamil Chodo≈Ça:
I wasn't logging to this amchine for last few days

> Pari:
ehh its still up no?

> Pari:
docker logs el-1-geth-prysm--2e6abfa2bc6840a7b9872e58f96a0306 --tail=20 -f

> CPerezz:
Dora seems down: http://209.127.228.122:32300/blocks

> Pari:
Yup, but seems like a dora thing and not a network thing

> Pari:
the CL seems unhealthy

> CPerezz:
I see. Haven't tried to bloat once you said you were working on it.

> Pari:
and as a result not producing blocks

> Pari:
but unsure why the CL is unhealthy

> CPerezz:
This is always the same.. You need to stop both maybe? Otherwise finality is lost

> Pari:
its fine for now, i can stop the EL and CL and take the state and continue on a proper network where its easier to know what's going on

> CPerezz:
Ping once I can restart to bloat there and start bloating in your machines too (if that'st the idea üôÇ )

> Pari:
Yup, it takes a bit to get the snapshot off the machine. ill keep an eye on it and let you know when its done and the new network is up.

> CPerezz:
yupyp. No pressure/rush whatsoever!! It's mostly to make sure we don't forget. THe weekend will be another golden chance to bump the bloating up (I've done some adjustments i hope will make bloating faster).

> Kamil Chodo≈Ça:
@CPerezz Did you made any conslusion on state structure to comapre it with mainnet?
Like for example on mainnet we have X accounts - on bloated - y etc

> CPerezz:
The ratio is almost the same as mainnet. Meaning, if we jumped 2x in size, we have exactly 2x contracts 2x accounts and 2x storage slots (although within storage slots the distribution is different as I've been trying to exploit having lartge-state contracts such that we can perform worse attacks than XEN

> CPerezz:
@parithoshj what's the status? Do you have a server I can use to bloat your machines? Also @KamilChNethermind @cbermudez97 what's the status on your server? I see that Geth is not running there.

> Pari:
its still moving data around, ill let you know once its up. But please dont start up geth or do any bloating right now. I need the data to be in its current frozen state till the operations are done.

> Kamil Chodo≈Ça:
I'm not touching it at all

> CPerezz:
Saw the data tranfering is done @parithoshj 

LMK once we can start the CL & EL to continue the bloating! Weekend will be a good time to do so

> Pari:
Yeah thats the first bit, now i have to still start a network from it. I wont be doing it over the weekend, ill set it up first thing on monday. IMO having 2x state is already enough, we should first stop here and analyse how stuff performs, it wont really help if we keep growing the state without getting the other parts of the puzzle working - RPC testing suite, sync test suite, benchmark/spamoor/analysis on performance.

> CPerezz:
Sure! But we aren't at 2x mainnet. Hence why I wanted to keep bloating during weekend.  It's fine if your machines are down. Just want to know if I can start back the ones from Nethermind. (The docker basically)

> Pari:
You can start back the nethermind ones now if you want, i have the state i needed now that the transfer is done

Although im not sure what the point of continuing to bloat that state is, the aim is to get it into a network right? and i just took the snapshot for the network, unless the idea is to keep bloating in this tool and restarting new networks in the future with larger states. But at that point you can also just bloat the network i setup anyway

> Pari:
theyd both grow at roughly the same pace

> CPerezz:
Mostly I want to try my changes to the scenarios to see if I can make them work better wit hthe RPC issues..

> CPerezz:
Thanks Pari!

> Pari:
Sounds good! feel free to reown the machines in that case!

altho since the nodes were offline for so long, you would have a bunch of empty slots and it might be hard to start the CL world up again. Its worth a try though, it should eventually build on itself

> CPerezz:
Hey @parithoshj! Any ideas when we can have the new shadowfork exposed for bloating?

> CPerezz:
BTW, as you predicted this was the case and CL is KO.

> Pari:
I messed up the download earlier today and had to restart, it should be done in a few hours and i can setup the network then :D

> CPerezz:
Awesome! I modified my bloating cases on several different ways to see if RPC-behaviour is a bit better.

> CPerezz:
Also @vdWijden unsure if that's interesting to Geth team or not. But RPC behaviour at this gas-limits and Tx-sending rates is quite bad and unpredictable. This ofc is not a prio, but just to let you know. Happy to chat about it anytime.

> CPerezz:
Hey everyone. We will start the first round of sync tests within the next few days (likely tomorrow or even today). For that, I wanted to make sure that all clients have some way to record the following data points for a follow-up study:
- Healing phase duration (if snapsync is supported)
- Overall sync duration
- Start-End DB size
- R/W performance for all ops.

@KamilChNethermind @mlnfltv @s1nam @gakonst @kt2am  can you confirm if you have ways to collect this kind of data and if so, do we need to enable anything special?  (Sina I know for geth @delweng had a ton of markers in his grafana panel. so we should just need to reuse it probably).

> Kamil Chodo≈Ça:
For Nethermind Healing phase duration there is no such metric afaik but very easy to get from Logs
Sync duration is a metric as far as I know
We can also extract times of each stage moreless from grafana (also form logs)
DB size we have with a info about each sub-db size
R/W kind of we have I believe - @ben_a_adams can you confirm?

> Pari:
Bloatnet status Its now up!
https://dora.perf-devnet-2.ethpandaops.io/

The geth DB is 1.3TB, so ~1.5x mainnet (1.3T ./geth). I'm starting to sync up all other clients now, so that we have a baseline at 1.5x 
mainnet. Once that's done, we can start state bloating again and observe access pattern degradations as well as sync related degradations.

And just to explain the naming scheme and purpose:
- perf-devnet-1: Compute related bottlenecks with existing state size
- perf-devnet-2: State bloat research and bottlenecks

> CPerezz:
Coud we get this somehow in a branch such that we can get all these metrics? As Pari correcrly suggested, it would be nice to have a "baseline" for all ELs. And to get it, we need to be able to collect the data.

> Pari:
Grafana dashboards should already have most of this data tbh, i guess the main difference will be sync stages. But yeah if clients already have an established way they test and verify sync - we're happy to just reuse it

> Kamil Chodo≈Ça:
Hmm not sure if I follow - we have quite nice dashbaord which should work out of the box with a lot of valuable insights - do you want to have some minimalistic version with only sync/db related things pulled out?

> CPerezz:
Mainly we want to know what % of time is healing . Also, R/W perf is important too (I assume most teams already have this one).

I'm worried about healing specially beceause when we test upwards, with higher gas limits and state size, compaction + healing can just kill the sync. So would be nice to have a baseline now. 

If it's a lot of work on whatever we can skip ofc. But thought it would be a nice-to-have

> Kamil Chodo≈Ça:
@asdacap we are attempting bloated state sync and want to ensure we have proper metrics in place - do you know how hard it would be to add healing part sync time metric?

> Georgios Konstantopoulos:
@rkrasiuk

> milen:
For Erigon we can get overall sync duration from the logs for now, db size is easy to get, r/w performance for all ops - we will be improving metrics for these over the next few weeks (depending on bandwidth). From my perspective, feel free to begin. We can always add more metrics and re-sync again (e.g. by joining with a new node) so the metrics can be re-gathered and iteratively improved from our end.

> Pari:
Synctest results:
Nethermind & Erigon synced already, yet to dig into what data is available and what is missing. 

Besu & Reth are syncing (reth is almost done). However Besu was started with full sync, so im going to correct that and let it default (which is how nethermind/erigon were run).

> Kamil Chodo≈Ça:
Did you made Geth sync with Snap also?

> Kamil Chodo≈Ça:
Hmmm seems like bloating is not sufficient enough?

Fresh mainnet state on Nethermind - 191GB
Blaotnet state after fresh sync - 230GB

> CPerezz:
Pari wanted to get baseline metrics for low streess at 1.5x mainnet.

> CPerezz:
But ye, we want to get to 2x bloating after we have these baseline metrics collected and stored

> Kamil Chodo≈Ça:
Just this is quite far still from 1.5 is what i mean

> Kamil Chodo≈Ça:
At least based on what i see in nethermind metrics

> CPerezz:
Let's take this as Base mainnet metrics then

> CPerezz:
And go for 2x for the first round after finishing baseline metric collection.

> Kamil Chodo≈Ça:
@parithoshj 
An idea for a proper comparision to mainnet.
Once we will have 2x state we should have 2 nodes for each client pair - one on mainnet and one on bloatnet.
Then enable a tooling which propagates mainnet blocks to bloatnet (as in theory they should be applicable - but maybe not really so to be checked).
Then compare performance of them 1-1 without any difference so we have a clean apples to apples results here

> CPerezz:
> Then enable a tooling which propagates mainnet blocks to bloatnet (as in theory they should be applicable - but maybe not really so to be checked). 

I worked on this for some days. We will need to start reproducing from the shadowforking block from mainnet. (We can't tget current ones). 
Also, another thing is we need the signed txs that originated the transitions. As otherwise, we lack the private keys.

Should be doable. But requires careful design. We wanted to do this to have a "realistic bloater" according to mainnet txs

> Pari:
Yeah agreed :/
So is the bloating tool wrong or did we calculate wrong?

> Kamil Chodo≈Ça:
I believe bloating is correct - just maybe some Geth metric while checking the current size is obfuscating that a bit?

> Kamil Chodo≈Ça:
But now having bloatnet as a scaled network we can spam better?

> Kamil Chodo≈Ça:
@parithoshj what is slot time and gas limit here?

> CPerezz:
Does it matter much? Meaning, now we have this baseline, Next stop is 2x. It doesn't seem to be that bad. 

And we will be able to bloat much better with multiple RPCs open (if that is the case) otherwise, I have also modifiied my scenarios to try to adapt to Geth's RPC behaviour

> Pari:
We have a couple of options now:
1. Just continue bloating, i.e do what we were doing before and stop once a week or so to perform tests like checking execution times between mainnet and bloatnet
2. We can keep it at this stage and just replay mainnet and compare diff between the two

> Pari:
slot time is the same, gas limit is 100M

> Kamil Chodo≈Ça:
Just not sure how this state size is better in regrds of "baseline" comapring to mainnet pure shadowfork?

> CPerezz:
Will be arround the same most likely.

> Kamil Chodo≈Ça:
Not to say it is bad - it is great that we managed to push that even tho having some issues with network

> Pari:
IMO the state is still not big enough for meaningful tests, we should aim for it to be atleast 2x before we stop and do the next round of tests

> Kamil Chodo≈Ça:
But I think we can start having any assumptions on 2x?

> Kamil Chodo≈Ça:
Let's bloat here now and measure - we have more cleints so more metrics to monitor

> CPerezz:
Can I get access to the machines @parithoshj so that I can setup the bloating?

> Pari:
I will wait for reth and besu to finish syncing. Once they are at head, i have to reorg the validators. Once thats done we can start bloating again.

> Kamil Chodo≈Ça:
https://spamoor.perf-devnet-2.ethpandaops.io/

> Pari:
Ill reach out once we're done. In the meantime @KamilChNethermind can you check if all the sync related metrics for nethermind exist? is something misssing?

> Pari:
ie this

> Kamil Chodo≈Ça:
I can see that for Nethermind on this spec SnapSync took 36 minutes + Healing which took around 40 seconds.
Now History backfill is happening but node is on tip and processing blocks well and very fast (I see in avg we have about 20Mgas blocks now)
State DB as I mentioned is 230GB while total DB size (with likely dropped partial history) is 849GB
DB Reads at peak were around 600k ops/minute but it was during sync - on tip now while backfilling history is around 23k ops/minute
DB Writes during sync peak at 350-400k ops/minute - now during backfilling is around 40 ops/minute

> Kamil Chodo≈Ça:
What I'd be really interested in is for example a test where I make a baseline on mainnet with snapserving from single peer and then comapre the same on bloatnet

We know in nethermind that our snapserver is much slower than Geth implementation and wanted to know if 2x state will cause that we will be 2x slower or rather 4x slower

> Kamil Chodo≈Ça:
Wait nevermind - current performance is not with backfiling :) Node is fully synced with all the history :)

> Kamil Chodo≈Ça:
all the history we wanted - so without premerge

> Pari:
Awesome, so plan of action is still:
- Wait for besu/reth sync
- Reorg validators
- Bloat state again

We can do sync tests in parallel with a different set of nodes. We're working on a better sync test tool.

> CPerezz:
Awesome.

Do we also want to gather performance metrics for state-worst cases? Max EXTCODESIZE exec for example as well as other stuff.

Or we want to do sync-only first?

> Pari:
Do you imagine that we'd see a big deviation from mainnet at these state sizes? or is it to just collect the metric for future comparisons?

> Kamil Chodo≈Ça:
We are working on gas-benchmarks "v2" - where we can work on top of any fork and any state as an input.

With that we can prepare real "benchmarks" and analyze the data like this - no need to run that on netwrok as it adds a lot of extra noise.

> Kamil Chodo≈Ça:
Still I'd suggest to have a set of nodes (like 5 for each EL) and sync them constantly 24/7 and just capture very basic things like "dir size", "total sync time", "network usage", "disk utilization" and just let it go while bloat will be happening.

Shouldn;t affect bloating in any way as these nodes will be "Separate" - non validators just syncing the chain.

We may capture some interesting things like slow degradation of sync speed, increased disk usage etc and maybe if bloating will be happening there will be some intyeresting "spikes" - so we will maybe be able to corrrelate load used at particular moment to these spikes and find out sync edge-cases (for example high networtk utilization will make healing part to stuck and never be accomplished)

> Kamil Chodo≈Ça:
From past experience in Nethermind phrase "Infinite healing" was something scary but was happening from time to time :D Now we live in better time-line :D 
Curious if we can stress test to the level this will start happening again

> CPerezz:
The latter. I don't expect much deviation

> CPerezz:
Would be important to capture snapshots every, say 100GB or so.

> CPerezz:
So that we can always have them in case we want to get back to that size and test something particular

> CPerezz:
So we have a reproducible state env

> Pari:
Yup this is exactly the plan, syncoor should be ready soon and will do exactly this. It'll also help us track historic sync metrics and compare it for the future and allow us to run it on mainnet + devnets.

> Pari:
We also already have this: https://ethpandaops.io/data/snapshots/

we will be pruning history by default soon and we can add perf-devnet-2 to the mix. Its just a question of cost/benefit.

> Kamil Chodo≈Ça:
Issue is - this will be costly especially if you want to keep it for each EL :D Otherwise you need to start network, sync other client, grasp database - which will be costly as well :D

> Kamil Chodo≈Ça:
Another interesting observation from bloatnet.

Seems like 40GB of state growth somehow causing only 30GB of general DB size increase :D Not sure what happened here but Receipts size is smaller on bloatnet comparing to freshly synced mainnet on Nethermind :D Need to investigate that. Probably Receitps being produced on mainnet are much much heavier than what we were producing on blaotnet with artificial state-aimed load.

> MariusVanDerWijden:
Maybe the db is doing some compression magic

> MariusVanDerWijden:
Are you bloating with compressable data

> Kamil Chodo≈Ça:
@CPerezz ?

> Philipp (pk910):
receipts contain log data. I guess the bloating scenarios emit way less logs than usual mainnet transactions?

> CPerezz:
Just 1 EL is ok I'd say, We can sync them all afterwards

> CPerezz:
Exactly. THat is the case.

> CPerezz:
Bloating scenarios don't log anything almost. I just get the txs mined after each block and take the data from there to show logs in Spamoor.

> CPerezz:
@parithoshj lmk when Erigon is done syncing and I can bloat to 2x mainnet size plz!

On the meantime, is it better to write the state-attacks like XEN/EXTCODESIZE spamming with EELS or spamoor?? @vdWijden @KamilChNethermind  would be interested on your take here.

> MariusVanDerWijden:
I think whatever works better for you. We will continue to maintain both of them going forward

> CPerezz:
The point is: Do we want to run isolated runs of the stressing txs? And measure them as in perfnet? Or do we want to force the network to eat up these txs over and over (so that we can see how compaction affects them for example)

> CPerezz:
The ideal scenario would be that spamoor integrates EELS

> MariusVanDerWijden:
I think @pk910 is already working on integrating EELS in spammor

> Pari:
Erigon has been done already, Reth also caught up. Besu is still syncing, ill already move around the keys so we aren't waiting on that. Lets give besu a bit more time and we can start spamming again after.

> Kamil Chodo≈Ça:
Dowe want to capture anything for Besu prior to spamming?

> Pari:
Mainly to get a baseline for sync time

> CPerezz:
@parithoshj this sync was done at 27M gas/block on avg IIRC no? I wonder if we want to start targetting target and gas_limit values for the new limits that validators are signialing. (So we are ahead of time already)

> Pari:
We will, but at 45M gas limit that is ~22.5M gas target. So the baseline is valid for a while atleast. We'd ofc redo the tests with higher limits too. There's enough tests to do, we just have to sequence it properly.

> CPerezz:
I'd do gas_limit also aside from target. (Specially since syncing is taking 1 day at most for now). I expect to see mroe copcation happening there and more interesting data and situations

> CPerezz:
EELS it is then

> Pari:
Okay I think Besu might take 1 more day to finish syncing. Perhaps its fine to move ahead with some basic data collection and state growth again without waiting on them?

I'd propose that today if @CPerezz has time, he could collect metrics on state attacks with XEN/EXTCODESIZE. Then he can restart his spammer and we can see how much we grow the state over the weekend. In the meantime Besu should be synced on monday.

> Kamil Chodo≈Ça:
100% good - I think we can always asynchronosuly collect mainnet data and will be good enough baseline (we can use same hardware).

We see at least it didn;t impacted some of the clients for now with such a grow but we need to have as accurate 2x as possible to test that out and make proper state assumptions.

> Ben {chmark} Adams ‚ü†:
where can I get connection details for bloat net to join a node?

> Pari:
https://perf-devnet-2.ethpandaops.io/

> Pari:
I gotta trigger an update to add more enodes and enrs, i can do it in a bit. but it shouldnt be a blocker for you.

> Guillaume Geth team:
Carlos is out today, FYI

> Pari:
Pks setup state bloater to target ~300M gas on perf-devnet-2. So we should be good for the weekend.

> CPerezz:
Heyhey. I have been quite sick. But back at it today.

@parithoshj still wanna run Extcodesize & XEN? Or wanna booat to 2x?

> CPerezz:
If we are runnuing the full suite, I'm also interested on compaction axtivation frequemcy as well as duration

> Pari:
Sorry to hear that! Hope you feel better soon!

Weve actually already been bloating over the weekend, so just gotta check what the levels are for now. If it‚Äôs working + the approach is fine - then we just leave it as is till we hit 2x

> CPerezz:
Has the ratio of the state being maintained for 2x target?

> CPerezz:
Ie. Accounts-Bytecode-storage ratio

> Pari:
Its the sstore bloater from spamoor

> CPerezz:
kk, We will need to turn on the account bloating too. We should be at ~25GB

> CPerezz:
Can I access the machine @parithoshj ? Or how should we proceed?

> Pari:
Ah gimme half an hour, I‚Äôll be at a pc and check access

> Pari:
I thought i had done it already

> Pari:
You should have access now @CPerezz

> Pari:
üñº ssh devops@<cl>-<el>-1.perf-devnet-2.ethpandaops.io
list of hosts here: https://github.com/ethpandaops/perf-devnets/blob/master/ansible/inventories/devnet-2/inventory.ini

> CPerezz:
In. what machine do you have the bloater running?

> Pari:
its on kubernetes, if you want a specific image run you can let me know and i can run it. but URL: https://spamoor.perf-devnet-2.ethpandaops.io/

> CPerezz:
Ideally I need access to the logs of the scenario to see if RPC is being a pain or not. 

Hence why I wanted a terminal to spam. 


Not familiar with kubernetes (aside from the very basis). Any way to get CLI access such that you can see also the spamoor dashboard with my stuff?

> Kamil Chodo≈Ça:
Spamoor UI gives you sceanrio logs just fyi

> Pari:
Yeah anything you could do on the terminal you should be able to via the UI as well. Its just more maintainable for us since its a shared place for the configs to live and we all know whats being spammed/modify it if needed.

> Pari:
If you need a custom fork of spamoor i can deploy it for you, but you can import your own scenarios if you like.

> CPerezz:
Ohh so It has to be YAML-based? Then, this doesn't allow me to setup completelly new scenarios in my fork right? 
Ideally (since we saw RPC is an issue at these tx rates) I wanted to be able to test, modify, pull and test again. Such that I can see what stalls and what doesnt the RPC for an optimal bloating. 

Same for testing the attack scenarios.

> CPerezz:
Can I still upload scenarios from my custom fork and all that stuff?

> Pari:
If the only difference in your fork is the scenario, then you should be able to use it as YAML. 

If its a fully new codebase that's needed to accomodate the scenario, then while we wait for it to get upstreamed - i can deploy a fork for you. 

But you won't be able to rapidly iterate, there are 5 EL RPCs and we're able to spam at ~300Mgas/sec without issues for ~3D now. 

If you absolutely want to run your own fork on the terminal, i can give you some funds and you can run it off of your machine while we upstream changes

> CPerezz:
Will try to setup the YAML. Also, do we have a way to monitor state size @parithoshj ? With some detail as with geth db inspect. Mainly to be sure that we maintain the ratios for account/storage/bytecode + stop when we are at the right size.

> Pari:
hmm, I can setup an alert - what's the geth metric to track?

> CPerezz:
Unsure if it's in Grafana. We should track account size, bytecode size and storage size. And the ratio we target is: ~25GB, ~25GB, ~250GB.

> CPerezz:
@s1nam do you know if Geth's grafana exposes that?

> MariusVanDerWijden:
We don't expose it

> Pari:
as in there's no metric to expose this data at all?

> CPerezz:
Any way to do so? Or do any other teams expose this? cc: @KamilChNethermind @kt2am @mlnfltv @rkrasiuk

> CPerezz:
Is it because you have a lock on the DB? Same reason why you can't inspect while running the node?

> Roman Krasiuk:
yes, we do expose table sizes in reth

> CPerezz:
@parithoshj we can monitor from Reth's grafana then.

> MariusVanDerWijden:
Yes, we would need to iterate over all the keys/values which takes a pretty long time

> Pari:
dashboard here: https://grafana.observability.ethpandaops.io/d/2k8BXz24k/reth?orgId=1&from=now-12h&to=now&timezone=browser&var-network=perf-devnet-2&var-instance=perf-devnet-2-lighthouse-reth-1&editIndex=1

> Pari:
Could you check if this is the data you were expecting?

> CPerezz:
üñº QQ/ask to all teams.

We are on our way to 2x mainnet state. We will be arriving soon. Thus, this is another reminder to teams to please provide a way to retrieve/log/record the following data points:

## For Syncing:
- Healing phase duration (if snapsync is supported)
- Overall sync duration
- Start-End DB size
- R/W performance for all ops.

## For DB compaction (or any similar procedure if applies)
- Compaction start/end time.
- IO perf when and when not in compaction.


Compaction is a really important metric. And core part of BloatNet explorations. So please can you say if you support or not giving such metrics and if not, what would it take you to do so?

cc:  @rkrasiuk @kt2am @mlnfltv 

@KamilChNethermind IIRC, we can grab yours from Nethermind's logs right? We can def do so. Though would be easier to have it in Grafana.

@vdWijden I know we have this as is shown in https://grafana.observability.ethpandaops.io/d/Jpk-Be5Wk1/geth-dual?orgId=1&from=now-1h&to=now&timezone=browser&var-exp=gballet-staking-1&var-master=https:%2F%2Fgrafana.finance.ethpandaops.io&var-percentile=0.5&refresh=5m. I wonder if there's slightly better IO performance metrics anywhere. Similar to https://github.com/jsvisa/goleveldb-bench/blob/pebble/pebble-2.md#ethdb-pebble-metrics

> CPerezz:
It does! @rkrasiuk can you confirm if bytecodes represents the whole tree or simply the raw bytecode sizes? It's at 30GB while for Geth is at ~20GB.

> MariusVanDerWijden:
Geth:
- Healing phase duration: logs
- Overall sync duration: logs
- Start-End DB size: need to stop node and run geth db inspect
- R/W performance for all ops: we only have per-block performance in the geth-grafana
- Compaction start/end time: geth-grafana
- IO perf: geth-grafana

> Kamil Chodo≈Ça:
Nethermind:
- Healing phase duration: logs
- Overall sync duration: logs + metric "nethermind_sync_time"
- Start-End DB size: metric "nethermind_db_size" which contains a switch to understand partial sizes (but not as detailed as one from Reth probably)
- R/W performance for all ops: metric 
- Compaction start/end time: @ben_a_adams ?
- IO perf: This needs to be added separatelly afaik

> Roman Krasiuk:
Reth

## For Syncing:
- healing: n/a
- overall sync duration: added reth_sync_total_elapsed metric last week (https://github.com/paradigmxyz/reth/pull/17321), should be a sum of all those across stages
- start-end DB size: there is already Database tables pie chart on the dashboard which accounts for mutable database data. the total DB size would be sum(reth_db_table_size) + sum(reth_db_freelist * reth_db_page_size) + sum(reth_static_files_segment_size)
- R/W performance for all ops:
tx open duration by type -

sum(rate(reth_database_transaction_open_duration_seconds_sum{outcome!=""}[$__rate_interval]) / rate(reth_database_transaction_open_duration_seconds_count{outcome!=""}[$__rate_interval])) by (outcome, mode)

flush to disk time -

avg(rate(reth_database_transaction_close_duration_seconds_sum{outcome="commit"}[$__rate_interval]) / rate(reth_database_transaction_close_duration_seconds_count{outcome="commit"}[$__rate_interval]) >= 0)

> milen:
we just noticed db bloat in erigon which may indicate that our prunning is not keeping up with the bloating - it is just a theory for now - we will need to add some more metrics to help us capture this

> Giulio:
gm

> CPerezz:
Having R/W perf is more than enough for now. That already allows to see how compaction degrades block root hash computation due to slower state writing/reading.

> CPerezz:
Any news on that?

> CPerezz:
@FunnyGiulio @kt2am any input on this? The other teams already supplied the info.

> milen:
Im in the process of adding metrics.

> CPerezz:
Would you need us to re-launch sync for Erigon to double check?

> milen:
No need, i have access to the machine

> CPerezz:
@pk910 In the scenarios running now (ERC20, SSTORE & BLOB) is there any that constantly creates contracts? We are over 18GB of data in bytecode from where we should be.

> milen:
But this will take a day or two just fyi

> CPerezz:
Sure no worries. I ask since we might halt sync tests for 2x mainnet size until you can either fix or add the metrics needed for the analysis. Otherwise we will just waste time

> Philipp (pk910):
no, there were no contract deployment scenarios running afaik. are you sure this is deployed bytecode only?

> Simon | Besu:
üñº Yep should be mostly doable in besu using grafana https://grafana.com/grafana/dashboards/16455-besu-full/

And logs. Will share specifics tomorrow if necessary.

> CPerezz:
Yup. Unless reth's Grafana bytecode field doesn't only give bytecode size but also intermediate DB nodes

> Matkt (Karim):
@siladu will provide this informations

> Pari:
üñº Seems like we're at ~1.5x size now. Did you get your spamoor working @Cperezz ?

> Kamil Chodo≈Ça:
@parithoshj do you have a quick command to trigger nethermind resync? In Nethermind DB size (the actual one) can be checked only on fresh sync soi would be nice to check that

> Kamil Chodo≈Ça:
OR - maybe even better option - will test FullPruning here :)

> CPerezz:
I touched some of the spammers that are running. Since I have no access to CLI and can't iterate fast, I don't want to add my spam cases as my priority is to not slow down the bloating on any way.

Since the SSTORE is working fine, I have 2 things in mind:
1. Stop the SSTORE bloater and setup the delegation 7702 txs (to bloat accounts). Once we are at ~25GB of account state, restart storage and let it run until we are done with 2x.
2. At some point (last 5-10% of bloating left), bloat XEN contract or some other creating a really big state for it. This is because I want to have tests that touch really big contracts in peculiar ways to check if effects on state root computation are noticeable.

> Pari:
its easiest if you use ansible ad hoc commands, but since its just one machine you can also ssh in and do it manually. docker stop, rm the /data/nethermind folder and then docker start

> Pari:
1. Why do we need to stop one to do the other? can't we do both in parallel?
2. Are we able to bloat xen contracts ourselves?
> Philipp (pk910):
2.  yea :)  I've actually added a scenario that can replicate that long running mainnet tx with the same tx style  and targeting the mainnet xen contract (on the SF)
can set it up if wanted?

> Pari:
Yes please! could you add it as a scenario in the existing spamoor?

> Philipp (pk910):
the current SSTORE spammer reuses the same contract too.  so whenever running the scenario it increases the state of one specific contract.

> Philipp (pk910):
xen is probably still bigger :D

> Pari:
would it make sense to bloat 3/4 via our own contract and 1/4 just bloat xen?

> CPerezz:
1. I said it to just be done with accounts and not needing to monitor constantly. But I left both with modified fees to prioritize account bloating and fill in with SSTORE for the rest.

2. Unsure. I'd say yes definitely as any user can transact into it I'd say. (we can forward any mainnet tx we see into the network anyways as they will be valid.

> CPerezz:
Is it customizable on any way? Also, my goal is to have a "bhuncky contract" in regards state size. It can beXEN (which is already massive or it can be another one, no preference. XEN just seems bigger)

> CPerezz:
Ye. I mean, it won't create a new level in the tree. But it's still interesting to see what will happen when contracts reach massive sizes. (Now XEN is the closest that allows us to test this, so we can just exagerate it even further).

> Philipp (pk910):
perf-devnet-2 struggles with finality again.
looks like erigon (partially) & nethermind (fully) missing

> Kamil Chodo≈Ça:
checking

> Philipp (pk910):
XEN contract bloater is running :)
It's customizable as the other scenarios, so you can set throughput / pending & the amount of gas to burn with bloating per tx

> Kamil Chodo≈Ça:
@ben_a_adams 
Another DB corruption - it is getting serious now

> Philipp (pk910):
lighthouse-reth seems to miss blocks, but attestations work.
seems like a engine call times out:
Jul 16 09:51:02.006 WARN  Execution engine call failed                  error: HttpClient(url: http://snooper-engine:8561/, kind: timeout, detail: operation timed out)

> Kamil Chodo≈Ça:
@parithoshj @pk910 not sure if this is Nethermind issue or Hardware issue with this one machine... It is suepr rare to have 2 DB corruptions over few days

> Kamil Chodo≈Ça:
But may also be sth about this state

> Kamil Chodo≈Ça:
Purging db and resyncing

> Pari:
^ Yeah we have ~10 similar machines and in general its a good machine, so i'd imagine its a nethermind issue... also the CL on that machine seems fine right?

> Kamil Chodo≈Ça:
I'll sync a side node on our infra and will keep on monitoring it

> Pari:
lighthouse-reth is also super interesting, it is keeping up with the chain but failing to propose

> Pari:
Jul 16 10:04:38.006 WARN  Execution engine call failed                  error: ServerMessage { code: -38001, message: "Unknown payload" }

> Pari:
operation timed out with unknown payload

> Kamil Chodo≈Ça:
That is something I seen in past i think

> Kamil Chodo≈Ça:
How slow are the blocks execution wise?

> Pari:
Depends on the client, some are at ~600ms, some at 2.1s

> Kamil Chodo≈Ça:
Not that long then... hmm

> Philipp (pk910):
it's probably related.
the followup message after the timeout is interesting too:
Jul 16 10:05:26.003 WARN  Execution engine call failed                  error: HttpClient(url: http://snooper-engine:8561/, kind: timeout, detail: operation timed out) 
Jul 16 10:05:26.003 WARN  Error processing HTTP API request             elapsed_ms: 2002.4736810000002, status: 400 Bad Request, path: /eth/v3/validator/blocks/57772, method: GET

there's always this HTTP API failure right after the operation timeout,   with exactly 2000ms (+ 2-5).
so it seems like lighthouse has a timeout of 2000ms for the engine call (probably getPayloadV*)

> Pari:
all 2+s blocks: https://dora.perf-devnet-2.ethpandaops.io/slots/filtered?f=&f.pname=lighthouse-reth-1&f.missing=1&f.orphaned=1&c=50&d=1+2+3+4+5+6+7+8+9+10+18+11

cc @rkrasiuk

> Pari:
Yeah let me ask what the timeout is for them, 2s seems interesting

> Kamil Chodo≈Ça:
So may be actually lightouse?

> Kamil Chodo≈Ça:
BTW - Nethermind resyncing now, progressing fine

> Kamil Chodo≈Ça:
trying to sync mine node in meantime

> Pari:
Confirmed, lighthouse has a 2s timeout on that call. We'll figure out why it exists. That being said, would still like to know why reth is taking so long on these blocks

> Philipp (pk910):
well,  we have 1,000M gas per block :D   I guess that's expectable at some point

> Philipp (pk910):
I'll gonna change the running spamoor scenarios a bit to avoid filling up blocks completely.
It worked way better with only half full blocks. Better for the basefee too ;)

> Pari:
üñº Yeah the spec for the timeout is 1s, which feels just insanely low and based on benchmarks would already break at 36M gas. Lighthouse has 2s in their codebase, Prysm has built blocks with ~3.9s as well. So clearly the spec is outdated and clients aren't following it. 

https://github.com/ethereum/execution-apis/blob/main/src/engine/prague.md#engine_newpayloadv4

Ill find out what all the clients have as current timeouts and propose a new timeout to be ~3s. We should ideally have it such that there's enough time for the block to still propagate before the attestation deadline.

> Philipp (pk910):
I've changed the scenarios so we have:
50% SSTORE bloating
25% EIP7702 authorizations
25% XEN Bloating

targeting a total of ~600M gas

> Philipp (pk910):
nimbus-besu builds quite a lot empty blocks.  they don't contain any transaction.
other blocks from nimbus-besu look fine,  so feels a bit buggy?

> Pari:
bad mempool behaviour? that would also explain why besu blocks look to execute fast

> Pari:
cc @siladu

> Philipp (pk910):
mempool should always have some transactions.
it looks like a all or nothing bug:
https://dora.perf-devnet-2.ethpandaops.io/slots/filtered?f=&f.pname=nimbus-besu&f.missing=1&f.orphaned=1&c=50&d=1+2+3+4+5+9+13+10+16+18+11

> Kamil Chodo≈Ça:
I think above we were anyluizing some time ago with @amezianehamlat - it may be also timeout during block building so if it happens - empty block is being produced

> Kamil Chodo≈Ça:
Generally we should sit together among all EL teams and discuss better approach to block building under heavy circumstances. It is always better to build something rather than nothing - so there should be some sort of procedure on how to build to avoid by any cost empty blocks especially when there is huge load on network which we need to handle

> Pari:
Yes i think we need to align the timeout properly across clients

> Philipp (pk910):
Ideally el clients should enforce not exceeding the timeout and cancel block building with whatever they have so far,  so we get half full instead of empty blocks :D
But I think that's not how block building works internally üòÖ

> Kamil Chodo≈Ça:
I think we could came up with more sophisticated solutions - not only to rely on static timeout but try to capture metrics for each transaction and give some additional input for block building so nodes may know a bit upfront what is a performance of each transaction comapring to potential gains

> Kamil Chodo≈Ça:
We improved on it quite a lot recently - doing much more frequent improvements. 
And also came with a theory where you may not necessarily need to rebuild block from scratch for next improvement but improve block iteratively on top of previous iteration. But is a huge thing to overcome.

> Kamil Chodo≈Ça:
Would be great to have nodes which are specialized only in benchmarking transactions in network and giving via p2p info on execution time of selected transactions :D So during block building you will have another variable to consider (so cheap gas wise transactions which are heavy to execute may be less likely to be included if not "tipped" properly)

> Kamil Chodo≈Ça:
This could be used only by local builders as mev builders may not want to do that - and at some attack situations everything will fall back to local building :D

> ahamlat:
Yes, the investigation is still ongoing on this issue, cc @spacio1999

> Ben {chmark} Adams ‚ü†:
which branch is it on? Can at least pull in the hard stop change; see if that prevents needing to resync each time

> Kamil Chodo≈Ça:
performance should be

> Pari:
Yup ethpandaops/nethermind:performance

> Ben {chmark} Adams ‚ü†:
ok, bringing it up to date; won't fix the issue; but may help it be more recoverable

> CPerezz:
Can we leave it as it was until we have the 25GB in accounts?

> CPerezz:
Then we can modify whatever

> Pari:
It seems that with those loads, lh-reth wasnt able to propose. So going lower means more blocks and in the end you hit 25GB accounts faster since there are more inclusions.

> Kamil Chodo≈Ça:
Guys - keep on bloating by any cost

> Kamil Chodo≈Ça:
I think we have some issue in Nethermind

> Kamil Chodo≈Ça:
Where I cannot properly start syncing or it hangs

> Kamil Chodo≈Ça:
May be likely due to load

> Kamil Chodo≈Ça:
May be also due to shoadwfork and problem ensuring peers but not sure. Now it started prohgressing

> Ben {chmark} Adams ‚ü†:
I think it picked up updated mainnet pivot block from my reset of performance branch (edit: have reversed it 3 weeks)

> CPerezz:
Ohh I was just arguiung about distribution rather that load per se.. But sure, will monitor more often

> Kamil Chodo≈Ça:
@ben_a_adams This must be some super valid issue here just spotted exactly where I expected.
Healing phase of Nethermind node simply stucks on such heavy load.

> Kamil Chodo≈Ça:
Wondering what would happen for other snapsyncing clients in current bloating phase

> Kamil Chodo≈Ça:
But for now need to restore finality likely

> Kamil Chodo≈Ça:
@CPerezz Can I stop bloating for a bit of time?

> CPerezz:
That confirms our suspicions @asqyzeron

> CPerezz:
Sure.stop the 4 scenarios. I can also modify the load

> Kamil Chodo≈Ça:
Wondering if this is reproducible on empty state but with heavy load

> CPerezz:
That's one more case that goes i to the testing pipeli e

> Kamil Chodo≈Ça:
Think should be easily reproducible on Kurtosis network - will give it a shot today

> CPerezz:
Can you elaborate a bit more(maybe even later) the issue? I want to have a log of all of the findings

> Kamil Chodo≈Ça:
I mean how you can notice it happens is that node keeps on spamming with these logs:
Changing sync StateNodes to FastSync, StateNodes
Changing sync FastSync, StateNodes to StateNodes

So it simply stuck on sync phase "StateNodes" - which usually on mainnet takes 4-5 minutes.
Here it is on this stage for already 12h

> Kamil Chodo≈Ça:
Stopping bloating for now to resync Nethermind and revive the chain

> Pari:
Thats amazing as a find!

> CPerezz:
So now, compaction is the next thing to check. 

These are by far the 2 things we have the highest conviction of being critical.

> CPerezz:
BTW, I wonder what happens with Geth and why it doesn't suffer from these issues?

I'd assume because we haven't tried syncing it under this load?

> Pari:
should i add a node and resync it with geth as well?

> Pari:
or let me ask rafael what the status of the sync tool is

> Pari:
i think he wanted to do fusaka tests first

> CPerezz:
I think we should not rush yet. We first would like to have all clients exposing the metrics we need

> CPerezz:
to better analyze the behaviour of nodes

> CPerezz:
So I'd go for baseline (target-gas load sync at 2x mainnet size).

And from there we scale up to 100Mgas or so. And keep going

> CPerezz:
WDYT?

> Kamil Chodo≈Ça:
We should resync Geth and Besu likely on snap but I'd prefer adding new ones

> Kamil Chodo≈Ça:
Also will work with @goldbillka to make a Kurtosis reproducible script for that

> Kamil Chodo≈Ça:
@parithoshj Current spamoor version which runs on perfnet2 is docker.ethquokkaops.io/dh/ethpandaops/spamoor:master-latest?

> Pari:
yeah this is right

> Pari:
üñº we already have a reproducible sync test script if you need it.

https://github.com/ethpandaops/kurtosis-sync-test

it has a makefile and instructions in readme

> Pari:
Okay i agree with @CPerezz though, lets focus on metrics and analyzing behaviour first. Clients can do sync tests to catch some bugs (like nethermind is right now), but we can do a proper effort once we hit 2x mainnet state.

> Pari:
Alternate approach is that since we started at ~500gb and now we're at ~750gb, we're at 1.5x state. We can stop here and find issues + sort out metrics. We clearly have already stressed the system and need to validate it across all clients.

> Kamil Chodo≈Ça:
Nethermind finishing sync now so will see the state size here soon

> Pari:
did reducing the load help with sync then?

> Kamil Chodo≈Ça:
almost at the stage which hanged so will ocnfirm in 10 mins

> CPerezz:
The point is we were at 800Mgas/block. Which is quite an unrealistic thing to focus on now right now. Hence why I proposed this other path.

From my talk with Ansgar and Marius it was clear they want to prio 100M + 2x mainnet state first

> Kamil Chodo≈Ça:
But 100M is AFTER getting 2x mainnet state?

> Kamil Chodo≈Ça:
And tbh not sure what kind of load causing this issue now

> Kamil Chodo≈Ça:
It may be that actually 100Mgas blocks with current state or even empty state may cause that issue

> Pari:
Okay lets give nethermind some time to figure out what is going on. Once that's there, we can take a call.

> Kamil Chodo≈Ça:
I'm a bit afraid it may be general healing issue - not only netherminds

> Kamil Chodo≈Ça:
We didn;t executed similar test like this in past for any client?

> Kamil Chodo≈Ça:
And none of the other snap sync client is syncing now except nethermind

> Kamil Chodo≈Ça:
Nethermind now Synced properly - Healing took 6 minutes

> Kamil Chodo≈Ça:
Already proposing blocks

> Kamil Chodo≈Ça:
I can resume on bloating if we want?

> Pari:
Yup please do

> MariusVanDerWijden:
Yes healing will naturally break down if there are too many changes between blocks

> Kamil Chodo≈Ça:
Question is - where is the moment it starts to break. We had 800mgas blocks - but may be with these kind of transactions we can break it much earlier maybe

> Kamil Chodo≈Ça:
Then it is a problem to tackle asap

> MariusVanDerWijden:
BALs can help us here

> Kamil Chodo≈Ça:
üñº :D

> Kamil Chodo≈Ça:
We need to understand tho if this is a problem we can face on current spec - or is more about high gas limit + specific attack and we can safely wait for BALs (which needs first to be confirmed to the fork)

> MariusVanDerWijden:
It does depend on the churn of the state and the size of the state you need to download (the speed of which you download the snapshot)

> Kamil Chodo≈Ça:
Bloatnet back on 100% participation

> CPerezz:
For healing also I'd say the more state you touch (in ammount) the worse it becomes specially the shallower it is distributed.

> CPerezz:
Should I start the spammers too?

> Kamil Chodo≈Ça:
Wait for a 30 minutes

> Kamil Chodo≈Ça:
@ben_a_adams trying to rejoin also

> Kamil Chodo≈Ça:
OK it is not reproducible that easily on empty state

> Kamil Chodo≈Ça:
Let's keep on bloating and try to reproduce that with smaller load later to see where it starts to break

> Ben {chmark} Adams ‚ü†:
are you letting me sync first? üò¢

> Kamil Chodo≈Ça:
Yup - waiting for you :)

> CPerezz:
Noted down

> CPerezz:
Are we ready?

> Ben {chmark} Adams ‚ü†:
Still syncing https://teragas.wtf/

> Kamil Chodo≈Ça:
Moving pretty slowly for Ben

> CPerezz:
MB! Will kickstart once I see the healing complete!

> Ben {chmark} Adams ‚ü†:
Yeah should be processing blocks at that stage

> Ben {chmark} Adams ‚ü†:
I limited the peers to the initial nodes as it was getting pretty promiscuous with mainnet nodes; so download is slower but less bad peers and drops

> Kamil Chodo≈Ça:
BTW

State size is 303 GB on Nethermind comparing to 190 on mainnet

So 1.5x it is! :) 80 more gigs to go!

> Ben {chmark} Adams ‚ü†:
1.16 ETH block rewards at @ 2gwei gas üòÖ

> Kamil Chodo≈Ça:
@CPerezz Are you monitoring if the data distribution is correct still?

> CPerezz:
Hence why wanted to be done with accounts

> Pari:
Is there a dashboard with this data?

> Kamil Chodo≈Ça:
https://grafana.observability.ethpandaops.io/d/nQaRO6Yizs/nethermind-imported?orgId=1&from=now-5m&to=now&timezone=utc&var-prometheus_ds=UhcO3vy7z&var-network=$__all&var-enode=perf-devnet-2-lighthouse-nethermind-1&refresh=10s

Checked here at very bottom right afetr sync

> Kamil Chodo≈Ça:
Did you got access to spamoor UI?

> CPerezz:
Yes I do have it!

> Pari:
I'm assuming we already know that receipts arent being bloated right?

> Kamil Chodo≈Ça:
yup

> Ben {chmark} Adams ‚ü†:
üñº Red is danger https://teragas.wtf/

> Kamil Chodo≈Ça:
Seems like a lot of danger

> Kamil Chodo≈Ça:
@FunnyGiulio @mlnfltv @somnergy

> Kamil Chodo≈Ça:
1 sec exec time? Why so slow mate

> CPerezz:
SSTORE is the worst scenario to be concerned. There are a lot worse ways to force you to write a lot 

Edit: Is compaction running?

> Kamil Chodo≈Ça:
Photo

> milen:
yea, im aware - i just finished adding metrics to help us figure out what is happening - we will discuss internally to further analyse this

> milen:
we just start falling behind basically

> Kamil Chodo≈Ça:
@CPerezz 

What are the worse scenarios? Why aren;t we yet having them here? The worse the better - edge case is what we are  concerned

> Kamil Chodo≈Ça:
As long as executable on mainnet - we should be concerned

> Philipp (pk910):
bloatnet focuses on bloating,  not on execution time?
that's why we're even spamming with ~500M gas, which is way higher than what we're currently targeting for mainnet. So high execution times are kinda expected?

> Ben {chmark} Adams ‚ü†:
affordability of the machine üòâ

> Kamil Chodo≈Ça:
But I understand that in a way that there is something which forces to write more? Even more than we do rn?

> Philipp (pk910):
ah,  probably contract deployments.

> CPerezz:
https://cperezz.github.io/bloatnet-website/bloating.html Shows the cheapest ratios on gas/byte

> Kamil Chodo≈Ça:
üñº Doing quite well on validators - I love this addition to Dora @pk910 :) 
Does it push the data to grafana also?

> Kamil Chodo≈Ça:
Oh I see it does quite well here :)
https://grafana.observability.ethpandaops.io/d/ee3565c0xam80d/l1-mgas-s?orgId=1&from=2025-07-17T12:21:16.043Z&to=2025-07-17T12:44:03.901Z&timezone=browser&var-prometheus_ds=UhcO3vy7z&var-network=perf-devnet-2&var-instance=$__all&var-slotTime=12&var-nethermind_instance=perf-devnet-2-lighthouse-nethermind-1

> Philipp (pk910):
afaik yes.  dora grabs the data from the rpc-snooper on the engine rpc connection.
the rpc-snooper exports metrics too, that are probably going to grafana - but I'm not sure

> Kamil Chodo≈Ça:
Seems liek you used exactly the same metrics names as jrpc-interceptor that is why it work?:)

> Philipp (pk910):
yeaa, that was exactly the reason :D   our rpc-snooper didn't export metrics before,  so I added that to maintain compatibility

> Ben {chmark} Adams ‚ü†:
Roughly 840 new contacts, 4.3k calls, 20k sstores per block

> Kamil Chodo≈Ça:
Except of erigon which fallen behind We are hoovering around max 1 sec processing time.
I feel like for bloatnet purpose we can easily bump bloating speed 2x?

> Philipp (pk910):
when exceeding 50% gas usage on average the base fee grows and we can't affort spamming at some point :D
we need to raise the gaslimit too if we wanna spam more

> Kamil Chodo≈Ça:
yeah why not tbh? 2x gas limit and 2x spamming then

> Ben {chmark} Adams ‚ü†:
Eth supply crunch

> Kamil Chodo≈Ça:
@CPerezz with current bloating speed what is your expectation on reaching 2x mainnet?

> Philipp (pk910):
should we actually add a contract deployment spammer?
can have a look into that. I think it should be possible with the current scenarios

> Kamil Chodo≈Ça:
Surely it was as I recall I was doing that at perfnet-0

> Kamil Chodo≈Ça:
We can fetch any contract we want from gas-benchmarks

> Kamil Chodo≈Ça:
Or just pick anything else which is worthy

> Kamil Chodo≈Ça:
Depends on @CPerezz if this is needed for now

> Philipp (pk910):
right :) just should be unique, otherwise we don't have huge writes when re-deploying existing code.
but I think you already solved that in the preparation spammer for mass contract reads?

> CPerezz:
Our pace is 1GB/h approx at this bloating rate. So we should be there in ~3days more or less

> CPerezz:
NOO!!!

> CPerezz:
Please don't deploy anything. 

The goal is to keep mainnet state ratio as: ~10% contracts 10% accounts and rest storage

> CPerezz:
Sorry for the dramatism hahahahah

> Kamil Chodo≈Ça:
Shall we generate on a side critical scenarios?

> Kamil Chodo≈Ça:
So like state only for contracts, state only with accounts etc?

> Kamil Chodo≈Ça:
Which will be smaller in size and probablt faster to bloat just for one case where we may want to have for example 200GB of just contracts?

> Kamil Chodo≈Ça:
And see how does it affect the time of executing specific contracts?

> CPerezz:
I'm almost done with the plan for the overall thing. For now, the prio as per Ansgar and Marius request is 100M gas at 2x mainnet state. 

This means, although these benchmarks are interesting, we should perform them first at that scale. Run all the possible scenarios and things we can imagine. And then, only after 100M is safe with 2x mainnet state, we go for the crazy long-distant things.

That's at least the strawman.

> Kamil Chodo≈Ça:
Absolutely - just throwing an idea that it would be nice to verify how "purely this" affects the perf of specific things

> CPerezz:
So I'd say the plan is to make it to 2x state size. Meanwhile, meet with all client teams and gather ideas on worst cases they expect related to state and discuss the ones we already have. And then, implement them all in EELS to have baselines and then run matrix measurements with different loads + envs (deep branches, compaction, reorg etc...)

Then, we also experiment with spamoor + these envs (not just a single run from EELS).

> CPerezz:
Sure! Perf is one of the critical things of the project!!!

> CPerezz:
Just unsure this is a time to do it. Mainly as Idk how much data gathering can we do. And how reproducible this will be (we have snapshot at 1.5x and 2x). Not at this particular point. So exact condition repro seems hard

> MariusVanDerWijden:
Yes agree that we should keep these scenarios on the backburner and focus on the 2x scenario for now, would be good to note them though to make sure we don't forget

> Kamil Chodo≈Ça:
2x is almost there - so we need to start thinking if we want to rather move into 5x direction and persist it (or if you like the number more - 1TB state) or focus on whatever else.

Question - with increasing gas limit do we rather expect that mainnet distribution will remain the same or it may evolve? Of course this is like asking for a lottery numbers - but maybe anyone have any thoughts what would happen if we will increase to 300 by end of next year for example?

> CPerezz:
I'd expect same. I only see new usecases/trends or EIPs to disrupt it. Anyways, seems impossible to predics how it would change.

> MariusVanDerWijden:
I think we should do the following:
- Focus on a 2x fork to create a set of benchmarks/testcases that we can replay on top of arbitrary state to get consistent metrics. We should spent most of our time here as it will inform us best about our move to 100M
- Keep bloating a fork at mainnet distribution to 4x, 6x, 8x and 10x mainnet size (optimize the workflow so it doesn't require much eyes), once we have a framework for tests, replay them on the different nx forks to see how the metrics evolve
- Start looking into degenerated test cases (broad vs. deep trie, account vs storage tries, ...)

> Giulio:
It also takes a lot of ETH to manipulate ETH into degeneracy to the point itnis not feasible. The MPT is hashed so unless manipulated, state writes should always be random

> CPerezz:
Reorg + Compaction + 100M block that touches a lot of state might be a lot of coincidence. But can happen. And we need to be sure the thing doesn't just explode.

I think these are more "realistic" degeneracy scenarios

> ahamlat:
Is anyone spamming the network with 0x transactions ? I see a lot of errors on besu node like this

| Error processing method: eth_sendRawTransaction ["0x"]

> ahamlat:
it looks better so far üëå

> ahamlat:
It just happened again 

2025-07-17 13:36:31.059+00:00 | vert.x-worker-thread-7 | ERROR | BaseJsonRpcProcessor | Error processing method: eth_sendRawTransaction ["0x"]

> CPerezz:
What about now?

> CPerezz:
LMK. Tried to shutdown some spammers to see which is the one that is introducing issues

> CPerezz:
Where can I check the logs @parithoshj ?

> CPerezz:
https://grafana.observability.ethpandaops.io/d/2k8BXz24k/reth?orgId=1&from=now-12h&to=now&timezone=browser&var-network=perf-devnet-2&var-instance=perf-devnet-2-teku-erigon-1&editIndex=1 doesn't shjow any data to me

> ahamlat:
It just happened again

> Pari:
https://grafana.observability.ethpandaops.io/d/be08p9zqvxl34b/logs-testnets?orgId=1&from=now-30m&to=now&timezone=browser&var-network=perf-devnet-2&var-consensus=teku&var-execution=erigon

or ssh in to the machine, you should have access to all. ssh devops@teku-erigon-1.perf-devnet-2.ethpandaops.io

> CPerezz:
2025-07-17 13:47:12.000 ERROR - Unable to retrieve validator statuses from BeaconNode.

Seems more like a Teku issue maybe?

> Pari:
Nope, Syncing     *** Slot: 66089, Head slot: 66088, Waiting for execution layer sync

> Pari:
let me restart the beacon to see if it helps

> CPerezz:
@pk910 I've tried the 3 different spammers (non-blobs) and all cause the same issue on Besu. Any ideas here?

> CPerezz:
Blobs seems safe BTW. THough I'm trying a few more combos

> Philipp (pk910):
huh,  that seems bad o.O  checking too

> CPerezz:
Left all except blobs to see if it is the source

> Giulio:
State degenerating means unbalancing MPT

> Giulio:
so  an unholy amounts of SSTOREs is required

> ahamlat:
It should be the same issue on all clients, but it is not urgent, it is just to be sure we don't spam clients with useless transactions

> Philipp (pk910):
spamoor doesn't throw errors and doesn't get out of sync.  so it's very likely the re-broadcast system that goes crazy in some way

> CPerezz:
I was more on deep tree branches or similar stuff. So things we can't control

> Giulio:
Yeah, possible but you can do better attacks for cheaper

> CPerezz:
I think it was the blobs? I stopepd it now and seems no more errors arise. Weird anyways..

> Philipp (pk910):
yea, that makes sense. the blob scenarios are the only ones that have a code path that sends raw tx directly. that was added because of the new blob format in fulu.
Thx for the analysis :)

> CPerezz:
Should I keep it stopped @amezianehamlat ? At the end, it doesn't help on anything now in bloating stage

> CPerezz:
@FunnyGiulio @mlnfltv any news on syncing issues with Erigon at 1.5x state?

> milen:
no news, it is lagging behind quite a lot, we are looking into it - it is not something that we can fix quickly in a day or two, it is probably going to take more time

> CPerezz:
Ohh I'm more interested on the identification of the issue rather than the fix itself.

No pressure at all. It's mostly to note it down as I'll do with any finding

> milen:
yea we're working on it - will let you know once we figure it out

> ahamlat:
I don't see the error since 20 minutes. The issue is polluting the logs, if you can stop it, it is better, but not very important at this level if we still have the error.

> CPerezz:
Blobs don't actually help us on bloating state. 

So I'm fine turning them down. @pk910 any objections or opinions?

> Philipp (pk910):
yea, fine to leave it turned off :)  fix is on the way

> Kamil Chodo≈Ça:
@parithoshj @mlnfltv said he is on it

> ahamlat:
What is the kind of load that is applied currently ?

> milen:
@CPerezz fyi added more metrics to the Erigon grafana https://grafana.observability.ethpandaops.io/d/b42a61d7-02b1-416c-8ab4-b9c864356174-v2/erigon-internals-v2?orgId=1&from=now-12h&to=now&timezone=browser&var-quantile=0.97&var-instance=perf-devnet-2-teku-erigon-1&var-nodo=$__all&refresh=30s

- Healing phase duration: N/A
- Overall sync duration: Blocks Execution/Time in initial cycle
- Start-End DB size: Database/DB Size and Table Sizes
- R/W performance for all ops: various metrics in Blocks Execution/Database/Process, will also add more in the future
- Compaction start/end time: Blocks Execution/prune related graphs
- IO perf: Process/Disk bytes/sec

> CPerezz:
That's the one I appreciate the most!! Thanks a lot!

> Pari:
üñº If we don't change anything, we might have the conditions to figure out why lighthouse-reth misses proposals regularly:

> Pari:
also teku-erigon is missing proopsals

> Kamil Chodo≈Ça:
Seems like it happens only from time to time - so may be that either Reth or Lighthouse have some hicckups

> Kamil Chodo≈Ça:
But Nethermind also is with Lighthouse

> Kamil Chodo≈Ça:
And proposes very consistently

> Kamil Chodo≈Ça:
So more likely sth on Reth

> CPerezz:
No metrics on reth Grafana show alarming statuses. Any ideas @rkrasiuk ?

> Pari:
i think its timeout related, but unsure how to debug this tbh. ill dig into CL logs in a bit.

> Roman Krasiuk:
i have 200 unreads in this chat, where should i start reading to catch up on the context of this question

> Kamil Chodo≈Ça:
TL;DR; Reth sometimes misses blocks on blaotnet where we spamm with around 400-500 state consuming blocks.
Not sure if this is blockbuilding timeouts, and if Reth is more the issue here of Lighthouse.

> CPerezz:
https://grafana.observability.ethpandaops.io/d/2k8BXz24k/reth?orgId=1&from=now-12h&to=now&timezone=browser&var-network=perf-devnet-2&var-instance=perf-devnet-2-lighthouse-reth-1&editIndex=1

> Kamil Chodo≈Ça:
https://grafana.observability.ethpandaops.io/d/liz0yRCZA/logs-perfnet?orgId=1&from=now-30m&to=now&timezone=browser&var-network=perf-devnet-1&var-consensus=lighthouse&var-execution=reth&var-ingress_user=perf-devnet-2&refresh=auto
https://dora.perf-devnet-2.ethpandaops.io/slots/filtered?f=&f.pname=reth&f.missing=1&f.orphaned=1&c=50&d=1+2+3+4+5+6+7+8+9+10+18+11

> Pari:
or you can use DNS :)
ssh devops@lighthouse-reth-1.perf-devnet-2.ethpandaops.io

> Roman Krasiuk:
taking missed slot https://dora.perf-devnet-2.ethpandaops.io/slot/72986 as an example:
2025-07-18T12:48:01.051377Z - we receive FCU with parent block as head hash _without_ payload attributes
2025-07-18T12:48:08.001296Z - we receive the same FCU _with_ payload attributes
2025-07-18T12:48:12 - slot timestamp
2025-07-18T12:48:14.040950Z - we seal the block

> Kamil Chodo≈Ça:
Isn't it a common thing? That for block proposal FCU is often sent twice from CL?

> Roman Krasiuk:
i don't know the exact reason, but i saw that pretty often especially when an option to always send payload attributes is enabled

> Kamil Chodo≈Ça:
Workaround - save some logs with ‚Äîtail to file and then grep

> Pari:
docker logs snooper 2>&1 grep "whatever" should work?

> Roman Krasiuk:
with a pipe that should work, ye, lemme try

> Pari:
Yeah the timeout on lighthouse is 2s, so im assuming it timed out once and it tried again?

> Kamil Chodo≈Ça:
Random Idea - CL should have much smaller timeout for getPayload to be able to ask even a few times in case of issues

> Roman Krasiuk:
it corresponds with me not seeing a response for first get payload request

> Roman Krasiuk:
that doesn't make sense to me though

> Roman Krasiuk:
there is nothing in the specs saying that we should stop serving the payload

> Roman Krasiuk:
there is the clause that says

Client software MAY stop the corresponding build process after serving this call.

> Roman Krasiuk:
but it doesn't mean that we should make the payload not available

> Pari:
Most cls do 1s timeout, lh does 2s apparently

> Roman Krasiuk:
there is a separate issue of why lighthouse gave us payload attribbutes only at 8 seconds into the slot

> Matthias Seitz:
unclear tbh

> Roman Krasiuk:
ah, ye, it's the default

> Roman Krasiuk:
--prepare-payload-lookahead <MILLISECONDS>
          The time before the start of a proposal slot at which payload
          attributes should be sent. Low values are useful for execution nodes
          which don't improve their payload after the first call, and high
          values are useful for ensuring the EL is given ample notice. Default:
          1/3 of a slot.

> milen:
@CPerezz apologies if this has been discussed already (struggling to find this in the chat) but what type of bloating are we doing in these fat 500MGas blocks so far - just blocks filled with lots of txns deploying new contracts with new storage keys or something else?

> Kamil Chodo≈Ça:
Besu Reth and Erigon seems like having huge striggles on Bloatnet

> Kamil Chodo≈Ça:
We lost finality

> Kamil Chodo≈Ça:
Wil try to sit in next 1 h to revive at least one here but can't promise as just sat for 3 minutes

> Kamil Chodo≈Ça:
Also geth teh same

> Pari:
Yikes, it seems worse than struggling tbh. It looks like we have 3 forks, one for besu, one for geth and one for nethermind. I'm assuming then reth/erigon are just syncing and refusing to participate.

> Pari:
found it :D
ERROR[07-18|23:45:13.083]
########## BAD BLOCK #########
Block: 22876662 (0xd4077f1cb6db368a4e4466ba019df99fbb5dae73c010cb69e2e38ab0f909c582)
Error: invalid gas used (remote: 575293625 local: 575293025)

> Pari:
here's what split besu and geth: https://dora.perf-devnet-2.ethpandaops.io/slot/0x84459b06e2feacaa6e843163fc3a0ed4eaa6b1f1d84761c8c698c5cba68defb8

> Pari:
its a block proposed by besu and rejected by geth, i am yet to see what the other clients did with it. 

cc @KamilChNethermind @siladu @amezianehamlat @vdWijden (or who is looking into it in geth?)

> MariusVanDerWijden:
I'm ooo today

> Pari:
Who can i ping to triage this? I only wanna find out if its mainnet relevant or not, if not then we can deal with this slower

> Kamil Chodo≈Ça:
@ben_a_adams @MarekM25 are you guys around to help analyze that from Neth perspective?

> Pari:
invalidGas.log

> MariusVanDerWijden:
600 gas difference between have and want

> Pari:
üìé nimbus-besu-1 relevant logs

> Pari:
paused all spammers for now

> MariusVanDerWijden:
Nethermind geth are on the same chain

> Gary Rong:
@parithoshj do you have any archive node for bad block tracing?

> Pari:
üñº are they? geth seems to be forked and proposing on a chain different from nethermind

> Pari:
checking

> MariusVanDerWijden:
At least they were around the issue afaict

> Gary Rong:
https://dora.perf-devnet-2.ethpandaops.io/slot/76273

> Pari:
Yup the bootnode should be an archive node

> Gary Rong:
the block proposed by geth is in the canonical chain?

> Pari:
ssh devops@bootnode-1.perf-devnet-2.ethpandaops.io

> Pari:
Yup, it seems to be a part of the canonical chain - i can check on nethermind

> ahamlat:
Thanks, I‚Äôm far from my laptop, will look info it when possible

> ahamlat:
Cc. @kt2am

> Pari:
Nethermind seems to not have emitted its regular block processed log for block 22876662

> Gary Rong:
i am trying to do it, but looks like bootnode (archive node) doesn't see this bad block

> Kamil Chodo≈Ça:
@parithoshj do we have tracoor attached?

> Pari:
we dont have tracoor running here :/

> Matkt (Karim):
the besu node that proposed it doesn't have it ?

> Pari:
the block was ~12h ago, so i think both have pruned it by now

> Kamil Chodo≈Ça:
Hmmm was it removed? It was for perfnet1 but is also down for it for some reason

> Pari:
no idea :/
i can add it for both networks again

> Pari:
Does anyone mind if i resync the bootnode and set it up as an archive node?

> Matkt (Karim):
is it possible to call eth_getBlockByHash in the besu node, maybe by chance we still have it

> MariusVanDerWijden:
Yes as I expected, there are two forks

> MariusVanDerWijden:
Photo

> MariusVanDerWijden:
The besu <> everyone else and then 400 blocks later a second one

> MariusVanDerWijden:
Neth, reth <> geth erigon

> ahamlat:
You can access Besu node Karim and do it

> Matkt (Karim):
I will DM you @amezianehamlat

> Pari:
0xd4077f1cb6db368a4e4466ba019df99fbb5dae73c010cb69e2e38ab0f909c582.json

> Matkt (Karim):
thanks

> MariusVanDerWijden:
I suspect this second fork is due to issues on the CL, not on the EL

> Pari:
could also be non finality related on the CL side, its very few peers - so if you just loose all of them then you build on yourself eventually

> ahamlat:
@parithoshj  could you share with karim the command to access  besu node

> Pari:
ssh devops@nimbus-besu-1.perf-devnet-2.ethpandaops.io

> Matkt (Karim):
ok thanks

> Gary Rong:
22877081

NETH/RETH: 0x43a29278d7711c93553289932a56f8f7405ed0016427713d3723d57bb7efde07
GETH/ERIGON: 0xebfec777c5287f342b96e82b16d08f4204eced7f2e33aa4c575df7b1f2e226c5

> Gary Rong:
However, the block of 0x43a29278d7711c93553289932a56f8f7405ed0016427713d3723d57bb7efde07 was seen by bootnode (geth)

> Gary Rong:
^ as a bad block

> MariusVanDerWijden:
Oh

> Gary Rong:
bad_blocks.json

> Gary Rong:
The bad-blocks dumped from the bootnode (geth)

> MariusVanDerWijden:
But the fork was at 22877074

> MariusVanDerWijden:
7 blocks earlier

> Gary Rong:
22877074
NETH: 0x92e857225ab84ac8bafd3b7e6a78fe63c693d0b1fcd93c8547b51f3b8bc34d8d
GETH: 0x1895ce728507404bf8db5db6b2dd8c0eb0f670cf87d5cc0848b84c62bbf81b4a

> Gary Rong:
These two chains split at this block

> Gary Rong:
yes right

> Gary Rong:
also, block 0x92e857225ab84ac8bafd3b7e6a78fe63c693d0b1fcd93c8547b51f3b8bc34d8d is not recorded as a bad block in Geth

> Gary Rong:
block 0x92e857225ab84ac8bafd3b7e6a78fe63c693d0b1fcd93c8547b51f3b8bc34d8d is  in the geth's database

> Gary Rong:
as a valid block

> MariusVanDerWijden:
Neth: https://dora.perf-devnet-2.ethpandaops.io/slot/77212

Can you figure out in which slot the geth block was mined? I can't seem to find it on dora

> MariusVanDerWijden:
There are a bunch of missed slots after the nethermind block on dora and I suspect the missing block was mined then

> MariusVanDerWijden:
So it seems to look like a CL issue as suspected

> Matkt (Karim):
trying to rollbak the state of besu in order to replay and trace the block

> Pari:
I've informed some CL devs and asked if someone is around to help

> Pari:
Status:
- 1st split was between besu and other EL, 2nd split was between neth/reth(lighthouse) vs geth/erigon(prysm/teku) (likely CL related)

Current steps:
- Besu is trying to get a trace out from their machine
- Geth archive is being synced to get a trace + we will use their sync target feature without a CL to validate if its a CL issue or not
- CL devs informed, waiting for debug action on their side
- Forky and Tracoor are up now, but cannot fetch historic information

> ahamlat:
The transaction where we have 600 gas difference is this one (besu reports 325722 where geth reports 325122)
Geth
28: cumulative: 575104025 gas: 325122 contract: 0x0000000000000000000000000000000000000000 status: 1 tx: 0xccc3f793875f9d02a332a7ed7ddccf686013a01c02f6ea2b39b329d395d94408 logs: [0xc0ffa93c30] bloom: 04000000000000000000000000000000000000000000000000000000000000020000000000000000400000020000000000000000020000000000000000000000100000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000100000000000000000000800000000000000000400000000000000000000000000000000000000000000000008000000000000000000000000000000000000000000000000000000000008002000000000000000000 state:


Besu
Transaction: 0xccc3f793875f9d02a332a7ed7ddccf686013a01c02f6ea2b39b329d395d94408
{"jsonrpc":"2.0","id":1,"result":{"blockHash":"0xd4077f1cb6db368a4e4466ba019df99fbb5dae73c010cb69e2e38ab0f909c582","blockNumber":"0x15d11f6","contractAddress":null,"cumulativeGasUsed":"0x22476671","from":"0xe18316bfdde4e8893f41f94b3439183cfb81a5d1","gasUsed":"0x4f85a","effectiveGasPrice":"0x3b9aca12","logs":[{"address":"0x410d7e4ea1093a532ef9a7a2d5df84084b05ec24","topics":["0x22c9005dd88c18b552a1cd7e8b3b937fcde9ca69213c1f658f54d572e4877a81","0x0000000000000000000000000000000000000000000000000000000000000002","0x0000000000000000000000000000000000000000000000000000000000000003"],"data":"0x","blockNumber":"0x15d11f6","transactionHash":"0xccc3f793875f9d02a332a7ed7ddccf686013a01c02f6ea2b39b329d395d94408","transactionIndex":"0x1c","blockHash":"0xd4077f1cb6db368a4e4466ba019df99fbb5dae73c010cb69e2e38ab0f909c582","logIndex":"0x353","removed":false}],"logsBloom":"0x04000000000000000000000000000000000000000000000000000000000000020000000000000000400000020000000000000000020000000000000000000000100000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000100000000000000000000800000000000000000400000000000000000000000000000000000000000000000008000000000000000000000000000000000000000000000000000000000008002000000000000000000","status":"0x1","to":"0x8c0bfc04ada21fd496c55b8c50331f904306f564","transactionHash":"0xccc3f793875f9d02a332a7ed7ddccf686013a01c02f6ea2b39b329d395d94408","transactionIndex":"0x1c","type":"0x2"}}

> Matkt (Karim):
we cannot really trace it for the moment, because the block is old

> MariusVanDerWijden:
Does anyone know what contract is deployed at 0x8c0bfc04...f564

> Matkt (Karim):
this is the bytecode

> Matkt (Karim):
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

> Matkt (Karim):
01610347565b506001016107a2565b5061081387610928565b50505050505050565b5f858152600460209081526040808320338452909152902054859060ff166108595760405163472511eb60e11b8152336004820152602401610347565b61070186610928565b61086a6109dc565b600180546001600160a01b0383166001600160a01b0319909116811790915561089a5f546001600160a01b031690565b6001600160a01b03167f38d16b8cac22d99fc7c124b9cd0de2d3fa1faef420bfe791d8c362d765e2270060405160405180910390a350565b6108da6109dc565b6005805463ffffffff191663ffffffff83169081179091556040519081527fd32d6d626bb9c7077c559fc3b4e5ce71ef14609d7d216d030ee63dcf2422c2c49060200160405180910390a150565b60025460405163dead6f7f60e01b8152600481018390525f916001600160a01b03169063dead6f7f90602401602060405180830381865afa15801561096f573d5f803e3d5ffd5b505050506040513d601f19601f820116820180604052508101906109939190610c10565b90506001600160a01b0381166109bc5760405163d92e233d60e01b815260040160405180910390fd5b365f80375f80365f80855af13d805f803e8180156109d857815ff35b815ffd5b5f546001600160a01b031633146105f55760405162461bcd60e51b815260206004820181905260248201527f4f776e61626c653a2063616c6c6572206973206e6f7420746865206f776e65726044820152606401610347565b600180546001600160a01b031916905561066e815f80546001600160a01b038381166001600160a01b0319831681178455604051919092169283917f8be0079c531659141344cd1fd0a4f28419497f9722a3daafe3b4186f6b6457e09190a35050565b5f8060408385031215610aa9575f80fd5b50508035926020909101359150565b5f602080835283518060208501525f5b81811015610ae457858101830151858201604001528201610ac8565b505f604082860101526040601f19601f8301168501019250505092915050565b6001600160a01b038116811461066e575f80fd5b5f8060408385031215610b29575f80fd5b823591506020830135610b3b81610b04565b809150509250929050565b5f805f805f60808688031215610b5a575f80fd5b853594506020860135935060408601359250606086013567ffffffffffffffff80821115610b86575f80fd5b818801915088601f830112610b99575f80fd5b813581811115610ba7575f80fd5b896020828501011115610bb8575f80fd5b9699959850939650602001949392505050565b5f60208284031215610bdb575f80fd5b8135610be681610b04565b9392505050565b5f60208284031215610bfd575f80fd5b813563ffffffff81168114610be6575f80fd5b5f60208284031215610c20575f80fd5b8151610be681610b0456fea26469706673582212206c1e8d666e61f914efddf6ba84e2e9670d3696569a556bc1077b3eaa5e0d7f9064736f6c63430008180033
```
```

> Pari:
could you post the full contract address?

We had a xen state bloater, 7702 max auth spammer, sstore state bloater and a blob spammer

> Pari:
cc @CPerezz in case you had something else running as well

> Pari:
pk is ooo today

> Matkt (Karim):
the address 
0x8c0bfc04ada21fd496c55b8c50331f904306f564

> Matkt (Karim):
this is the transaction with the input 

```json
 {
            ‚ÄúaccessList‚Äù:[            ],            ‚ÄúblockHash‚Äù:‚Äú0xd4077f1cb6db368a4e4466ba019df99fbb5dae73c010cb69e2e38ab0f909c582‚Äù,
            ‚ÄúblockNumber‚Äù:‚Äú0x15d11f6‚Äù,            ‚ÄúchainId‚Äù:‚Äú0x1‚Äù,
            ‚Äúfrom‚Äù:‚Äú0xe18316bfdde4e8893f41f94b3439183cfb81a5d1‚Äù,            ‚Äúgas‚Äù:‚Äú0xe4e1c0‚Äù,
            ‚ÄúgasPrice‚Äù:‚Äú0x3b9aca12‚Äù,            ‚ÄúmaxPriorityFeePerGas‚Äù:‚Äú0x3b9aca00‚Äù,
            ‚ÄúmaxFeePerGas‚Äù:‚Äú0x8bad3a36‚Äù,            ‚Äúhash‚Äù:‚Äú0xccc3f793875f9d02a332a7ed7ddccf686013a01c02f6ea2b39b329d395d94408‚Äù,
            ‚Äúinput‚Äù:‚Äú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‚Äù,            ‚Äúnonce‚Äù:‚Äú0x4‚Äù,
            ‚Äúto‚Äù:‚Äú0x8c0bfc04ada21fd496c55b8c50331f904306f564‚Äù,            ‚ÄútransactionIndex‚Äù:‚Äú0x1c‚Äù,
            ‚Äútype‚Äù:‚Äú0x2‚Äù,            ‚Äúvalue‚Äù:‚Äú0x0‚Äù,
            ‚ÄúyParity‚Äù:‚Äú0x1‚Äù,            ‚Äúv‚Äù:‚Äú0x1‚Äù,

> ahamlat:
The input of the transaction
`
{
            ‚ÄúaccessList‚Äù:[
            ],
            ‚ÄúblockHash‚Äù:‚Äú0xd4077f1cb6db368a4e4466ba019df99fbb5dae73c010cb69e2e38ab0f909c582‚Äù,
            ‚ÄúblockNumber‚Äù:‚Äú0x15d11f6‚Äù,
            ‚ÄúchainId‚Äù:‚Äú0x1‚Äù,
            ‚Äúfrom‚Äù:‚Äú0xe18316bfdde4e8893f41f94b3439183cfb81a5d1‚Äù,
            ‚Äúgas‚Äù:‚Äú0xe4e1c0‚Äù,
            ‚ÄúgasPrice‚Äù:‚Äú0x3b9aca12‚Äù,
            ‚ÄúmaxPriorityFeePerGas‚Äù:‚Äú0x3b9aca00‚Äù,
            ‚ÄúmaxFeePerGas‚Äù:‚Äú0x8bad3a36‚Äù,
            ‚Äúhash‚Äù:‚Äú0xccc3f793875f9d02a332a7ed7ddccf686013a01c02f6ea2b39b329d395d94408‚Äù,
            ‚Äúinput‚Äù:‚Äú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‚Äù,
            ‚Äúnonce‚Äù:‚Äú0x4‚Äù,
            ‚Äúto‚Äù:‚Äú0x8c0bfc04ada21fd496c55b8c50331f904306f564‚Äù,
            ‚ÄútransactionIndex‚Äù:‚Äú0x1c‚Äù,
            ‚Äútype‚Äù:‚Äú0x2‚Äù,
            ‚Äúvalue‚Äù:‚Äú0x0‚Äù,
            ‚ÄúyParity‚Äù:‚Äú0x1‚Äù,
            ‚Äúv‚Äù:‚Äú0x1‚Äù,

> ahamlat:
‚Äúr‚Äù:‚Äú0x88f90a4f05393ed2ee5e5a094bf36ba187c26beff506e2d9973802440b0eb2ad‚Äù,
            ‚Äús‚Äù:‚Äú0x278c64d2f774fc8166bd1b16cd5a3129369d455134f581d08fec7137a919e14‚Äù
         }
`

> ahamlat:
Is it possible to call debug_traceCall with a geth node ?

> Pari:
not yet

> Terence:
Notes from my end:

 ‚ÅÉ nimbus-besu-1 has been on its own fork for a while, so we are constantly getting multiple blocks of two proposers in the same slot
 ‚ÅÉ At slot 77206, it's the last prysm-geth block without getting orphaned 
 ‚ÅÉ At slot 77221, it's the first prysm-geth block that displays orphaned on Dora
 ‚ÅÉ Couldn't find any error in the log except for the besu block which correctly black listed
 ‚Ä¢  ‚ÅÉ The lack of peer count is concerning, but for prysm-geth, we stopped having any peer (reason unclear?) that's why blocks show up as orphaned I believe, it has no one to send to except xatu? attached graphs

> Terence:
Photo

> Pari:
alteast according to dora, prysm and teku are on the same head right? so they must be peered?

> Matkt (Karim):
@parithoshj Is it possible to restart a Besu node, make it start from the beginning of the shadow fork, and then  import blocks one by one? if it's possible I think I will be able to provide a besu node that stop to sync just before the invalid block and like that I will be able to have a valid state

> Terence:
I dont think so anymore, it shows 0 peer now üòõ 

curl http://localhost:5054/p2p
bootnode=[enr:-Iq4QO5d3m9DzBlj1PLCovLotwN8b9mFp_MrCx-KiNjzltphcYYT7oeD-VU7qyuNJd50X3wlJwjd6C989B3DkVcQ3tiGAZfpKz9kgmlkgnY0gmlwhJ20DuWJc2VjcDI1NmsxoQJJ3h8aUO3GJHv-bdvHtsQZ2OEisutelYfGjXO4lSg8BYN1ZHCCIzI enr:-LK4QAtyVYCHLTOF1iJ8TK84phLbeuUt42ub5x9OODjTbvnwHS96e6NFlpappH6eeUcyiVmS98usL3cwHUb8F1Y3NTwIh2F0dG5ldHOIAACAAQAAAACEZXRoMpBAw1JZYIAnZP__________gmlkgnY0gmlwhJ20DuWJc2VjcDI1NmsxoQJMpOlHHZBgztApCE2mE4_MZRqgAi2Y81u446NOF7RE04N0Y3CCIyiDdWRwgiMo enr:-Mq4QNiucamyTUE9hnWkibGb5BgmKQkEBTkzhLCZJa0SpH6IOLWx4Bkx0E-xzVea9A0-cmtBbk3Sz416oVN8tCq1IMGGAZfpPPVeh2F0dG5ldHOIAAADAAAAAACEZXRoMpBAw1JZcAAAAP__________gmlkgnY0gmlwhJ20DuGEcXVpY4IyyIlzZWNwMjU2azGhA7HMxNX8T5X4lwmmbnuglDAgSHvAw_XSIEu4sf0I4gdYiHN5bmNuZXRzD4N0Y3CCIyiDdWRwgiMo]
self=enr:-Mq4QDqPV33Mwek0jDXe2oM0oLX8zefnF5th5WzqYAup7KzRchN95pOaeh0AimGki7wQHu-EfnPH6POwwrWvjLwHApmGAZfz3uzPh2F0dG5ldHOIAAAAAAAAwACEZXRoMpBAw1JZcAAAAP__________gmlkgnY0gmlwhJ20DuGEcXVpY4IyyIlzZWNwMjU2azGhAwujK9HRt269aZ9BWWIRVQSyT-F76iQ1tPCu8wAxBQK8iHN5bmNuZXRzD4N0Y3CCIyiDdWRwgiMo,/ip4/157.180.14.225/tcp/9000/p2p/16Uiu2HAmDSR9M52UrZVTvcYpU86vUvFHtP9fx7SCPorpBTf1Mcif,/ip4/157.180.14.225/udp/13000/quic-v1/p2p/16Uiu2HAmDSR9M52UrZVTvcYpU86vUvFHtP9fx7SCPorpBTf1Mcif,/ip4/172.18.0.9/tcp/9000/p2p/16Uiu2HAmDSR9M52UrZVTvcYpU86vUvFHtP9fx7SCPorpBTf1Mcif,/ip4/172.18.0.9/udp/13000/quic-v1/p2p/16Uiu2HAmDSR9M52UrZVTvcYpU86vUvFHtP9fx7SCPorpBTf1Mcif

0 peers

> Pari:
hmm we bloated the state from a mainnet state and then took a snapshot from that - but due to the way we bloated the state, we dont have a snapshot for besu. we had to sync it from the geth node that had the bloated state.

> Pari:
got it, so it could just be that the CL lost peers and decided to build on its own fork and its just a reality of small networks with chaos

> Matkt (Karim):
it will be hard to have another besu node syncing from a geth node started from this snapshot ?

> ahamlat:
üñº We found the same transaction on mainnet https://etherscan.io/tx/0xccc3f793875f9d02a332a7ed7ddccf686013a01c02f6ea2b39b329d395d94408 which means that the issue could be related only to our performance branch

> Matkt (Karim):
logs are also the same

> Pari:
we can, but full sync last time took days to even sync to a small fraction of blocks

> Pari:
oh nice, so its a mainnet contract and not related to our spamming at all?

> ahamlat:
Yes, that's what I understand as well

> Matkt (Karim):
I think it's ok to wait the sync, it will just help to 100% validate our theory, because without the state I think it will be hard to validate

> Pari:
can you revert the head back and force it to forward sync? or is it too old for that?

> Pari:
okay im afk rn, but can resync besu (or should i start a new besu node?) in a few hours

> Matkt (Karim):
we tried but not working, we are still trying but not sure it will work, I will prefer to have a fallback just in case.

I will prefer to start a new node in order to keep this one

> Pari:
So the current status is that it isn‚Äôt a mainnet issue but relevant to figure out from the besu performance branch before making any future release right?

> Marek Moraczy≈Ñski:
@KamilChNethermind Perhaps a long shot, but I'm curious if it could be related to the precompile update on our perf branch (not yet released).

@parithoshj I suppose we're still unsure whether the split between Geth and Nethermind can happen on mainnet. Am I correct?

> CPerezz:
Only 3 spamming scenarios were running: XEN, ERC20 & 7702 delegation.

That means is either XEN or whatever @pk910 is using  as ERC20

> CPerezz:
So, if you saw a tx in mainnet, I'd bet is XEN most likely.

Nevertheless, would be nice to confirm. I lack access to laptop until tomorrow mid-day. Will check by then. On the meantime, have we halted the whole network @parithoshj ?

> Kamil Chodo≈Ça:
It can be also tx propagated from mainnet

> CPerezz:
Can anyone check if compaction was running in Geth, NM or Besu when these blocks happened? Do CPU/RAM/IO metrics show anything super weird?

> MariusVanDerWijden:
Pretty sure the geth <> nethermind split was due to non-finality on the CL

> CPerezz:
What about the other??

> MariusVanDerWijden:
I just landed, still on the plane. Will investigate deeper once I'm back home

> Matkt (Karim):
@parithoshj We found with @amezianehamlat that you're using an invalid branch in this instance:
https://github.com/hyperledger/besu/commit/ebc8b77f0fc352931730738b7ce74af0a70c4881

This branch includes a different gasPricing for modexp ‚Äî it sets the cost to 500 instead of 200.

In the transaction we're analyzing, there are two calls to modexp. They should each cost 200, but with your branch, they're charged 500, resulting in an extra 600 gas (500√ó2 instead of 200√ó2).

You should be using this branch instead:
https://github.com/hyperledger/besu/commits/performance

> ahamlat:
Even if the name of docker image is performance, it is built from performance-modexp branch. I reported a similar issue few days ago with the docker image.

> Pari:
Ahh! We‚Äôre using 
  besu: docker.ethquokkaops.io/dh/ethpandaops/besu:performance
Indeed

Why are we pushing the modexp changes to the same image though? And wont the change apply in fusaka? This devnet shouldnt have only pectra

> ahamlat:
We agreed with @KamilChNethermind to push Osaka pricing for pectra on performance-modexp to test the improvement directly from repricing on pectra hardfork. I don‚Äôt know though why you use performance-modexp to build performance image

> Kamil Chodo≈Ça:
Maybe is some issue with docker image build action?

> Kamil Chodo≈Ça:
Should be two separate branches - performance and performance-modexp

> ahamlat:
Yes they‚Äôre separate, performance-modexp has the tweak on modExp repricing and performance has the same setup as current mainnet

> Pari:
Okay so rebuilding and redeploy the image as well as resync besu should be enough?

I can look into why the wrong image was pushed there on Monday

> Pari:
Its likely someone just overwrote it by mistake

> Kamil Chodo≈Ça:
Seems like it works well on opcodes benchmarking

> ahamlat:
Yes, that should fix the issue üôè

> ahamlat:
There was another issue reported by Philip related to the same problem. Opcodes benchmarking doesn‚Äôt validate the blocks across clients, and that‚Äôs why we couldn‚Äôt spot on it.

> ahamlat:
https://t.me/c/2653114634/2891

> Kamil Chodo≈Ça:
Ok will check on that as soon as will be able to!

> Kamil Chodo≈Ça:
Seems weird

> Pari:
resync started

> CPerezz:
@parithoshj @pk910 Reth reports +40GB in Bytecodes (almost double of what we would want).

I find it hard that mainnet traffic is adding this much data to bytecodes. But we also aren't running any scenario that deploys as Pk said.

So, what is going on with Bytecode size?? 

Also, how's the process going? Wondering when we can turn on bloating again. We should be done with accounts by the end of the day if we let it run

> Kamil Chodo≈Ça:
I see that Besu still struggles and Geth is out of sync - will check if there is anything I can do here

> ahamlat:
besu node is waiting for newPayload calls from nimbus. Nimbus was very close to the head few minutes ago and now is syn'ced. I still don't see newPayload calls on besu side

> ahamlat:
let me restart besu

> Pari:
Im looking into geth right now

> Pari:
if either of the two works, then we should finalize and can start spamming again

> ahamlat:
I see these logs on Nimbus side 

DBG 2025-07-21 08:26:00.071+00:00 newPayload: inserting block into execution engine topics="gossip_blocks" executionPayload="(parent_hash: \"e6cdd37b\", fee_recipient: \"(data: [134, 207, 1, 111, 184, 115, 213, 10, 123, 143, 49, 235, 21, 76, 146, 52, 221, 49, 176, 88])\", state_root: \"a232f402\", receipts_root: \"835ec4f3\", prev_randao: \"69630eb8\", block_number: 22883799, gas_limit: 1000000000, gas_used: 287008, timestamp: 1753086360, extra_data: \"Nethermind v1.33.0a\", base_fee_per_gas: \"7\", block_hash: \"ead6a216\", num_transactions: 7, num_withdrawals: 16, blob_gas_used: \"0\", excess_blob_gas: \"0\")"
DBG 2025-07-21 08:26:00.071+00:00 Sending JSON-RPC request                   topics="JSONRPC-HTTP-CLIENT" address=ok(snooper-engine:8561) len=5871 name=engine_newPayloadV4 id=14357
DBG 2025-07-21 08:26:00.075+00:00 newPayload: succeeded                      topics="gossip_blocks" parentHash=e6cdd37b blockHash=ead6a216 blockNumber=22883799 payloadStatus=SYNCING
DBG 2025-07-21 08:26:00.119+00:00 Block processed                            topics="gossip_blocks" head=28ca97cd:93275 blck=28ca97cd:93275 validationDur=1ms980us481ns queueDur=170us88ns newPayloadDur=3ms593us773ns addHeadBlockDur=39ms153us216ns updateHeadDur=4ms957us367ns

what is snooper-engine ?
besu is listening on 8551 from what I see on the VM.

> Pari:
its a MITM proxy that logs all the engine API calls

> Pari:
docker logs snooper-engine -f --tail=20 to see all calls happening on the engineAPI

> ahamlat:
@parithoshj did you delete besu database when you installed the new docker image ?

> Pari:
Yup, it should have triggered a fresh sync from its peers

> Pari:
prysm-geth-1 is now moving ahead, it should sync up to the canonical head in a bit. it reorged successfully without a resync once i forced the Cl to give it a new fcu.

> CPerezz:
That's awesome! 
Also, I assume we can conclude that all this was due to the gass-diff cost. Is that correct? Or are there still any investigations we should conduct urgently (prior to re-bloat)?

> Pari:
That seems to be it. i need to still see why the wrong image tag was pushed on our builder, but that shouldnt have anything to do with the clients.

> Pari:
Final confirmation would be once besu reaches head or processes those blocks ofc. The main thing is that its not a mainnet bug, so its something we can sort out in the purview of the devnet.

> Kamil Chodo≈Ça:
THis was the exact tag used for Besu which appeared to be wrong?

> Kamil Chodo≈Ça:
Looking at docker hub it should be completely fine....

> ahamlat:
Yes, that one was using the wrong branch : performance-modexp

> ahamlat:
docker.ethquokkaops.io/dh/ethpandaops/besu:performance-d770152 is reference the good commit on the good branch

> Kamil Chodo≈Ça:
Yeah that is using indeed wrong one but looks like built correctly... Not sure, need to investigate a bit why did that happened.

> Kamil Chodo≈Ça:
Ahhh see something

> Pari:
My guess was that it was a manual job triggered by mistake

> Pari:
the builder config all looks fine to me

> Kamil Chodo≈Ça:
Had same impression tbh - but don't see anything from logs which can give a clue

> Kamil Chodo≈Ça:
TBH retriggered a build

> Kamil Chodo≈Ça:
To refresh this image

> Kamil Chodo≈Ça:
Some docker weirdness tbh - Rebuilt the docker, removed container and repulled same tag - still invalid version.
Did docker system prune and started besu again - now is on valid one

> Pari:
let me fix it on dockerhub and ask for the cache (we use harbor to avoid ratelimit issues) to be invalidated

> Kamil Chodo≈Ça:
I did above locally for me

> Pari:
yeah ill do it at the source and then we try again

> Kamil Chodo≈Ça:
Anyways - triggered rebuilt of besu on performance and now is fine - maybe we can add a more dynamic name in build actions so we can have more context abotu a run jsut from name perspective

> Kamil Chodo≈Ça:
Let me prep a PR

> Pari:
both fixed now:
docker run ethpandaops/besu:performance --version
besu/v25.7-develop-d770152/linux-aarch_64/openjdk-java-21


docker run docker.ethquokkaops.io/dh/ethpandaops/besu:performance --version                              
besu/v25.7-develop-d770152/linux-aarch_64/openjdk-java-21

> ahamlat:
@parithoshj related to besu node, it seems to be a peering issue. Most of the peers are from mainnet, and I'm struggling to check the head on Bloat network, explorer is not working. Also, besu node created an invalid block. It would be better to regenerate besu enode. To do so, could you delete key file inside besu database folder. Also could you change --sync-min-peers=1 (it is currently 5) and restart the node üôè

> Pari:
head can be checked here: https://dora.perf-devnet-2.ethpandaops.io/

and sure! ill regenerate the enode and add the flag

> ahamlat:
I meant the head of the connected peers, so check if the head of the connected peers are on mainnet or on bloat network. https://explorer.perf-devnet-2.ethpandaops.io/block/0x9493557872aec61394a89ac5584ff11a449f051c88309ccc9a5109c5d3c0ad42 was not working for me.

> Pari:
Yeah we dont run blockscout because thats basically a mainnet EL explorer and those are very hard to run.

you can find EL heads here: https://dora.perf-devnet-2.ethpandaops.io/clients/execution or here: https://forkmon.perf-devnet-2.ethpandaops.io/

> Pari:
flag added and key file deleted so it should regenerate enode.

> ahamlat:
It is still struggling to find useful peers. Are these bootnodes still correct : 

--bootnodes=enode://22c1dd13d7738ae461740bbe1340733bdabe88c0682b5788bb308bb80f0d6281085196600416f187dc0f1d5d9613b1c2590e6e5b4003329a464910f613bd096a@157.180.14.229:30303?discport=30303,enode://59131cd68e4948a8e06343175bfd76eb5e0676eaed156092b0f80dc465b4c0584b4b91bee6d7d8f15132bec74c978d892cc56b918cd5fe6ff1c808ff3d041dad@157.180.14.225:30303?discport=30303,enode://d54f2c88dad80719c6f2be2c9925c6c2609f845ed84c9e2f961c2a7e67bfe218a657118a70a45d1b245e96e2678b0276ca2ca869d7aa5688fcb1a52381d6a1e3@157.180.14.230:30303?discport=30303,enode://13965dbb025ed9b18ec450f681b9dc035d100b1772faf5e72c5560b199f895619fba006a5a121c28fbe5806cf7a9dddb250a32604387f5cfc08e8bdffac1e961@157.180.14.226:30303?discport=30303,enode://292d8de2de3977c2a4c136a66e81da3896cea872a3b95451d4aa1737654e39dfea0ff1d060a0fba780d7bb7b16518202d0609a2756775bc633d0d5d95d5f6bb3@135.125.119.2:60303

 ?

> Pari:
Yup, but i can quickly refresh them!

> Pari:
^ updated, if there's no diff then it wont change

> ahamlat:
Ok, going to sync a node a my side, it is better to troubleshoot and understand why it is not able to find good peers

> Pari:
https://dora.perf-devnet-2.ethpandaops.io/

geth is synced now, we should finalize in a bit. it was indeed a CL peering issue, just rebooting the CL was enough to get the CL chains to reconcile. There was no resync performed, geth just reorged through.

> MariusVanDerWijden:
How deep was the reorg?

> MariusVanDerWijden:
Do you have logs for the reorg? These might be valuable for informing state bloat work

> CPerezz:
Quick question: Are there ways to test reorg already established? I wonder if this is a good way to test reorgs as I'd like to have clients compacting/prunning while this happens and see if the chain survives the reorg.

> Terence:
could you use the relationship of latest block.TimeStamp vs block.Number, this applies for both reorg and a skipped slot though

> CPerezz:
Seems much simpler! Thanks.

@parithoshj do you happen to have any tool that helps with these kinds of tests?

> Pari:
Yup, to do it you'd need kubernetes though and i dont think we've tried such large networks for it. It involves splitting the network in 2 and putting it back together to test a reorg. 

I think we might need a smarter way for this size of state :/ Probably something in hive might be a better way

> Pari:
WARN [07-21|08:33:55.438] Large chain reorg detected               number=22,877,073 hash=c3d47c..a84ef6 drop=1678 dropfrom=1bc674..68bb8e add=1 addfrom=92e857..c34d8d

> Pari:
so metrics around that time should tell us more about the node

> MariusVanDerWijden:
1600 block reorg, not bad

> MariusVanDerWijden:
Can you send me the log lines before and after?

> Ben {chmark} Adams ‚ü†:
I assume would be issue to do now; but if we restart a new bloatnet could we use a different networkId than 1? (easier on peering)

> Pari:
we cant really, its a shadowfork so that we can start with a realistic state

> Pari:
INFO [07-21|08:29:22.124] Reenabled snap sync as chain is stateless
ERROR[07-21|08:29:22.124] Downloader sync mode changed mid-run     old=full new=snap
WARN [07-21|08:31:25.109] Beacon client online, but no consensus updates received in a while. Please fix your beacon client to follow the chain!
WARN [07-21|08:33:54.429] Truncating freezer table                 database=/data/geth/chaindata/ancient/state table=storage.index items=1,489,351 limit=1,487,801
WARN [07-21|08:33:54.455] Truncating freezer table                 database=/data/geth/chaindata/ancient/state table=account.data items=1,489,351 limit=1,487,801
WARN [07-21|08:33:54.481] Truncating freezer table                 database=/data/geth/chaindata/ancient/state table=storage.data items=1,489,351 limit=1,487,801
WARN [07-21|08:33:54.505] Truncating freezer table                 database=/data/geth/chaindata/ancient/state table=history.meta items=1,489,351 limit=1,487,801
WARN [07-21|08:33:54.530] Truncating freezer table                 database=/data/geth/chaindata/ancient/state table=account.index items=1,489,351 limit=1,487,801
INFO [07-21|08:33:54.559] Importing sidechain segment              start=22,877,074 end=22,877,074
WARN [07-21|08:33:55.438] Large chain reorg detected               number=22,877,073 hash=c3d47c..a84ef6 drop=1678 dropfrom=1bc674..68bb8e add=1 addfrom=92e857..c34d8d
INFO [07-21|08:34:00.073] Imported new chain segment               number=22,877,074 hash=92e857..c34d8d blocks=1 txs=18 mgas=394.726 elapsed=5.514s      mgasps=71.586 age=2d5h40m  triediffs=32.36MiB triedirty=0.00B
INFO [07-21|08:34:00.468] Indexed transactions                     blocks=1677 txs=285,962 tail=20,527,075 elapsed=332.163ms
INFO [07-21|08:34:00.999] Imported new potential chain segment     number=22,877,075 hash=aa042a..c7a9b5 blocks=1    txs=17      mgas=454.650 elapsed=918.725ms   mgasps=494.870 age=2d5h39m  triediffs=70.31MiB triedirty=0.00B
WARN [07-21|08:34:01.005] Ignoring already known beacon payload    number=22,877,075 hash=aa042a..c7a9b5 age=2d5h39m

> MariusVanDerWijden:
So ~5 seconds for the reorg? Thats pretty good

> Pari:
seems like it! not bad at all, i was skeptical it would even work üòÖ

> ahamlat:
üìé besu node is now sync'ing on another instance I created. It struggled to find peers then could start sync'ing with Snap sync. The only change I did is to change bootnodes to static nodes
 --static-nodes-file=/data/besu/static-nodes.json
@Pari can we try this on besu-nimbus nonde ?

> Pari:
of course! can i do static nodes without a file as well?

> ahamlat:
Sorry, we can specify them only with a file. Besu by default checks also if a file named static-nodes.json in data directory and loads it, but better make it explicit.

> Pari:
cool let me try this

> Pari:
and does it need to be a json file? We already have bootstrap_nodes.yaml and .txt on the node

> ahamlat:
Yes a json file

> Pari:
added

> ahamlat:
The node is sync'ing now üëå

> CPerezz:
Specially since we will need to trigger this when compaction is triggered in some EL clients

> Pari:
@skylenet from the team is working on the sync tool and has a v1. We want to add the metrics that would be interesting to the tool (they‚Äôre prometheus metrics, but to have everything in one place). Do you have a list of queries we can use @CPerezz ?

> CPerezz:
List of queries meaning what datapoints you should collect while running sync tests?

> Pari:
List of grafana panels that are relevant or promtheus metrics to track, i think you had a list of what clients reported + whats missing?

> MariusVanDerWijden:
Bandwidth impact on the syncing and the providing node, Time to sync, Speed of healing vs snap syncing

> ahamlat:
Do other clients have issues to snap sync on Bloat network ? besu is struggling a lot, so in this case, snap sync metrics are screwed

> CPerezz:
üñº Yup I do. 

https://hackmd.io/@CPerezz/SkzDZmngT

> MariusVanDerWijden:
Wdym screwed? Wouldn't this just imply that Besu is just not keeping up and there might be an issue there that needs investigating?

> CPerezz:
Forgot to mention ytd in ACDT. But we would appreciate if Geth would move the metrics from logs to Grafana. 

As is tedious to collect them from logs.

> Kamil Chodo≈Ça:
While bloating happens in such heavy pace nethermind cannot heal - but I think it may apply to each client for now

> ahamlat:
Yes, I meant on besu side at least, but wanted to confirm if other clients have the same issue. Also, it is struggling since the beginning, so not related to the bloatnet use case IMO

> CPerezz:
@vdWijden @KamilChNethermind @amezianehamlat @rkrasiuk @mlnfltv 

Could you double check the metrics and comment on the doc what would you add? Marius just mentioned bandwidth-related data. 

I'd appreciate if we could all collab on reaching consensus on the minimal set of data to expose.

> ahamlat:
If I'm not wrong and haven't mixed things up with perf-devnet-1, I saw blocks on the bloatnetwork around 500 mgas, are those blocks part of the test ? If it is the case, we need to report maximum newPayload execution time and memory usage. CPU usage can be very different depending on how clients use parallel executions.

> Kamil Chodo≈Ça:
I don't think it is a "test" - it is a bloating process which is heavy to the maximum.

It kind-of exposes some additional issues we will have on higher gas limit but I believe we should bloat till for example 2x, reduce gas limit to more reasonable value (100-150) and then do sync tests while doing heavy txs - so much more realistic (but still above mainnet) usecase

> ahamlat:
But we still need to snap sync those big blocks part of the bloating process, correct ?

> CPerezz:
The goal of BloatNet is not to have such loads. But we need to have them for the bloating at least. Otherwise, it will take us forever to reach 2x mainnet state

> CPerezz:
Sure. But while snapsyncing, you request ranges of addr right? How do you get affected by this? Also, block sizes weren't really massive

> ahamlat:
Yes, was thinking mostly about block size and the issue raised by Geth on 60 mgas that was fixed since. I understand that these blocks are needed to speedup the increase in state size but just wanted to raise the possible impact on snap sync from those big blocks.

> CPerezz:
What is that issue?

BTW, saw your message:

The same as above, we dont have IO metrics during compaction on a block basis.


What's the closest you have? Same question for your comments on sync

> ahamlat:
We can get read and write performance averages aver a period of time. In general, I think it will be hard to compare between different clients related to this metric because you need to record the metric exactly in the same way, which will be hard because of different languages, standards .... I suggest using the same tool for all clients related to IO performance, like node exporter and compare the change in performance between mainnet and bloat network, but also during compaction phases.

> CPerezz:
This is one of the things I wanted us to reach consensus on. 
What do other clients think would be the best way to precisely benchmark IO perf while at the same time, being compatible with other clients to compare?

> ahamlat:
@vdWijden I think you have more context around the 60 mgas issue you found on geth related to snap sync

> Pari:
Do you have the actual dashboards, panels or metric queries we can ingest though? this is just a high level overview of who has what and what we want to track

> MariusVanDerWijden:
I don't remember which one üôà

> Ben {chmark} Adams ‚ü†:
I think it was holding too many big blocks in memory? (Bad for small RAM machines, like 8GB)

> ahamlat:
Yes the range used with big blocks doesn't work anymore for some hardware specs

> MariusVanDerWijden:
Ah yes

> MariusVanDerWijden:
The issue was that if a lot of empty blocks were followed by a lot of big blocks, we would fetch a bunch of the big blocks and run oom

> CPerezz:
Will collect one by one and list in the HackMD. Give me some time! Will ping ASAP

> CPerezz:
Geth has synced fine anyways no? None of the clients that SnapSync had any issues in 1.5x state sync tests. 

Should we lower gas limit to create smaller blocks and be safe for the rest of the bloating?

> Kamil Chodo≈Ça:
Geth was syncing while no load at all generated, right?

> CPerezz:
Load was same as with nethermind. About target-gas limit IIRC. @parithoshj can confirm. But I think it was 24M gas or similar

> Pari:
Geth didnt have to sync yet because we used geth to create the original bloated state. And the archive geth i setup yesterday + the geth that reorged both did so during non finality, where we stopped bloating.

> CPerezz:
Thought we had another one syncyc when we run the whole thing. I'd expect the sync test matrix to sync all clients independently of the one that was used for the bloating.

That means we miss a baseline for Geth?

> Pari:
You're right, we do miss a baseline for geth :/

> CPerezz:
Could we sync it now and use it as baseline? Won't be 100% exact I agree. But it's better than nothing.

> CPerezz:
Prior to re-start bloatnet I mean

> CPerezz:
BTW, logs still seem to show quite some warnings for Besu @amezianehamlat 

2025-07-22 09:26:32.039+00:00 | nioEventLoopGroup-3-3 | WARN  | SyncTargetManager | Best peer has wrong pivot block (#22889369) expecting 0xcd09e164eb7ec4bd00004f109b722e114a9b24202bc37209a843ff0d7042053c but received 0x7acc7d0b5fe1860c6e943291600204a96a88fe159e053936d2e9b6d9efb19201.

> ahamlat:
Yes, that's why I was asking if other clients are able to sync without issues. Currently besu is struggling to have good peers, but it could also an issue on our side.
I started a node in parallel yesterday and it is almost in sync though

{"@timestamp":"2025-07-22T09:44:27,418","level":"INFO","thread":"EthScheduler-Services-315 (importBlock)","class":"ImportBlocksStep","message":"Block import progress: 22086201 of 22887764 (96%), Peer count: 25","throwable":""}

> Pari:
Yup, i synced an archive node for Gary. let me see if he needs it and if not i can use geth there instead.

> Kamil Chodo≈Ça:
I feel like actual baseline we can do anytime on same grade machines, right? I mean using mainnet snapshot from a month ago vs now will be almost no difference, we can do shadowfork and see - should give identical results

> Pari:
Yeah i think its late at night for Gary already. Lets move ahead without Geth, so once Besu is fine with it - we should just continue with bloating and gather the geth baseline later on. Sorry for the oversight there!

> CPerezz:
No worries. We might as well just get the baseline from mainnet. We are decently close to 2x that it might not even make sense to collect it if we move on without them

> CPerezz:
Did you take a snapshot at 1.5x and stored it somewhere @parithoshj ? If not, we should def do it at 2x. We want to force deep branches and similar stuff and we will corrupt the DB by doing so. Which means we should have a backup

> CPerezz:
Can't you set start-end time to the timespan in which a block is processed?

> Kamil Chodo≈Ça:
Few ideas/thoughts
1. Sync depends heavily on peering - sync tests maybe should only sync from single peer to reduce the randomness + stress test serving
2. Baseline should be collected from mainnet - best also from single peer to have the same approach of measuring
3. 2x should be persisted for future use
4. We should be capable to collect client specific metrics and answer a fundamental questions "When sync will start to break or slows down significantly?", "Is 2x state makes system 2x slower (in terms of sync and in terms of normal day to day activities like block processing, production, rpc)?", "How 2x and higher will affect home stakers - do we need to consider increasing recommended hardware etc?"

> Pari:
we havent at 1.5x, the idea was to do so at 2x

> Pari:
cc @skylenet as inputs for syncoor

> Rafael Matias:
Happy to add anything like that if clients support the ‚Äúsync only from one peer‚Äù feature. But that would mean that we need some fixed good peer to serve all the requests. And results will then depend heavily on that single peer.

> Pari:
Yeah --maxpeers would be a crude analog, but then it would depend on the peer on the other side.

> Kamil Chodo≈Ça:
I test snapserving in nethermind this way so can tell you how to do that in easiest way

> MariusVanDerWijden:
You can disable networking in geth and only add a trusted peer

> Kamil Chodo≈Ça:
If we will manage to do that - we wil be able for example to check how quickly geth/nethermind/besu serves data

> Kamil Chodo≈Ça:
And then using one of them we can check the sync performance

> Kamil Chodo≈Ça:
As should be moreless the same everytime - we can find some interesting bottlenecks

> CPerezz:
We can probably then have 2 different sync test scenarios.

Single peer one which allows for perfect comparison between different state-sizes and loads.

Multi-peer one which allows us to see more "unpredictable" and complex scenarios for the node. 

Would that make sense?

> CPerezz:
I don't have a good feeling of going single-peer only TBH. 

QUite distant from reality in mainnet

> Kamil Chodo≈Ça:
With bloatnet we will always be quite far unless we will be able to provide 50-100 peers

> Kamil Chodo≈Ça:
So it would need to scale quite a lot

> Kamil Chodo≈Ça:
Multipeer ofc is nice to check as well - but single-peer will give you more clarity on results

> Kamil Chodo≈Ça:
Single Geth as serving node
then geth syncing from geth let's say 5 hours
neth synging from geth 4 hours
besy sycning from geth 3 hours

And then we can choose different server and see if results matches (and if not crashing at all)

> Kamil Chodo≈Ça:
With multipeer you may end up being synced well which is good - but you may miss that for example nethermind and besu breaks on SnapServing and we rely only on Geth again

> Kamil Chodo≈Ça:
And is we really want to catch

> ahamlat:
We do have block execution time, but getting slot reads metrics for example can have a lot of overhead, as it can be thousands

> ahamlat:
It is still doable, but in general, we agree on Kamil's points. We were discussing similar items inside besu team :
- Having mainnet as a baseline. Maybe it could be interesting to have another shadow fork that has the same state as mainnet once bloat reaches 2x mainnet state. This is to compare the performances of similar blocks between bloat network and the mainnet shadow fork
- The impact on node performance, which can be assessed with either mgas/s or block execution time on block processing.
- The impact on sync time by comparing client x against client x, ex. besu with besu. This is because it depends a lot on how metrics are collected and how sync is implemented.
- The impact on RPC performance (Node x on bloat vs Node x on mainnet with the same load, x is a client)
- The impact on database growth 
- Compaction and R/W performance metrics could be interesting but high level metrics are better IMO, as those metrics are included in the high level metrics (Sync time, block processing performance, RPC performance ...etc)

> Kamil Chodo≈Ça:
DB metrics are nice to follow if we have such an option - but focus on end user "visible" things like sync time, execution time, rpc time + the issues which can appear and make all these to perform slower under specific circumstances. All the metrics will help to analyze the issues - but focus on analyzing these points first to see if we need to be any concerned at the moment and make an input for any further gas limit increase

> Kamil Chodo≈Ça:
BTW @parithoshj @skylenet Would be great if syncoor will allow me to quickly retest issues like this which now requires me to set up whole infra for 2 clients and play with it quite a bit :D 

https://github.com/NethermindEth/nethermind/issues/5482

> Kamil Chodo≈Ça:
Or if not quickly (as it is still mainnet sync) at least will allow me to check if the connectivity between clients is good and no weird disconnections happened if all the clients are "healthy" and follow the chain well without any reason to be disconnected

> Pari:
hmm, not fully sure syncoor is the tool for this. its sort of like snapshooter, its meant for one purpose and we can't overload it too much. 

However raw kurtosis should be enough to quickly test this right?

> Kamil Chodo≈Ça:
Wondering if these issues are easily reproducibklt on empty state - some of them rely on what is happening on mainnet and the scale here

> CPerezz:
üñº Hey!  To start prepping for coding scenarios within EELS for BloatNet (besides what we already have) I've setup a doc with questions for all client teams.

Please, I'd appreciate if teams could take 10 minutes of your time to go through the questions for your EL client and try to elaborate  on your replies. Mainly, because you know the deep details of your client impls and tradeoffs you took. And this is something extremely helpful for me to then create test cases to stress the nodes with the best scenarios possible.

If you have any doubts or questions, please, DM me anytime! Link: https://hackmd.io/@CPerezz/HkZlj_jIgg. (you all should have write access).

Thanks so much in advance!!

cc: @vdWijden @amezianehamlat @mlnfltv @gakonst @rkrasiuk @KamilChNethermind

> ahamlat:
@parithoshj @KamilChNethermind just to let you know that the node I started on my side on perf-devnet-2 is sync'ed and executing head blocks 

Imported #22,894,796  (36f3c.....7ae5d)|    4 tx ( 75.0% parallel)| 16 ws|  0 blobs|      7 wei bfee|     125,260 (  0.0%) gas used| 0.008s exec|  15.66 Mgas/s| 25 peers
FCU(VALID) | head: 36f3c.....7ae5d | safe: ae940.....6c606 | finalized: a6a85.....14ac6

> CPerezz:
Restarting spamoor then. Would you prefer smaller blocks @amezianehamlat ?

> Kamil Chodo≈Ça:
Wait - but this is separate node

> Kamil Chodo≈Ça:
Validators still are not working and also Reth styruggles from previous epoch,,,

> Kamil Chodo≈Ça:
@amezianehamlat Can you check if this is not doing some weird arcive sync by mistake?

> Kamil Chodo≈Ça:
üñº https://github.com/ethpandaops/perf-devnets/blob/f69de637ec3f525c2041e04af9a7d7f8be7b5fed/ansible/inventories/devnet-2/group_vars/besu.yaml#L34

Should be snap but not sure, doing this:
Block import progress: 13139052 of 22894618 (57%), Peer count: 85

So seems liek still quite a lot

> ahamlat:
No, it is importing blocks during snap but really slow, as it struggles to have good peers.

> Kamil Chodo≈Ça:
OK so it needs extra time

> Kamil Chodo≈Ça:
@CPerezz looking at reth now...

> Kamil Chodo≈Ça:
Hi @shekhirin 

On Reth node on Bloatnet we are getting:

"2025-07-23T08:31:10.447622Z ERROR Error serializing response: Error("Memory capacity exceeded", line: 0, column: 0)"

Node stopped proposing blocks now and we lost finality

> Kamil Chodo≈Ça:
But seems like still following the chain

> Kamil Chodo≈Ça:
https://grafana.observability.ethpandaops.io/d/liz0yRCZA/logs-perfnet?orgId=1&from=now-30m&to=now&timezone=browser&var-network=perf-devnet-1&var-consensus=lighthouse&var-execution=reth&var-ingress_user=perf-devnet-2

> ahamlat:
Yes, @ben_a_adams mentioned the issue on peering because of keeping the same chainId as mainnet. That is hurting a lot besu nodes to snap sync

> Kamil Chodo≈Ça:
But the beggining isn't same blocks as mainnet? Only from the moment we forked out are different?

> Ben {chmark} Adams ‚ü†:
Only care about the mainnet blocks if full syncing; same chainId means you accept peer and then have to go through dance working out if on same fork

> ahamlat:
Yes during snap sync we keep connecting and disconnecting from mainnet peers, because they're on the same chainId but when besu checks the pivot block of the peers, it finds that it is different from what report other peers and ends by disconnecting it, and that process starts again. And there much more mainnet nodes then bloatnet nodes, besu struggles to keep good peers

> CPerezz:
Indeed I just saw Reth lost finality

Ohh dammit. It's a mess to halt the chains to do whatever. This is specially inconvenient with Geth forcing you to stop the node to do a DB inspection for example

> Kamil Chodo≈Ça:
Waiting for Reth to briefly let me know what it is and will restart the node afterwards

> Kamil Chodo≈Ça:
Oh - now reth started proposing blocks

> CPerezz:
Just saw!

> Kamil Chodo≈Ça:
Can it be that on bloatnet with current load sometimes Reth build so huge blocks that it cannot properly repond in GetPayload? :D

> CPerezz:
That should have happened already no? Also, we were not bloating atm so blocks hsould have been empty mostly (except maybe some pending txs)

> Kamil Chodo≈Ça:
Right...

> Kamil Chodo≈Ça:
So very interesting then - no load, yet it is a struggle here but it self healed

> Kamil Chodo≈Ça:
Also - Erigon processing empty blocks in 0,5 second?

> CPerezz:
@amezianehamlat How likely it is to have a solution for Besu's ChainID confusion issues? Wondering if we need to fix this prior to continue bloating or we can proceed without Besu and give you time while we reach 2x mainnet to try to find a fix

> CPerezz:
I see no data at all for Erigon-Teku in https://grafana.observability.ethpandaops.io/d/2k8BXz24k/reth?orgId=1&from=now-12h&to=now&timezone=browser&var-network=perf-devnet-2&var-instance=perf-devnet-2-teku-erigon-1&editIndex=1&refresh=auto

??? 

@parithoshj any ideas?

> Matthias Seitz:
hmm, the response limit for the engine server should be 750MB

> ahamlat:
tagging @siladu on this to have his opinion as well, but on my side I think we can continue like this and investigate a solution from our side to better handle shadow forks. WDYT @siladu ?

> Kamil Chodo≈Ça:
Besu seems that can be ready late today or for sure by the morning?

> Kamil Chodo≈Ça:
Will Besu sync at all if we will start doing heavy spam?

> Kamil Chodo≈Ça:
üñº Worked for me?

> Kamil Chodo≈Ça:
@FunnyGiulio @mlnfltv Can you check erigon logs? Processing empty blocks in more than 0,5 seconds

> Kamil Chodo≈Ça:
https://grafana.observability.ethpandaops.io/d/liz0yRCZA/logs-perfnet?orgId=1&from=now-30m&to=now&timezone=browser&var-network=perf-devnet-1&var-consensus=teku&var-execution=erigon&var-ingress_user=perf-devnet-2

> ahamlat:
We usually disable tx pool during sync, but it is enabled on performance branch to mitigate one block building issue, so it can have an impact on sync for sure. better to wait tomorrow if that works for everyone.

> CPerezz:
Ohh I meant the dashboards in https://grafana.observability.ethpandaops.io/d/2k8BXz24k/reth?orgId=1&from=now-12h&to=now&timezone=browser&var-network=perf-devnet-2&var-instance=perf-devnet-2-teku-erigon-1&editIndex=1&refresh=auto

> Kamil Chodo≈Ça:
Ahh right

> CPerezz:
Sure. We can wait

> Kamil Chodo≈Ça:
@CPerezz but it is Reth dashbaord, no?

> CPerezz:
Reth's one works for me. Nethermind, Geth, Erigon and Besu I don''t see any data whatsoever..

IGNORE

> CPerezz:
@mattsse worth investigating further this issue? Since we won't be bloating till we have Besu synced.

We can deeply check logs if there's interest on this self-healing

> Kamil Chodo≈Ça:
I mean is that Dashboard you sent is called "Reth" so it uses reth metrics and will work only for Reth if I'm not mistaken

> CPerezz:
You're right!

> Pari:
would trusted peers help besu right now? i can add it as a trusted peer on a healthy node

> ahamlat:
You mean on another node like geth ? that could help as the peer should not be disconnected as it is on the same and good chain. WDYT @siladu ?

> Simon | Besu:
A node on our own stack is in sync on bloatnet now, so there's not a fundamental problem AFAICT, just peering issues/a certain amount of luck.

I think @amezianehamlat used static/trusted peers to get this one going? so could do the same for nimbus-besu-1

> ahamlat:
We already have the same setup on nimbus-besu-1 for perf-devnet-1 and perf-devnet-2. That's actually what made the nodes start sync'ing as they were completely blocked

> Simon | Besu:
could add your now in-sync node as an extra static peer, might help?

> Simon | Besu:
would need to enable snap serving and maybe reboot/increase max-peers to help this one connect

> ahamlat:
Yes, good idea üëå. Will share enode to add to static peers

> ahamlat:
@parithoshj could you add this enode to static-nodes.json file on perf-devnet-2

"enode://e6f8823dbb36a33d88d441e6aba0c864e955258cd235a7267119564123faf0dae39f2b4000248cff027a36d4e32776f86bea2860ee85e0b8f3b365201314b463@34.196.213.41:30303"

> Kamil Chodo≈Ça:
Pari must have some chnages locally for perf-2 as I have for perf-1 :) So will do the same for both chains and commit to repo

> Kamil Chodo≈Ça:
wait

> Kamil Chodo≈Ça:
Of course- only for perf2

> Kamil Chodo≈Ça:
The above one :)

> ahamlat:
This is only for perf-devnet-2, it should not be added to perf-devnet-1

> Kamil Chodo≈Ça:
Restarting

> Kamil Chodo≈Ça:
What about this flag?

- --bonsai-historical-block-limit=1000

> Simon | Besu:
Can add 
--bonsai-limit-trie-logs-enabled=false 
instead to avoid pruning

> Pari:
you applied the changes right? now i have no diff to master

> Kamil Chodo≈Ça:
Yes :) Just now having some issues redeploying locally

> Kamil Chodo≈Ça:
So need to figure out why - but anyways added another static node and connected to 6 instead of 5 so should hav eworked

> Kamil Chodo≈Ça:
Don't see any faster pace in downlaoding still :(

> CPerezz:
If there  are no spammers running, how is it possible that we have Reth processing blocks with 19 txs ?

> CPerezz:
It's impossible that mempool still contains anything after 2 days of no bloating

> Pari:
Its a mainnet shadowfork, so some peers from mainnet might just be still seeding txs and they can be valid and included

> CPerezz:
Arghhh! True! MB. I keep forgetting!

> milen:
@parithoshj is there an easy way to add txn gas used to dora transactions tab? We‚Äôre missing a block explorer but i remember the answer to this was that it‚Äôs not possible to setup because it‚Äôs a shadow fork? What are the issues with that?

> Philipp (pk910):
It's unfortunately not that easy to show for dora. It can show transactions, because they're part of the beacon block,  but the used gas per tx is part of the receipts only.
There's currently no indexing for el block data. Would be a bigger thing to add - especially performance wise as there can be thousands of transactions per block

> milen:
We can use Erigon Archive + OtterScan. No indexing will be needed

> Pari:
I can see how easy it is to enable otterscanüòÑ

> CPerezz:
üñº A couple of updates for everyone!!

FIrst, a reminder for teams to review and answer the questionaire whenever they can.  That will be super helpful to start creating EELS & spamoor tests. Link to the doc is pinned in this chat. 
- Geth: ‚úÖ Thanks @rjl493456442 
- Besu: ‚úÖ Thanks @amezianehamlat 
(Will followup with both privately).
- Nethermind: ‚ùå
- Erigon: ‚ùå
- Reth: ‚ùå

Second, I've setup a list of all scenarios I so far have in mind. Feel free to take a look, critizise, improve, propse new ones, etc.. I'm the one that knows less. So looking forward for expert opinions.

See: https://hackmd.io/@CPerezz/ryoMhzaLel

> Simon | Besu:
@pandaops Hoping this will ease our peering issues: https://github.com/ethpandaops/perf-devnets/pull/19

Could I get the extra flag deployed on perf-devnet-2/nimbus-besu-1 please and the container restarted. No image update required as it's existing functionality

> Pari:
Yup deploying now

> Pari:
I had to regenerate the enode, but it seems to be progressing now!

> CPerezz:
Snapsync seems to be working well indeed!Awesome! Seems we will be able to continue the bloat today!!!!

> Pari:
The refresh only helped for a bit @siladu and @amezianehamlat , it quickly goes from ~3 peers to 1/0 peers and stops syncing again. Is besu just too aggressive in requesting and hte peers ban it?

its extremely slow syncing even now

> CPerezz:
Dammn true. Lost all peers and has progressed <1% in the last 1'5h

> Pari:
I'd wait for @KamilChNethermind to confirm, but shall we continue with bloating? we can hit the 2x mark and then stop and debug besu issues in the meantime

> Kamil Chodo≈Ça:
Yeah seems reasonable - we can have 2x in a matter of 2-3 days @CPerezz ?

> Kamil Chodo≈Ça:
We can always redo the test for Besu - and it will be still reproducible on bigger state and we can start doing other things in meantime

> Kamil Chodo≈Ça:
So let's resume bloating before chain will go to nonfinality again :D If we need, maybe we should move keys temporarily to other node

> CPerezz:
If we can bloat at 800M I think so

> Kamil Chodo≈Ça:
Let's do it then :)

> CPerezz:
@amezianehamlat are you ok with fast bloating? It wasn't clear to me if you preferred to go slower

> ahamlat:
Yes, it should be slower. I think we can start without besu and even if is slower, it will catch up. The node I started on my end was much quicker to sync, but not sure what is making that node that slow.

> CPerezz:
Spamming started! Will leave 1 more day approx of account bloating. Then leave it up to contract storage only

> Kamil Chodo≈Ça:
Let's reach 2x and then continue on helping besu rejoin properly so we can also in parallel start setting up tools to utilize 2x state

> Pari:
btw i think erigon can't handle this load (we're at 500M+ gas). Its not producing blocks again.

> Kamil Chodo≈Ça:
I think we should move keys to unstuck ourselves

> Kamil Chodo≈Ça:
Give it to geth/reth/neth for now

> Pari:
we're at high attestation levels, thats fine

> Pari:
its about proposals

> Pari:
i can move keys if we want high proposals too

> CPerezz:
Hmmm. I'll leave?

> Kamil Chodo≈Ça:
Ahhh right - so keep it until is finalizing

> CPerezz:
We will re-run all these high load benchmarks (at lower levels). So we aren't missing key data

> Kamil Chodo≈Ça:
Only thing here would be that reth/neth/geth will get higher load

> CPerezz:
Anyways @mlnfltv do you have strong opinions?

> Pari:
Yeah our options are reducing load or to move keys around (or leave as is). All are fine tbh.

> Kamil Chodo≈Ça:
Leave it for now? If progressing fine

> milen:
No strong opinions. Whatever you decide. We‚Äôve identified what is causing us to fall behind (it is our trie root calc). We already have a project underway about this so it‚Äôs a known thing we need to improve. It‚Äôs sort of halfway there but not quite finished yet. It‚Äôs something that will take time - weeks, a month, maybe few months, can‚Äôt promise a timeline at the moment. Not much we can do in the short term. So feel free to take whatever direction you decide.

> CPerezz:
Thanks for the report. Will list this in the findings report

> CPerezz:
@KamilChNethermind can you give me an invite link for the group? Or make me adming? Whatever is easier üôÇ

> Simon | Besu:
With no further changes (but I do think --required-block helped), nimbus-besu-1 eventually got in sync and has been proposing blocks since ~1 hour ago
https://dora.perf-devnet-2.ethpandaops.io/slots/filtered?f=&f.pname=besu&f.missing=1&f.orphaned=1&c=50&d=1+2+3+4+5+6+7+8+9+10+18+11

> Kamil Chodo≈Ça:
Besu also proposing again at perfnet-1 :)

> Kamil Chodo≈Ça:
But on Bloatnet I see now Geth and Erigon struggle a lot and we are on non-finality

> Kamil Chodo≈Ça:
cc @CPerezz @parithoshj

> Kamil Chodo≈Ça:
First we lost erigon about 5h ago and 2h ago we lost Geth

> Pari:
Checking

> Kamil Chodo≈Ça:
yeah looking as well

> Kamil Chodo≈Ça:
Just had some struggles with connection to logs

> Pari:
Prysm has no peers, gonna restart it and see if it helps geth

> Pari:
Prysm is indeed fetching blocks now and peered up again. I would assume geth is at head soon.

> Kamil Chodo≈Ça:
I see geth improts blocks well

> CPerezz:
What is going on with peers

> CPerezz:
??

> CPerezz:
Dod that happen in perfnet1 too?

> Pari:
Prysm-geth should be back now.

> Pari:
I'm not sure tbh, this is the second time perysm is having a peering issue on this network. It could be that some timeout is leading to down scoring? we are running the network at way higher loads than its meant to be run üòÖ

> Pari:
either ways, its temporary - we can continue spamming for now

> CPerezz:
So Geth was peering issues and Erigon what we already know

> Pari:
it wasnt geth wtih the peering issue, but prysm - either way same outcome

> CPerezz:
Fair enough. MB

> Pari:
Yup its proposing again, we should finalize in 2 epochs

> CPerezz:
I dont see how peering and state/high loads can be related TBH

> CPerezz:
Any ideas? Blocks aren't massive so should not be a compute issue

> CPerezz:
Except for Erigon for state root

> Kamil Chodo≈Ça:
Prysm as I recall have this "peer score" thing - I recall there was a option to disable that?

> Kamil Chodo≈Ça:
Maybe gets worse overtime on shadowfork?

> Pari:
We're at ~500M gas blocks, so there's inherently more load there. But we also have to pass a ton more via the engine API. Combine that with a very small network and timeouts, and then you'd end up with more chances to downscore peers.

> Pari:
Yeah if it happens again i can look into it

> Pari:
did someone just restart geth 2min ago?

> Kamil Chodo≈Ça:
nope not me

> CPerezz:
More than in mainnet? (once we have the high load)

Curious on how networking can be worse here. Is it just a matter of less peers = more work for them?

> Kamil Chodo≈Ça:
It may be a lot of factors - see besu just catched, erigon struggled to process so automatically CL peer may be less valuable, Reth I see was missing quite a lot of blocks 2-3h ago so also was treated "worse"

> Kamil Chodo≈Ça:
And every single time one node "struggle" it is liek 20% of network struggles - may be some small tweaks to be done for CLs and ELs in networking layer to make network more resilient here - probably worth to have CL devs looking even more for a situations where we propagate super heavy blocks (in terms of payload size, not necessarily gas heavy or compute heavy) and see if any even smallest "hickup" for CL does not make the peer completely thrown out by some other CLs - and then some sort of domino effect starts

> Pari:
Yeah on mainnet it doesnt really matter if you ban one peer, there are like ~15k others to choose from. But on this network you have to imagine you're banning ~40-50% of the network, or deeming them unsuitable rather than banning. We're also wasting a ton more time on serializing/deserializing than we would on mainnet, i haven't looked but its likely going to make up a bigger % than on mainnet - so there's just more wasted time that's unexpected.

> Kamil Chodo≈Ça:
But is actually a valuable input - CLs could harden the peering a bit to not make too fast decisions on some peers which have temporary problems

> Kamil Chodo≈Ça:
BTW - Prysm issue alligns perfectly with Reth having problems with blocks proposals

> CPerezz:
Ye, I was wondering what will happen when BALs come into play in Glamsterdam

> CPerezz:
As with state-diff BALs and 100M big blocks. It can def be an issue for peer scoring.

@parithoshj should we ping Raul about this? Or anyone in CL?

> Pari:
wdym? nothing about peerscoring is broken, its functioning as expected. We're using a really small network and just seeing outsized effects. 

Why would BALs change the peer scoring topic?

> CPerezz:
Because it won't just be big blocks. It will also be BALs propagated alongside them

> CPerezz:
Which I'd expect to even make peerscoring get worse. Specially for homestakers. Or low-bandwidth validators.

> CPerezz:
Also, 

25 Jul 09:36:52 | Error response handling JsonRpc Id:335966 Method:eth_sendRawTransaction | Code: -32010 Message: AlreadyKnown 


A lof of these in Nethermind. Any ideas what is going on?

> CPerezz:
Just logging that it already has this tx? @KamilChNethermind

> Kamil Chodo≈Ça:
Likely some rebroadcasting and Neth already had that one

> Pari:
I haven't looked at BALs yet, but I highly doubt peerscoring would be the issue. We just need to get better propagation on the p2p layer or reduce gossip amp. These are topics that are being worked on irrespective of peer scoring though or BALs.

> Philipp (pk910):
This is quite normal with spamoor, as txs are initially sent to 3 clients, and then re-submitted every 30-60sec till inclusion.
Seems a bit unnecessary, but has quite a positive effect on the propagation & inclusion rate.

> CPerezz:
Kind reminder to Nethermind, Reth and Erigon to please take a look to this so that we can resume the reviews & follow-ups and coding the scenarios in collaboration with EELS teams.

cc: @KamilChNethermind @rkrasiuk @mattsse  @mlnfltv

> Kamil Chodo≈Ça:
Will do today - we have small emergency which we need to handle first

> CPerezz:
Thanks a lot!

> Pari:
How far away from 2x are we @CPerezz ?

> Pari:
Can someone from reth look into lighthouse-reth-1.perf-devnet-2? 

https://dora.perf-devnet-2.ethpandaops.io/slots/filtered?f=&f.pname=reth&f.missing=1&f.orphaned=1&c=50&d=1+2+3+4+5+6+7+8+9+10+18+11

The CL says its synced, but its clearly failing proposals. The failure is always with unknown payload and its timeout related (i'd guess). I want to know if its the CL or EL that needs to look into it.

> Pari:
I guess EL if reth emits this log?
2025-07-28T08:47:05.982258Z ERROR Error serializing response: Error("Memory capacity exceeded", line: 0, column: 0)
Maybe the payload is just too large to serialize? @shekhirin

> Alexey Shekhirin:
i'm OOO until next week, cc @mattsse @rkrasiuk

> CPerezz:
I think we are there right? 

I want to keep accounts until a couple more GB. But we should be good to go.

At least it appears so from Reth's DB tables panel

> Pari:
Okay, lets stop the spamming and go down to ~45M once we're at 2x then. We can focus on making sure all nodes are at head then and take a snapshot for the future.

> CPerezz:
I think we can leave the account spammer running. And stop prior to ACDT to then kickstart snapshot saving.

> Pari:
First we have to get all clients to head üòÖ
i think doing anything at 500M gas won't help with that

> Pari:
@rjl493456442 Geth seems to be in a restart loop now:
Fatal: Failed to register the Ethereum service: decoding SET value: pebble: invalid batch
INFO [07-28|09:06:35.795] Maximum peer count                       ETH=200 total=200
INFO [07-28|09:06:35.796] Smartcard socket not found, disabling    err="stat /run/pcscd/pcscd.comm: no such file or directory"
INFO [07-28|09:06:35.797] Set global gas cap                       cap=50,000,000
INFO [07-28|09:06:35.797] Initializing the KZG library             backend=gokzg
INFO [07-28|09:06:35.798] Enabling metrics collection
INFO [07-28|09:06:35.798] Enabling stand-alone metrics HTTP endpoint address=0.0.0.0:6060
INFO [07-28|09:06:35.798] Starting metrics server                  addr=http://0.0.0.0:6060/debug/metrics
INFO [07-28|09:06:35.798] Allocated trie memory caches             clean=154.00MiB dirty=256.00MiB
INFO [07-28|09:06:35.810] Using pebble as the backing database
INFO [07-28|09:06:35.810] Allocated cache and file handles         database=/data/geth/chaindata cache=512.00MiB handles=524,288
Fatal: Failed to register the Ethereum service: decoding SET value: pebble: invalid batch
Fatal: Failed to register the Ethereum service: decoding SET value: pebble: invalid batch

I noticed that the node wasn't at head, so i restarted the CL/EL to get it a new fcu. The machine still has ~2TB free, so it isnt disk space related. 

Here's the entire logs before the restart when it stalled: https://drive.google.com/file/d/1qKUxraVcyXk2DvfWJvgfIlgq7NuE9Z5r/view?usp=sharing

> CPerezz:
Oh no. I stopped bloating 30 min ago more or less. Only 150M are going through

> CPerezz:
I can stop the 150M of account 7702 bloating if needed btw. Just let me know if that will help the CL to catch up

> Pari:
Right now it seems to not matter, we have 3 open issues worth investigating:
- Geth reboot loop
- Reth timeout on building blocks
- Erigon keeping up with head (already known and looked into)

None seem strictly CL related as of now

> CPerezz:
@rjl493456442 @vdWijden Can any of you take a deeper look into it?

> MariusVanDerWijden:
Yes taking a look, it looks like the db is fried

> MariusVanDerWijden:
I see around 55 unclean shutdowns in the logs
The shutdown seems to occur without any logs

> MariusVanDerWijden:
This is most likely OOM

> MariusVanDerWijden:
What I also see is a bunch of WARN [07-24|15:46:10.891] Block building is interrupted            allowance=2s which means the geth node will produce an empty block

> MariusVanDerWijden:
Because block building takes over 2 seconds

> Pari:
üñº Its an 8 core 32GB RAM machine, very close to recommended specs. And yeah the RAM use pattern seems insane

> MariusVanDerWijden:
It would be great if we could get a memory dump from the machine, but I guess its not easy to capture the spikes

> Pari:
its also in a reboot loop, so that wont help. However we also have a bootnode that runs geth, let me see how thats doing

> CPerezz:
This happened the 26th and 27th. Unsure if we have that much traceback for a memdump no?

> Pari:
and here's the geth dashboard: https://grafana.observability.ethpandaops.io/d/Jpk-Be5Wk1/geth-dual?orgId=1&from=now-7d&to=now&timezone=browser&var-exp=perf-devnet-2-prysm-geth-1&var-master=https:%2F%2Fgrafana.finance.ethpandaops.io&var-percentile=0.5&refresh=5m

> Pari:
üñº Yup, similar pattern on the bootnode. Let me see how the logs look

> MariusVanDerWijden:
But the bootnode has not crashed?

> MariusVanDerWijden:
GImme access

> Pari:
it crashed 17h ago apparently

> MariusVanDerWijden:
but not recently?

> Pari:
Nope, and it seems to be at head. So probably useful to get the memory dump from it. 

You should have access now

> CPerezz:
So in the 27th, we processed a 5.1Ggas block. 

Likely a result of random stuff happening. But seems quite insane Could that be related?

> MariusVanDerWijden:
Oh the bootnode also has a tracer enabled? Thats a bunch of additional allocations, does the other one also have the tracing?

> Pari:
nope, the other has these flags:
geth_container_command_extra_args:
  - --http.api=eth,net,web3,debug,admin
  - --http.vhosts=*
  - --networkid={{ ethereum_network_id }}
  - --bootnodes={{ ethereum_el_bootnodes | join(',') }}
  - --syncmode=full
  - --miner.gaslimit=1000000000
  - --txpool.globalqueue=10000
  - --txpool.globalslots=10000
  - --maxpeers=200

> Pari:
üñº ah the trace calls are probably coming from tracoor: https://tracoor.perf-devnet-2.ethpandaops.io/

i started it up because we had the issue last week with besu

> MariusVanDerWijden:
80% of all memory allocations come from the tracer :D

> Pari:
Should i turn it off? we'd run into the issue that if there's a problem then we can't solve it easily

> CPerezz:
How is that even possible? Are you skipping freeing memory or something?

> CPerezz:
I mean, I'd expect a tracer to report the data. And then free the memory. If tracoor is pinging every block time, that should not lead to such issues no?

> MariusVanDerWijden:
No its constantly allocating and deallocating. No if thats not happening on the other node than we shouldn't disable it

> CPerezz:
Wow

> CPerezz:
Does tracoor have the same effect in other clients @parithoshj ?

As this makes the measurements quite distant from reality no?

> Pari:
Seems fine on the other clients, also the focus for now is to get the state to 2x and not really measure performance. Agreed that when we get to the testing stage we can disable it.

> Pari:
Nethermind has a different memory load since the tracoor was started, but besu/erigon(out of sync i guess) and reth seem fine.

> CPerezz:
I'll still note down that tracoor seems to mess Geth quite badly.

Even if it's just to leave an issue there if Geth team of PandaOps wants to tackle it at some point

> CPerezz:
Also, how should we proceed to start getting in sync Besu and Erigon? Any particular procedure to follow? I can significantly downgrade all throughput leaving only blob txs.

> Kamil Chodo≈Ça:
If we are on 2x we should stop all spamming including blobs

> Kamil Chodo≈Ça:
If the 2x is well aligned with mainnet data distribution

> Kamil Chodo≈Ça:
Then revive all ELs and persist DB

> Kamil Chodo≈Ça:
So disable all to allow seamless sync for now and then start playing with more specific load

> Pari:
Besu is fine, its proposing and attesting. 

Status:
- Besu fine: attesting + proposing
- Nethermind fine: attesting + proposing
- Reth: attesting but not proposing, at head (timeout bug)
- Geth: crashloop on one node, bootnode is fine and at head
- Erigon: ~700 blocks behind head, not attesting or proposing

> Kamil Chodo≈Ça:
Erigon will likely heal itself when load stopped?

> Kamil Chodo≈Ça:
I recall it was like this last time

> CPerezz:
So now that Geth is resolved, we only need to figure out why Reth is not proposing. cc @mattsse @rkrasiuk

> Pari:
geth is not resolved, one node has issues and other has a corrupted DB. The team is investigating rn, i guess we know its likely tracing related but its still a weird behaviour

> CPerezz:
Ohhh MB. Missread prev msg!

> Pari:
Have you already stopped the spamoor scenarios or shall i?

> CPerezz:
Noting corrupted DB! Any logs we can attach that are significant? Is it the perfnet-2 node or the archive one?

> CPerezz:
I already did. Nothing is running

> Pari:
its on the perf-devnet-2 node, Nothing stands out in the logs tbh but its over 1gb of logs so its hard to sift through

> CPerezz:
@vdWijden Any particular regex we could perform to find DB-corruption-related logs?

> MariusVanDerWijden:
WARN [07-24|15:47:18.924] Old unclean shutdowns found              count=1

> Gary Rong:
what's the geth version?

> Gary Rong:
From the log, it seems Geth has been idle for 2 hours. What happens at that time? Looks like no event emitted from the CL

> Gary Rong:
and did you terminate the geth and then restart?

> Pari:
it was the performance branch till a few mins ago, i deployed master now since marius asked for that. 

Geth has been in a reboot loop, so the CL just marks it as absent and not sending it any traffic. 

Earlier in the day, yup, i ran a docker restart execution - that the usual command we've used in the past to reboot the EL

> Gary Rong:
Photo

> Gary Rong:
after that, geth hasn;'t received any events from the CL

> Gary Rong:
the database is corrupted unfortunately. This issue is thrown by Pebble and we can't do anything about it.

Pebble has a weird restriction that the size of single database entry can't be more than 4GB. 

INFO [07-27|06:45:41.853] Imported new chain segment               number=22,913,377 hash=8cf09f..9540db blocks=7   txs=254   mgas=1503.519 elapsed=5.664s      mgasps=265.419 triediffs=3.87GiB   triedirty=0.00B     ignored=1

From the last log, it's around 3.87GB, not sure if it's relevant. But this issue has been fixed in the master branch

> Pari:
Got it! Ill resync the broken node, the healthy node is on master now anyway and should be resilient (also we likely wont spam at those levels again)

> Gary Rong:
Yeah, it's an internal issue from Pebble. Honestly It's my second time to see it. The first is also from your geth archive node. Probably it's easy to trigger some weird issues in your environment üôÇ

> Gary Rong:
I will open a ticket for tracking

> Gary Rong:
https://github.com/ethereum/go-ethereum/issues/32290

> Roman Krasiuk:
@parithoshj re "Memory limit exceeded", Matt responded here earlier

> Pari:
Thank you, it does seem that once we stopped spamming reth is fine again

> CPerezz:
@parithoshj Seems all clients are synced completelly.

Also, just saw this for Teku:

2025-07-29 06:22:12.003 ERROR - [31mValidator   *** Failed to produce block  Slot: 150256 Validator: b5bd2b9[0m

> CPerezz:
Should we start setting up sync tests?

> Pari:
Yup @skylenet are we ready to do a run?

> Pari:
In the meantime, can we redo some benchmarks between perf devnet 1 and perf devnet 2? The idea is that we can see the degradation with 2x state

> CPerezz:
@mlnfltv Any issues in GH I could link to this? Also, could you or someone of your team take the time to review `Erigon`s section in https://hackmd.io/WUe1KgwSS3iT8xoIbDg3Ng ??

> Kamil Chodo≈Ça:
Sent invite to Jochem so maybe he can help us with testing the exact same sceanrios across two perfnets :)

> Kamil Chodo≈Ça:
gas-benchmarks are compute ones so state size probably doesn't matter and we run them on empty state

> CPerezz:
Maybe worth testing with 2x state anyways? JIC. I agree it should not matter at all. But it is also one less image to care about no? We simply have perfnet-2 and we run it all there.

> CPerezz:
Just an idea, not strongly oppinionated

> Pari:
Sequence of events:
- Redo gas benchmarks on perfnet-2
- Sync tests in parallel to gather data
- Start figuring out state tests and how they integrate with EEST/how we can directly execute against perfnet
- Snapshot 2x state for future use

> Kamil Chodo≈Ça:
We have two ways of doing that for now:
1. Spamoor which as I recall @pk910 moved all our scenarios in automatic way from gas-benchmarks here so we can trigger it on both and see the performance (but gas limit needs to be kept high to fit all of them)
2. EEST "execute" with already moved scenarios from gas-benchmarks here - need to tak with STEEL team about that - maybe will give more precise info

> CPerezz:
I'll try to meet ASAP with EELS to understand what state-tests can/should go to EEST and which ones should be spamoor or whatever else

> Kamil Chodo≈Ça:
I believe we should rather try to go the "reproducible way" with any "benchmarks-like" tests. So preserve 2x ASAP and then get a machine on which we will be simply doing sequence like:
1. Restore EL1 DB (using OverlayFS mount)
2. Do a statefull tests 
3. Remove mounted DB to keep backup untouched
4. Start from a point 1 but with EL2 and so on.

> Pari:
Yup, got it, so we'll see if we can combine sync test + snapshot and prio that higher.

> Kamil Chodo≈Ça:
Will try to do some basic spamoor test rn

> Kamil Chodo≈Ça:
Wait - will do that once Geth will be healthy

> Kamil Chodo≈Ça:
Can we revive it or some analysis are still ongoing?

> Pari:
i triggered a resync last night

> Pari:
its 76% done

> CPerezz:
Have we stored a snapshot of the DB?

> Kamil Chodo≈Ça:
So will wait for that then :)

> Pari:
not yet, im making a plan for that. will present it in a bit.

> Kamil Chodo≈Ça:
Nethermind before DB snapshot creation should be also resynced (in case we pick DB from existinging node)

> Pari:
So the constraint on doing snapshots, for them to be useful (especially for shadowforks etc), we need them all at the same height. This sucks for a running network. 

We have two options:
- Sync a brand new set of nodes, use those to create snapshots (BUT this approach takes a ton of time + some clients are struggling to sync as is. Erigon for e.g needed the team to step in to get it to work, im not able to get it to work since they used some specific branch)
- I move all the keys onto the bootnode. This will keep the chain running. I then create a snapshot from all the running nodes. Once snapshot is done, we can restart the nodes and continue with devnet-2. 

IMO 2nd one is our only real option. Otherwise we run into the issue that we break the network with this state and have to startover again. The downside with the 2nd approach is that we can't do any real testing until snapshots are done (they are large, so it might take ~1d). But once they are done we will have a ton more flexibility.

> Kamil Chodo≈Ça:
Option 2 seems like makes sense tbh.

Even to consider Option 3 - just simply stop the chain at one height, get all needed and then perfnet-3 which will be a bloating continuation - but probably unnecessary extra work with setting it up

> Kamil Chodo≈Ça:
So Option2 imo gogogo

> Kamil Chodo≈Ça:
@CPerezz Will look at doc now to answer all needed questions

> Pari:
Yeah option 2 still gives us the option to do option 3 if it doesnt work out :D

> Pari:
so current approach: wait for prysm-geth to get done syncing, then take snapshot. Do tests after snapshot stage is done. Wait on pari or rafael to give the green light.

> MariusVanDerWijden:
You are pari...

> Pari:
The eest discussion can happen in parallel though. @KamilChNethermind @CPerezz where should we have this?

> CPerezz:
That's fantastic! Thanks so much!

> CPerezz:
I pinged Mario to see who shall we speak with from EELS

> MariusVanDerWijden:
Oh "please wait on Pari" not "I'm waiting on Pari to give the signal" :D

> Pari:
Its really early for him, lets ask in the STEEL team chat. Probably spencer or Dan can answer it as well.

> CPerezz:
OMW

> CPerezz:
Nevertheless, is quite obvious we can only run the single opcode tests or really easy setup tests. 

Unsure about complex long-running stuff

> Pari:
we would likely need the overlay FS stuff that kamil brought up.

> Kamil Chodo≈Ça:
I did a quick test to move spamoor scenarios from perf1 to perf2 and check if this makes any change and seems like bloatnet struggles much more on Jochem scenarios than perfnet1 but maybe i loaded 2 too much here - was a 6 sec processing from erigon at some point.

> Kamil Chodo≈Ça:
But is too chaotic and not reproducible well enough so need to work more on taht

> Kamil Chodo≈Ça:
Had a temporary drop of finality here and is now reviving

> Pari:
oh that temp drop of finality was me...

> Pari:
please refer to instructions here :P

> Pari:
I'm moving keys around to prepare for the snapshot such that the network continues on without the nodes

> Kamil Chodo≈Ça:
Ahh ok so was a coincdence then uff :D Was preapring for spamoor and wasnt looking bad when I stopped it

> Kamil Chodo≈Ça:
And thought this is some aftershock :D

> Kamil Chodo≈Ça:
But then seems okay :)

> Pari:
no no, im just ripping validator keys out so that i can make sure the network proceeds as the hosts are stopped

> Kamil Chodo≈Ça:
Good good :) Some relief :D Was very fast what I did on perf-1 so wasn;t expecting any hiccup on perf2

> Kamil Chodo≈Ça:
And it really wasn't then

> CPerezz:
Any link to Jochem's tests? Notice the network was at 800Mgas limit or 1B or some monstruosity like this. So if not configured properly, maybe it was due to that?

> Kamil Chodo≈Ça:
Nope - contract deployment was utilizing a lot but was light - calling was doing only 1 tx per block

> Kamil Chodo≈Ça:
And calling was a heavy part to be

> Kamil Chodo≈Ça:
So is good

> CPerezz:
Wow

> CPerezz:
Lmk if there's a link to the tests. Curious on how he made Erigon take 6s

> Kamil Chodo≈Ça:
May be a temporary hiccup for Erigon due to other things

> Kamil Chodo≈Ça:
https://spamoor.perf-devnet-2.ethpandaops.io/ - tests added here

> Pari:
please dont execute anything now :P
we have the entire network pointed at one node to keep block production going while the snapshots are taken

> CPerezz:
Oh wow I see! That's really cool

> CPerezz:
@mattsse @rkrasiuk @shekhirin @mlnfltv @FunnyGiulio 

We're blocked now on your answers to keep marching forward and implementing as many useful scenarios as possible.

Could you please take some time today to review your client's section in https://hackmd.io/WUe1KgwSS3iT8xoIbDg3Ng  ??

LMK if I can do something to make it easier for you.

> Pari:
Thank you for the snapshots @skylenet ! :D
We have them live here: https://ethpandaops.io/data/snapshots/?snapshot-network=perf-devnet-2

give me a bit to reorg the keys back to the original orientation, once that's done - we should be good to go again

> Kamil Chodo≈Ça:
Interesting that there is so big difference in sizes betweet clients

> Kamil Chodo≈Ça:
Did all clients had history drop?

> Pari:
we used all defaults though

> Pari:
oh but the bloating was before history drop

> Pari:
so sorry, no drop - but i can confirm this in a bit

> Kamil Chodo≈Ça:
No worries is not a big deal I think.
Before 3x we need to ensure that as it will start getting big

> Pari:
most clients should have an offline pruner, i can see how that works

> Kamil Chodo≈Ça:
Nethermind requires resync for sure

> Simon | Besu:
ahh Besu is ...bloated üòÑÔ∏èÔ∏èÔ∏èÔ∏èÔ∏èÔ∏è

This config will be adding a decent amount of unnecessary storage but we did it to make debugging/recovery easier...
--bonsai-limit-trie-logs-enabled=false

^ It basically stores the full history of state diffs as logs (not in a trie)

Normal staking nodes would only store the most recent 512 by default (but that only gives you ~1.5 hours of history for recovery).

> Kamil Chodo≈Ça:
Will the "bloated" besu be impacted perf-wise in case of tests?

> Pari:
Key reorg is done, so all validators are where they originally were. Snapshots are saved :D


@KamilChNethermind @CPerezz pls go wild on your tests!

> Pari:
I think we should repro the gas limit tests once to make sure the state doesnt have a significant effect, afaik @KamilChNethermind was doing this.

> Kamil Chodo≈Ça:
Will check if I can simply do a EEST execute on perf-devnet-2 :) Maybe quite good enough for some first checks

> Simon | Besu:
I doubt it but will double check. A trie log corresponds to a block and access is mostly individual key-value lookups during block processing, e.g. once per block. 
State access is via the flat database (flattened trie) and should dominate the disk access performance. 
Where we might do heavy trie log lookups/processing is during long reorgs or an RPC request for an older block.

> CPerezz:
Should we start wit hteh full set of tests from perfnet?

> CPerezz:
On my side I'm blocked by Reth and Erigon. 

And also on following-up with EELS

> Pari:
Yup this would be good

> Simon | Besu:
Most clients appear to have --max-peers=200 set which is high IMO esp now they're in sync and may impact CPU for the tests. Besu default is 25. Do you think we should lower before testing?

> Kamil Chodo≈Ça:
Hmmm I was changing that but can't really tell why for now - was some issue I had before.

> Kamil Chodo≈Ça:
I think we can change that

> Pari:
Yup, i can remove that flag from all nodes now

> Jochem:
Hey all üëã

> Kamil Chodo≈Ça:
Hi!

We have bloatnet with 2x state of mainnet and we want to check how this increase affects the performance of clients.

Is there a chance you can reexecute some of your perfnet-1 tests on perfnet-2 and do some sort of comparison?

> Matkt (Karim):
For me there is no impact to increase the number of trielogs. It is a separate column and as Simon said we read the flat DB for execution. 

There may be an impact on finding the trielog in the database during reorgs, but I think it should be fine.

> Matkt (Karim):
And of course an impact on the size of the database because there will be more state diff in it. Before we didn't had pruning of the trielogs and besu was fine. The only difference will be bigger trielogs in this case if transactions change more slots/accounts

> CPerezz:
I thought perfnet scenarios were mainly about compute opcodes. Not state-diff perf. üòï

> CPerezz:
Good to know! Maybe we can reuse something

> Jochem:
Hey, sorry for replying so late! I am moving and going on vacation soon but will ensure I address this before I am OOO.

For this state bloat and re-executing the perfnet/devnet-2 scenarios here are the two main things which were the general "idea" there:

(1) Loading as many contracts as possible. Due to the increased state size, if there is no flat lookup (address => code, but rather MPT lookup => code) this means this scales with the tree depth. I have to check but I think this will scale with log_16(2) here (not double size of lookup) as constant in case we do MPT lookups.
(2) doing state read/writes on contract with a huge state (like XEN)

The interesting part I'd think here is seeing the worst case scenarios with state read/writes. This might differ per client due to how each client has optimized their state read/writes and caching.

The state here is "twice the size of mainnet". How was this achieved? Based on that I can craft some targeted attacks against this new state. The alternative is to redeploy the perfnet/devnet-X attacks which requires the pre-setup of code/contracts (or redeploy XEN "attacks")

> Jochem:
So to be specific: the state is twice the state of mainnet, how was this achieved? Do we have mainnet as initial state and then deployed contracts on that? (How do I get to the state of this network, i.e. how would I reproduce the state of this network? üòÉüëç)

> CPerezz:
We have run the following:
- Significantly enlarged XEN contract
- Doing tons of SSTORE to various contracts
- Do 7702 authorisations setting EOA code
- Deploying 24kb contracts.


The bloating has been done keeping the ratio in state as in mainnet
(bytecode, accounts, contract storage). And we shadowforked mainnet and bloated it at 800Mgas approx until we got it to 2x. 

To reproduce the state you simply need to run the spamoor bloaters again and you should have a similar state with different addresses probably.

> Jochem:
Which attacks are ran on the bloatnet? One of the attacks I did on the perfnet was a massive read attack. For a 30M gas tx this read 11k+ contracts

> Kamil Chodo≈Ça:
I'd be mostly interested in XEN like scenarios assuming these things:
1. 60MGas single transaction imitating "Pectra state" - so to check how slow it will be on Pectra spec and with possibility to increase to 60 before Fusaka
2. 100Mgas block with 30+30+30+10 XEN transactions - to see the worst case for "post Fusaka" with tx max size cap and theoretical 100MGas shortly after Fusaka is live
3. Impossible yet interesting 100MGas XEN only single tx scenario - to stress test clients.

Apart of that as @CPerezz mentioned - 2x bloat is done to imitate as closely as possible current mainnet state - so in case you have any other interesting scenarios we should try to replicate them trying to assume always:
1. 60MGas on Pectra (so no tx cap limit)
2. 100MGas on Fusaka (with tx cap)

> Kamil Chodo≈Ça:
What we want to learn is simply:
1. Does 2x state slows down everything 2x (or more or less)
2. Are there scenarios which in current state size are "managable" (so for example less than 1 second execution on 60MGas) comaring to significant increase after 2xMainnet state (so on same gas limit execution will be above 3-4 seconds - so not linear 2x slow down but maybe some logarithmic slow downs)

So we need to have a way to execute very same scenarios on both perfnets moreless at the same time or in a same network state to have close comaprision

> Kamil Chodo≈Ça:
@jochembrouwer tell me whatever you need to execute that and I can provide you with anything

> CPerezz:
üñº Nothing ran yet aside from the bloating scenarios. See: https://cperezz.github.io/bloatnet-website/bloating.html


Also, the plan is to run all these: https://hackmd.io/9icZeLN7R0Sk5mIjKlZAHQ?both

I'd be super interested @jochembrouwer on discussing the scenarios with you and come up with new ones!

> CPerezz:
Suggestion from Besu:


Not sure if it is already planned or not, but how about reproducing XEN contract patterns, refund of 5000 up to 20% of the transaction gas used

> Jochem:
Small note for Fusaka we currently have 16.x M limit (lowered from 30M) https://eips.ethereum.org/EIPS/eip-7825

For the XEN-mainnet like attacks we have to "mine" XEN. This takes a day. First have to start mining and then have to redeem it. This takes a day.

I think for state there are 2 factors: (1) is the trie depth. If you double the amount of items in the trie, the depth itself does not double (this would be the case if we have a binary trie). However we have to deal with the overhead of the database itself (will depend on whatever backend DB is used, LevelDB or something else) because we essentially "random access" this DB (I am not aware to how this scales).

I think there are XEN attack spamooor scenarios laying around. I'll take a look at this when I'm back at desk (in a hour or so üòÉüëç)

> Jochem:
This is also a good suggestion! We have to be aware that the practical execution gas limit is thus 20% higher than the actual limit

> Kamil Chodo≈Ça:
> Small note for Fusaka we currently have 16.x M limit (lowered from 30M) https://eips.ethereum.org/EIPS/eip-7825

Valid. ü§¶‚Äç‚ôÇÔ∏è

> Jochem:
Do we know if all clients have a "flat" lookup? That is, does every client have a single lookup Address => Account and maybe also StorageKey => Value? I am not aware how each client implements it üòÉüëç

> Jochem:
If it is the MPT lookup it thus has on average trie depth lookups in order to get to the value which we want

> Jochem:
No worries! Just a small note, it will not change much in practice I think (except that we now need more txs so we have the 21k upfront cost extra, so this will slightly lower the "attacks" we can do per block)

> Kamil Chodo≈Ça:
Will significantly improve parallelization of transactions (instaed of min 2 on 60m you have a minimum of 3 + headroom for other transactions to squeeze in

> Pari:
Currently the perf-devnet-2 doesnt have fusaka live though, so we can still test worse scenarios if we want. But agreed that 16.xM cap exists soon on mainnet.

> Kamil Chodo≈Ça:
What I meant is - we can artificially test it out even on Pectra Spec. That is why I mentioned bot having same level of importance

> Kamil Chodo≈Ça:
It is not for Nethermind as we are nto still on any flat DB design.

> Jochem:
So if Nethermind reads an account it traverses the trie? And same for storage?

> CPerezz:
This should be the same for all except Erigon/Reth. Though IIRC, Nethermind had certain optimisations on top of RocksDB

> CPerezz:
üñº Maybe this doc helps: https://hackmd.io/@CPerezz/HkZlj_jIgg

> Jochem:
For XEN, here is a minimal version of the mining logic

contract BatchSpam {
    XEN constant target = XEN(0x06450dEe7FD2Fb8E39061434BAbCFC05599a6Fb8);

    function init() external {
        target.claimRank(1);
    }

    function loop() external {
        address share = address(uint160(address(this)) + 1);
        target.claimMintRewardAndShare(share, 50);
        target.claimRank(1);
    }
}

`init()` has to be called first. This starts "mining" for a day (which is the "1" param). The `loop()` will perform the actual "attack", this does a lot of storage changes in the XEN contract. It will also start re-mining it again

> Jochem:
If you loop() without waiting a day it will fail because you cannot claim those tokens

> Jochem:
I'm trying to find the scenario because it should be somewhere, I remember setting it up üòÖ

> CPerezz:
Do we need to keep track of the accounts/whatever we have mined for (such that we have a battery of them ready for any scenario run we want to perform?)

> CPerezz:
We're meeting with @jochembrouwer tomorrow to discuss about the scenarios. Improvements, things missing etc...

If anyone wants to join please lmk!

> Jochem:
We use a CREATE2 factory to set this up so we can somewhat easily loop over them. To analyse this attack is kind of nasty because we thus have this 1 day warmup time üòì

> CPerezz:
I see. I mean that once we mine, I'm not sure if we need to keep the account secret key or similar such that it can issue a call after the 1day period passes. Or it's not needed.

> Jochem:
Let me share the code I created back then 

// SPDX-License-Identifier: UNLICENSED
pragma solidity ^0.8.28;

import "./XEN.sol";

// Uncomment this line to use console.log
// import "hardhat/console.sol";

import "../node_modules/@openzeppelin/contracts/utils/Create2.sol";


contract StorageContract {
    uint256 time; 

    constructor() {
        time = block.timestamp;
    }

    fallback() external {
        time = block.timestamp;
    }
}

contract StorageSpamCREATE { 
    uint256 public saltTracker;
    bytes32 public constant initcodeHash = keccak256(type(StorageContract).creationCode);

    constructor() {
        saltTracker = 1;
    }

    function deployContracts() external returns (uint256) {
        uint saltBefore = saltTracker;
        uint gas = gasleft();
        deployContract();
        uint deploymentCost = gas - gasleft() + 5000;

        while(gasleft() > deploymentCost) {
            deployContract();
        }
        return saltTracker - saltBefore;
    }

    function deployContract() public {
        uint localSalt = saltTracker;
        Create2.deploy(0, bytes32(localSalt), type(StorageContract).creationCode);
        saltTracker = localSalt + 1;
    }

    function getAddress(uint256 salt) public view returns (address) {
        return Create2.computeAddress(bytes32(salt), initcodeHash);
    }

    function loop(uint256 index) public returns (uint256) {
        require(index >= 1);
        uint salt = saltTracker;
        require(index < salt);
        uint count = 0;
        uint gasNow = gasleft();
        loopSpecific(index);
        uint gasThreshold = (gasNow - gasleft()) + 5000;
        count++;
        index++;
        while (index < salt && gasleft() > gasThreshold) {
            bool ok = _loopSpecific(index);
            if (!ok) {
                break;
            }
            index++;
            count++;
        }
        return count;
    }

    function loopDefault() external returns (uint256) {
        return loop(1);
    }

    function loopSpecific(uint256 salt) public {
        require(_loopSpecific(salt));
    }

    function _loopSpecific(uint256 salt) private returns (bool) {
        address target = getAddress(salt);
        (bool success,) = target.call("");
        return success;
    }
}

> Jochem:
Whoops that was the storage spam one not the XEN

> Jochem:
Let me post the code

> Jochem:
// SPDX-License-Identifier: UNLICENSED
pragma solidity ^0.8.28;

import "./XEN.sol";

// Uncomment this line to use console.log
// import "hardhat/console.sol";

import "../node_modules/@openzeppelin/contracts/utils/Create2.sol";


contract BatchSpam {
    XEN constant target = XEN(0x06450dEe7FD2Fb8E39061434BAbCFC05599a6Fb8);

    function init() external {
        target.claimRank(1);
    }

    function loop() external {
        address share = address(uint160(address(this)) + 1);
        target.claimMintRewardAndShare(share, 50);
        target.claimRank(1);
    }
}

contract StorageSpamXEN { 
    BatchSpam immutable public spammer;
    uint256 public saltTracker;
    uint256 public initializedTracker;
    bytes32 public constant initcodeHash = keccak256(type(BatchSpam).creationCode);

    constructor() payable {
        saltTracker = 1;
        initializedTracker = 1;
    }

    function deployContracts() external returns (uint256) {
        uint saltBefore = saltTracker;
        uint gas = gasleft();
        deployContract();
        uint deploymentCost = gas - gasleft() + 5000;

        while(gasleft() > deploymentCost) {
            deployContract();
        }
        return saltTracker - saltBefore;
    }

    function deployContract() public {
        uint localSalt = saltTracker;
        Create2.deploy(0, bytes32(localSalt), type(BatchSpam).creationCode);
        saltTracker = localSalt + 1;
    }

    function getAddress(uint256 salt) public view returns (address) {
        return Create2.computeAddress(bytes32(salt), initcodeHash);
    }

    function loop(uint256 index) public returns (uint256) {
        require(index >= 1);
        uint initTracker = initializedTracker;
        require(index < initTracker);
        uint count = 0;
        uint gasNow = gasleft();
        loopSpecific(index);
        uint gasThreshold = (gasNow - gasleft()) + 5000;
        count++;
        index++;
        while (index < initTracker && gasleft() > gasThreshold) {
            bool ok = _loopSpecific(index);
            if (!ok) {
                break;
            }
            index++;
            count++;
        }
        return count;
    }

    function loopDefault() external returns (uint256) {
        return loop(1);
    }

    function loopSpecific(uint256 salt) public {
        require(_loopSpecific(salt));
    }

    function _loopSpecific(uint256 salt) private returns (bool) {
        BatchSpam target = BatchSpam(getAddress(salt));
        try target.loop() {
            return true;
        } catch {
            return false;
        }
    }

    function init() public returns (uint) {
        uint currentDeployedSalt = saltTracker;
        uint currentLastInitialized = initializedTracker;
        uint startInitialized = currentLastInitialized;
        if (currentLastInitialized < currentDeployedSalt) {
            uint gasNow = gasleft();
            BatchSpam(getAddress((currentLastInitialized))).init();
            uint gasThreshold = (gasNow - gasleft()) + 25000;
            currentLastInitialized++;
            while (currentLastInitialized < currentDeployedSalt && gasleft() > gasThreshold) {
                BatchSpam(getAddress((currentLastInitialized))).init();
                currentLastInitialized++;
            }
            initializedTracker = currentLastInitialized;
        }
        return (currentLastInitialized - startInitialized);
    }
}

> Jochem:
üñº https://gist.github.com/jochem-brouwer/a976b9dc17b23de1b579d6508dd49e74

> Jochem:
deployContracts() will deploy the XEN spammer contracts. init() will initialize the contracts. loop(X) will start to loop (this is the attack) from index X

> Jochem:
loopDefault() will just start at the first index

> CPerezz:
@pk910 or @parithoshj is this the scenario we have loaded as XEN in spamoor within perfnet2? I assume not right?

> Philipp (pk910):
there is a separate scenario for xen.   but it's currently only covering the claimRank part from as many wallets as possible (that's what we've seen in these huge xen txs),  the token mining afterwards is not there yet

> Pari:
@jochembrouwer @CPerezz did we manage to reproduce some of the tests from perf-devnet-1 onto perf-devnet-2? is there some writeup or reference dashboard for any potential degradation?

> CPerezz:
We have to run it still IIUC.

I've been diving into EEST to understand it and know how to use it and code tests.

@jochembrouwer has been working on spamoor scenarios for EEST. Which are promising.


Wanted to reach @KamilChNethermind to start prepping for it later today.


Also, still pending from my side to check the diff between tests we have and tests we want

> CPerezz:
Also @parithoshj is syncoor ready to perform the first round of sync testing at 2x mainnet state?

> Pari:
üñº it is, first job is running here: https://github.com/ethpandaops/perf-devnets/actions/runs/16747610905

> Pari:
job from friday had some peering issues, i updated enodes and i'm trying again with just one instance to debug if there are issues. ill run all if its successful.

> CPerezz:
@KamilChNethermind how viable is to run perfnet test/bench suite in perfnet2? Anything from our side needed/we can do?

> Kamil Chodo≈Ça:
Currently checking EEST tests on our gas-benchmarks and seems like working fine - so now is a matter of taking them and executing via EXECUTE

> Kamil Chodo≈Ça:
Will finish what I have yet to be done with EEST integration in gas-benchmarks and will take a look on that + will ask CarlosB to analyze as well

> CPerezz:
What ETA do you anticipate?

> CPerezz:
Just to know, no pressure at all

> Kamil Chodo≈Ça:
Today :) Having EEST tests automatically fetched from EEST running on gas-benchmarks infra, just dealing with some minor issues and one bigger task with warming of opccodes to bue done and then I can go with that

> Kamil Chodo≈Ça:
But already notified @cbermudez97

> Kamil Chodo≈Ça:
So he will jump on that as a top priority also

> CPerezz:
Will the results be displayed in the same place too?

> CPerezz:
Also, would be interesting to see the grafana metrics of the host machine when running the thing. 

Mainly interested on seeing what what happens internally with each opcode etc..

> Kamil Chodo≈Ça:
for gas-benchmarks - YES.

For perfnet-1 vs perfnet-2 - this is like completely separate test. So need to figure out how to grasp that well and compare 1 to 1 with toher network

> Kamil Chodo≈Ça:
Gas-benchmarks - benchmarking tool which is standalone and can be executed even locally

Perfnets are separate newtorks on which we can execute transactions - also from gas-benchmarks. But the thing here is -0 it just spams tx pool and there is no measurement made in a same way as pure benchmarking does

> Justin Florentine:
did anyone do a writeup on conclusions from perf-devnet-1 and what we're exploring going into devnet2?

> Pari:
perf-devnet-1: Gas limit increase testing, mostly compute bottlenecks
perf-devnet-2: State growth testing, currently at 2x mainnet and we pause here and do tests before moving to larger states

both will live in parallel and have different purposes

> Justin Florentine:
thanks

> CPerezz:
@parithoshj @KamilChNethermind going OOO for 2 weeks tomorrow. Was wondering what's the final status on Syncoor run for 2x state with target gas limit. Same for EEST run within Perfnet 2.

Also, interesting to see if there's anything needed from my side prior to leave. Stateless team (@asqyzeron ) will be taking over some of the scenario building & other stuff. 

Will still be reachable ofc. But a lot less present.

> Kamil Chodo≈Ça:
Had unexpected issues with final touches on EEST <-> gas-benches (about proper warming up - blockhash mismatches) - should be ready today and will continue on EEST Execute on perfnets -will check with CarlosB his status and will discuss with STEEL on that

> Kamil Chodo≈Ça:
We will take care of everything, no worries, take well deserved time out! :)

> Pari:
Not yet, we only have 1 sync machine till i can iron out bugs. Yesterday it had stalled so i restared the sync process to see if it was a one off. If it works this time ill order more machines and scale it out. Ill keep you updated!

> CPerezz:
Thanks a lot to you both guys!

As a reminder, @gakonst @mlnfltv would still appreciate answers to the doc I've shared a couple times already. Specially since you share DB type, I'm interested on being able to craft scenarios for it.

See the replied message.

üôèÔ∏èÔ∏èÔ∏èÔ∏èÔ∏èÔ∏è

> Pari:
üñº The nethermind sync test failed because of this:
09 Aug 21:26:54 | Processing block failed. Block: 6420836 (0x34d0d9...b0a341), Exception: System.InvalidOperationException: Maximum size of branch reached (8192). This is unexpected.
cc @ben_a_adams @KamilChNethermind 

download the logs from here: https://github.com/ethpandaops/perf-devnets/actions/runs/16832360142

> Ben {chmark} Adams ‚ü†:
@asdacap

> Asdacap:
Master?

> Asdacap:
Does it include this? https://github.com/NethermindEth/nethermind/pull/9086

> Ben {chmark} Adams ‚ü†:
might be performance branch so not have that @parithoshj can you confirm?

Will reset to master if so

> Pari:
ahh yea we're using the performance branch (same as what's running on the devnet)

> Ben {chmark} Adams ‚ü†:
k, will reset

> Ben {chmark} Adams ‚ü†:
should be good now

> Pari:
thanks! so using the perf branch in the next run should be fine?

> Ben {chmark} Adams ‚ü†:
Hopefully or our next release will be problematic ;)

> Gary Rong:
Hey guys, I am trying to join the perf-devnet-2, but looks like it's a bit hard to find the peer (basically it's all mainnet peers). Any easy way to grab peers so that I can trigger snap sync?

> Pari:
Yup enodes here should be valid: https://config.perf-devnet-2.ethpandaops.io/api/v1/nodes/inventory

Let me know if this doesnt work, I‚Äôll regenerate the file and get the latest values

> Gary Rong:
enode://22c1dd13d7738ae461740bbe1340733bdabe88c0682b5788bb308bb80f0d6281085196600416f187dc0f1d5d9613b1c2590e6e5b4003329a464910f613bd096a@157.180.14.229:30303?discport=30303
enode://d54f2c88dad80719c6f2be2c9925c6c2609f845ed84c9e2f961c2a7e67bfe218a657118a70a45d1b245e96e2678b0276ca2ca869d7aa5688fcb1a52381d6a1e3@157.180.14.230:30303?discport=30303
enode://41383da176e6bcb21003b7dd2de4a15255111320f819bf83db0c23b6c19bd83ca89edaa3f68d0f652ff9924e9d8c0bda84e5e8e3bfec33e0a8c387a23e044b5c@157.180.14.227:30303?discport=30303
enode://3ba5f46aa1a9d863de4ce5ff67ec46704c43b2a1d5100e272dbfec5e56b350113018d3e67c400f09db60d4ec55df025bd209802c67826717a33a48acbe13b305@157.180.14.226:30303?discport=30303
enode://62b16e448d6aa6a8ff7a2a25701b43935d956398d7f7bf57fccce281dd989d19cee4d6e2142e11d4be42ffacc394a138b686099885a11d50b4e13db5b8080180@157.180.14.225:30303?discport=30303
enode://d38286e50eea1bb75da0f89a65fa8236255d0e52b2980998c7d903be4c69e3477f9f0e54f6039c2308d8e3c41123f3d1aeee2ad4f2811ebd48557a3dc406a273@157.180.14.228:30303?discport=30303

I think these nodes are basically fully connected and barely have a slot to connect with?

> Pari:
hmm you are right... on the CL side they seem to peer together as expected. On the EL side, we see extremely round numbers - likely they are client defaults and don't have empty slots. 

I can restart some nodes to free up a slot, but we do need to figure out how to do this longer term for such shadowforks. (There's no new fork, so forkID would also be the same as mainnet)

The admin API is accessible over RPC though, i can give you credentials for it so that you aren't immediately blocked

> Csaba:
I think the forkID should be forced different. Otherwise they are really just the same from the ELs PoV, and the EL is right to be confused.

> Gary Rong:
We can extend the forkID by adding a salt

> Pari:
Yeah its a shadowfork, so unless we schedule a BPO/fork (maybe BPO is the way to go since its minimally invasive), the forkID wont change. I would also say a longer term solution would be to extend the forkID to add some network identifier we can modify (or a salt)

> Csaba:
There are performance related mechanisms in the sync logic, but dropping peers and finding the right peers would be extremely slow because the number of "good" peers is extremely slow compared to mainnet.

> Csaba:
salt or network ID seems like a cleaner solution, but any change is OK temporarily üôÇ
With the current setup, sync testing is I think limited to peers you set up. And those nodes will fill up with mainnet peers I think, so won't stay free for long.

> Pari:
Yeah for now id just use the admin api to make sure there's a free slot

> Csaba:
That makes the sync tests a "sync (download) from server" test, right? Not a problem, just want to be sure we are on the same page when interpreting what we actually test.

> Pari:
in such a small network, that would always be the case though. What we're mostly interested is in seeing the time each sync phase takes and how increased state size affects it. Ideal sync situation at the very least.

> Simon | Besu:
Is using --whitelist to require a shadow fork block sufficient for geth?
The Besu equivalent seemed to do the trick eventually https://besu.hyperledger.org/public-networks/reference/cli/options#required-block

> Pari:
Yeah would this account for peers? or just for what you request during sync?

> Simon | Besu:
For besu this will disconnect mainnet peers after discovery and after status message is exchanged, so will impact all peer activities including sync. Obv still need to find MSF peers too. 
Not sure about geth impl but seems very similar from the description...I mentioned the deprecated flag, should now be --eth.requiredblocks for geth 

> Comma separated block number-to-hash mappings to require for peering

> Rafael Matias:
Would be good to have a simple way to somehow limit peering. Currently all nodes reached their max peers and it‚Äôs difficult to do a sync test. Either somehow forcing a new network id, or the previously mentioned required/whitelist blocks approach (but it also has to disconnect peers that don‚Äôt serve that block).

> Pari:
Yeah right now we can only sync like half the clients, even after using the admin API. It'd be great if we can get some help here! 

We probably need some short term solution and a longer term one too.

Clients that work: geth, besu
Clients that dont work: nethermind, reth, erigon (they however have a env var we havent tried yet and getting clarification on)

> Pari:
üñº and cc @skylenet these are the client prometheus metrics we wanted to collect: https://hackmd.io/boZCC4eNR66zN3m9VDYFWQ

How do you want to showcase the metrics? link to our grafana dashboard or collect it directly on the metrics exporter?

> CPerezz:
How much work is to try those?

> Pari:
the github action is ready, so its easy to try out changes now

> CPerezz:
Anyway I can trigger it? Or help with it? On this way we can tell client tieams exactlu what we need (if the env var doesn't work)

> Pari:
üñº You can run a manual run here: https://github.com/ethpandaops/perf-devnets/actions/workflows/syncoor-devnet-2.yaml

getting you up to date errors in a bit.

> Rafael Matias:
If you run them right now, you'll have peering issues. I've been manually cross adding peers using admin_addPeers or static peers on startup. It's annoying as hell. That's why it would be good to have the conntected peers, limited to peers that are just part of perf-devnet-2.  And not filled with random mainnet nodes.

> Pari:
How about i add this to perf-devnet-2 geth nodes: --eth.requiredblocks value, that way atleast geth should filter out mainnet blocks. If that is the case, then we should be sure that atleast geth will always have peering slots available for sync tests. 

Do other clients have similar flags?

> Rafael Matias:
That's a besu flag. On geth it's `--whitelist=" . Besu says that they also limit peers based on that (also stated in their documentation). I'm not sure if geth does the same. I also had a quick glance across flags on other clients, and couldn't find something similar. But I might be wrong.

> CPerezz:
@mlnfltv @shekhirin @ben_a_adams do you happen to expose a flag or env var to config such that we can limit peers or whitelist them? Similar to what Besu has in here üëÜ

> Pari:
Applied on geth and currently updating the required block on besu. So we should have 3/6 nodes on the network with free slots to peer up. I'd guess this helps us do sync tests, we'll try a run again now. 

In the meantime:
- would be great if reth/erigon/nethermind have a similar flag
- erigon if you can get back to me about --shadowfork.block or if we should use `SNAPS_GIT_BRANCH=perfnet2_shadow_fork_main`?

> Alexey Shekhirin:
cc @rkrasiuk

> Roman Krasiuk:
we have a --trusted-only flag that would connect to or accept connections only from trusted peers which you can specify with --trusted-peers flag

> Rafael Matias:
That's not so ideal, cause we don't rly want to keep restarting the nodes to add more trusted peers. E.g. me starting random sync tests and having to add these new nodes as trusted peers on the synced nodes.

> CPerezz:
But the machines aren't always the same ones in perfnet2? How is it an issue?

> Pari:
because the CI machines are using self hosted machines, but each run it gets a new enode. So each run, we'd need to restart reth with the updated enode so it peers - and that wouldn't really be feasible. We can use it to make sure it only peers with the 6 nodes that are static, but those aren't doing any sync tests

> CPerezz:
Can you elaborate onm what would be the ideal feature? And ping Roman on it to see if we can make it happen or we can do some trick somehow?

> Rafael Matias:
Something like --eth.requiredblocks like geth and besu have would be useful. 

    --eth.requiredblocks value                                             ($GETH_ETH_REQUIREDBLOCKS)
          Comma separated block number-to-hash mappings to require for peering


If I remember correctly, this flag was added during the pectra incidents, which made it easier to find peers that were on the right chain. It's also quite useful for this case, since we're on a mainnet shadowfork, so we only want to have peers that are on a this specific fork.

> Roman Krasiuk:
you can supply trusted peers via admin api tho

> Rafael Matias:
The problem is that we would have to be calling admin_addTrustedPeer on both sides. On the new machines that are starting to sync it's easy. It gets more complex for us to extract the enodes from the new nodes and then also call addTrustedPeer on the synced nodes.

> Roman Krasiuk:
@mattsse fyi

> CPerezz:
@skylenet @parithoshj yesterday I saw syncoor running for Besu and Geth. So far, I only see Geth's result reported. But not Besu's.

Any ideas what is going on?

> CPerezz:
Also, Reth has 0 EL peers reported. Which will make it fail the sync. @mattsse any workarrounds from your side?

> Rafael Matias:
Check the most recent besu report. It‚Äôs there.

> Rafael Matias:
https://syncoor.perf-devnet-2.ethpandaops.io/#/test/sync-test-1756120105605747182

> CPerezz:
LOL I thought since the in-progress runs get updated automatically without refreshing, the rest of parts did too. (Which doesn't seem to be the case. MB).


Is the status we are still missing the feature flag from Reth and Nethermind?

> Rafael Matias:
Haha. I see :) Yeah, the results don‚Äôt update automatically yet. Just the live view.  But could be improved. 

Yes, it would be nice to have the requiredblock(s) flag on Nethermind, Reth and Erigon. Would make the peering easier.

> CPerezz:
I think @KamilChNethermind is back today. So maybe he can push for it internally?? 

Also awaiting for @mattsse and @mlnfltv or @FunnyGiulio on their respective solution proposals

> Rafael Matias:
Yeh, there‚Äôs already an issue for it on Nethermind: https://github.com/NethermindEth/nethermind/issues/9204

> Kamil Chodo≈Ça:
@skylenet Why is it doing archive sync for Nethermind? Can You add "Sync.FastSync=true, Sync.SnapSync=true"?

> Kamil Chodo≈Ça:
Btw about this:
https://github.com/NethermindEth/nethermind/issues/9204

This will break a peering of historical backfill, right? I mean - depends on what we exactly want to achieve having this feature where peer to be connected to us at all to contain very particular version of specific block we will throw out all the "valid" peers which will ahve mainnet history which we backfill